---
title: "Classification of Acute Respiratory Distress Syndrome"
author: "Robert Edwards"
output:
  
  pdf_document:
    
    latex_engine: pdflatex
    number_sections: no
    fig_caption: yes
  word_document: default
  html_document:
    
    code_folding: hide
    df_print: paged
    fig_caption: yes
geometry: "left=3cm,right=3cm,top=3cm,bottom=3cm"
header-includes: 
  \usepackage[bottom]{footmisc}
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{color}
  \usepackage{xcolor}
---

```{r setup, include=FALSE, echo = FALSE, eval = TRUE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, comment = NA)
options()
```

```{r libraries, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(moderndive)
library(skimr)
library(tidyr)
library(kableExtra)
library(gridExtra)
library(xtable)
library(knitr)
library(GGally)
library(broom)
library(naniar) # missing data eda
library(VIM) # data imputation
```

```{r import_data, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
data.df <- read.csv(file = "../data/ARDS_data_clean.csv", header = TRUE)
```



#Data Preparation
##Scaling the continuous variables
```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, results = "hide"}
#feature_names <- colnames(data.df)[5:35]
data_standard.df <- data.df %>%
  select(5:ncol(data.df)) %>%
  mutate_all(funs(scale))
#  mutate_each_(funs(log), vars=feature_names) %>%
#  glimpse()

# Reordering columns
data_standard.df <- cbind(data.df[,1:4], data_standard.df) 

#standardized <- apply(data_standard.df[, 5:ncol(data_standard.df)], 2, scale)  # scale the features
#data_standard.df <- cbind(data_standard.df[,1:4], standardized)  # rebind variables

# Changing categorical to 0 or 1
#data_standard.df <- data_standard.df %>% 
#  mutate(Gender = ifelse(data_standard.df$Gender == "m" , 0, 1)) %>% # male = 0, female = 1
#  mutate(ECMO_Survival = ifelse(data_standard.df$ECMO_Survival == "Y" , 1, 0)) # Yes = 1, No = 0

# Factoring categorical
#data_standard.df <- data_standard.df %>%
#  mutate(Indication = factor(Indication)) %>% 
#  mutate(ECMO_Survival = factor(ECMO_Survival)) %>%
#  mutate(Gender = factor(Gender)) %>%
#  suppressMessages()  # suppress output

#data_standard.df %>%
#  glimpse()
```

##Dropping NAs from dataset
Dropping variables with >40% NAs (`PreECMO_Albumin`)
```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
data_standard.df <- data_standard.df %>% 
  select(-PreECMO_Albumin)# Drop variable missing >40% data

#data_standard.df <- data_standard.df %>%
#  drop_na()  # Drop rows missing data

```


##Train / Validation / Test
```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
set.seed(123)

n <- nrow(data_standard.df)
ind1 <- sample(c(1:n), round(n / 2))
ind2 <- sample(c(1:n)[-ind1], round(n / 4))
ind3 <- setdiff(c(1:n), c(ind1, ind2))
train.ARDS <- data_standard.df[ind1, ]
valid.ARDS <- data_standard.df[ind2, ]
test.ARDS <- data_standard.df[ind3, ]
```


```{r model1, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
model_logit <- glm(ECMO_Survival ~ . -Pt_ID, data=train.ARDS, family=binomial(link="logit"))

model_logit %>%
  summary()
```

A quick note about the plogis function: The `glm()` procedure with `family="binomial"` will build the logistic regression model on the given formula. When we use the `predict` function on this model, it will predict the log(odds) of the Y variable. This is not what we ultimately want because, the predicted values may not lie within the 0 and 1 range as expected. So, to convert it into prediction probability scores that is bound between 0 and 1, we use the `plogis()`.  For more info see (blog on logisitic regression)[http://r-statistics.co/Logistic-Regression-With-R.html]. 

```{r predict1, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
## Using the predict function to predict for the validation data.
pred.valid <- plogis(predict(model_logit, valid.ARDS))  # predicted scores
```

###Optimal Prediction Probability Cutoff for the Model
The default cutoff prediction probability score is 0.5 or the ratio of ---- in the training data. But sometimes, tuning the probability cutoff can improve the accuracy in both the development and validation samples. The `InformationValue::optimalCutoff` function provides ways to find the optimal cutoff to improve the prediction of ----, ----, both ---- and ---- and o reduce the misclassification error. Let's compute the optimal score that minimizes the misclassification error for the above model.

```{r cutoff1, echo = FALSE, eval = FALSE, warning = FALSE, message = FALSE}
# Find the optimal cutoff value to use (default = 0.5)
library(InformationValue)
opt_cutoff <- optimalCutoff(test.ARDS$ECMO_Survival , pred.valid)[1] 

## Changing the predictions to predited labels.
pred.valid.label <- ifelse(round(pred.valid) <= 0.5, 0, 1)

## Changing the predictions to predited labels.
pred.valid.label.opt <- ifelse(round(pred.valid) <= opt_cutoff, 0, 1)

```

##Model Diagnostics
```{r xtab1, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
## Cross-classification table.
logit.table <- table(valid.ARDS$ECMO_Survival, pred.valid.label)
## Correct classification rate (CCR).
sum(pred.valid.label == valid.ARDS$ECMO_Survival) / length(pred.valid.label)
## Class-specific CCRs.
diag(logit.table) / rowSums(logit.table)

## Cross-classification table.
#logit.table.opt <- table(valid.ARDS$ECMO_Survival, pred.valid.label.opt)
## Correct classification rate (CCR).
#sum(pred.valid.label == valid.ARDS$ECMO_Survival) / length(pred.valid.label.opt)
## Class-specific CCRs.
#diag(logit.table) / rowSums(logit.table)
```





\newpage
#Logistic Regression LASSO



\newpage
#K-Nearest Neighbors


\newpage
#LDA/QDA


\newpage
#Support Vector Machines
















