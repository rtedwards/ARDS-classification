
---
title: "Classification of Acute Respiratory Distress Syndrome"
author: "Robert Edwards"
output:
  
  pdf_document:
    
    latex_engine: pdflatex
    number_sections: no
    fig_caption: yes
  word_document: default
  html_document:
    
    code_folding: hide
    df_print: paged
    fig_caption: yes
geometry: "left=3cm,right=3cm,top=3cm,bottom=3cm"
header-includes: 
  \usepackage[bottom]{footmisc}
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{color}
  \usepackage{xcolor}
---

```{r setup, include=FALSE, echo = FALSE, eval = TRUE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, comment = NA)
options()
```

```{r libraries, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(skimr)
library(tidyr)
library(kableExtra)
library(gridExtra)
library(xtable)
library(knitr)
library(glmnet)
```

```{r import_data, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
data.df <- read.csv(file = "../data/ARDS_data_clean.csv", header = TRUE)
```



#Data Preparation
##Scaling the continuous variables
```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, results = "hide"}
#feature_names <- colnames(data.df)[5:35]
data_standard.df <- data.df %>%
  dplyr::select(-"Pt_ID",
         -"ECMO_Survival",
         -"Gender",
         -"Indication") %>%
  mutate_all(funs(scale))
#  mutate_each_(funs(log), vars=feature_names) %>%
#  glimpse()

# Reordering columns
data_standard.df <- cbind(data.df[,1:4], data_standard.df) 

#standardized <- apply(data_standard.df[, 5:ncol(data_standard.df)], 2, scale)  # scale the features
#data_standard.df <- cbind(data_standard.df[,1:4], standardized)  # rebind variables

# Changing categorical to 0 or 1
#data_standard.df <- data_standard.df %>% 
#  mutate(Gender = ifelse(data_standard.df$Gender == "m" , 0, 1)) %>% # male = 0, female = 1
#  mutate(ECMO_Survival = ifelse(data_standard.df$ECMO_Survival == "Y" , 1, 0)) # Yes = 1, No = 0

# Factoring categorical
#data_standard.df <- data_standard.df %>%
#  mutate(Indication = factor(Indication)) %>% 
#  mutate(ECMO_Survival = factor(ECMO_Survival)) %>%
#  mutate(Gender = factor(Gender)) %>%
#  suppressMessages()  # suppress output

#data_standard.df %>%
#  glimpse()
```

##Dropping NAs from dataset
Dropping variables with >40% NAs (`PreECMO_Albumin`)
```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
data_standard.df <- data_standard.df %>% 
  dplyr::select(-PreECMO_Albumin) %>% # Drop variable missing >40% data
  drop_na() # Drop rows missing data
```


##Train / Validation / Test
```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
set.seed(123)

n <- nrow(data_standard.df)
ind1 <- sample(c(1:n), round(n / 2))
ind2 <- sample(c(1:n)[-ind1], round(n / 4))
ind3 <- setdiff(c(1:n), c(ind1, ind2))
train.ARDS <- data_standard.df[ind1, ]
valid.ARDS <- data_standard.df[ind2, ]
test.ARDS <- data_standard.df[ind3, ]
```


```{r model1, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
model.logit <- glm(ECMO_Survival ~ . -Pt_ID, data=train.ARDS, family=binomial(link="logit"))

model.logit %>%
  summary()
```

A quick note about the plogis function: The `glm()` procedure with `family="binomial"` will build the logistic regression model on the given formula. When we use the `predict` function on this model, it will predict the log(odds) of the Y variable. This is not what we ultimately want because, the predicted values may not lie within the 0 and 1 range as expected. So, to convert it into prediction probability scores that is bound between 0 and 1, we use the `plogis()`.  For more info see (blog on logisitic regression)[http://r-statistics.co/Logistic-Regression-With-R.html]. 

```{r predict1, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
## Using the predict function to predict for the validation data.
pred.valid.logit <- predict(model.logit, valid.ARDS, type = "response")  # predicted scores
```

###Optimal Prediction Probability Cutoff for the Model
The default cutoff prediction probability score is 0.5 or the ratio of ---- in the training data. But sometimes, tuning the probability cutoff can improve the accuracy in both the development and validation samples. The `InformationValue::optimalCutoff` function provides ways to find the optimal cutoff to improve the prediction of ----, ----, both ---- and ---- and o reduce the misclassification error. Let's compute the optimal score that minimizes the misclassification error for the above model.

```{r cutoff1, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
# Find the optimal cutoff value to use (default = 0.5)
library(InformationValue)
#opt_cutoff <- optimalCutoff(test.ARDS$ECMO_Survival , pred.valid)[1] 

## Changing the predictions to predited labels.
pred.valid.logit <- ifelse(round(pred.valid.logit) <= 0.5, 0, 1)

## Changing the predictions to predited labels.
#pred.valid.label.opt <- ifelse(round(pred.valid) <= opt_cutoff, 0, 1)

```

##Model Diagnostics
```{r xtab1, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
## Cross-classification table.
xtab.logit <- table(valid.ARDS$ECMO_Survival, pred.valid.logit)

## Correct classification rate (CCR).
ccr.logit <- sum(diag(xtab.logit)) / sum(xtab.logit)
err.logit <- 1 - ccr.logit

## Class-specific CCRs.
ccr.class.logit <- diag(xtab.logit) / rowSums(xtab.logit)
err.class.logit <- 1 - ccr.class.logit

## Precision / Recall
tpr.logit <- xtab.logit[1]/(xtab.logit[1] + xtab.logit[2]) #sensitivity
fpr.logit <- xtab.logit[2]/(xtab.logit[2] + xtab.logit[4]) #False Positive Rate
```





\newpage
#Logistic Regression LASSO

```{r lasso_regression, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}

# Note alpha=1 for lasso only and can blend with ridge penalty down to
# alpha=0 ridge only.
x.train <- model.matrix( ~ . -ECMO_Survival -Pt_ID, data=train.ARDS) # glmnet expects a matrix not dataframe
y.train <- as.factor(train.ARDS$ECMO_Survival)

x.valid <- model.matrix( ~ . -ECMO_Survival -Pt_ID, data=valid.ARDS) # glmnet expects a matrix not dataframe
y.valid <- valid.ARDS$ECMO_Survival

n.train <- length(y.train)
n.valid <- length(y.valid)

###################################
### Implement ridge regression (alpha=0), Lasso regression (alpha=1) and elastic net (alpha=0.5)
###################################

model.ridge <- glmnet(x.train, y.train, family="binomial", alpha=0)
model.lasso <- glmnet(x.train, y.train, family="binomial", alpha=1)
model.net <- glmnet(x.train, y.train, family="binomial", alpha=.5)

###################################
### Choosing between ridge regression, Lasso regression and elastic nets using glmnet()
###################################
nets.a <- seq(0, 0.1, by=0.001)
n.a <- length(nets.a)

nets.model <- vector(mode="list", length=n.a)
nets.prediction <- vector(mode="list", length=n.a)
nets.mse <- vector(mode="numeric", length=n.a)

for (i in 1:n.a) {
  print(paste("Iteration", i, "of", n.a))
  
  nets.model[[i]] <- cv.glmnet(x.train, y.train, type.measure="class", alpha=nets.a[i], family="binomial")
  nets.prediction[[i]] <- predict(nets.model[[i]], s=nets.model[[i]]$lambda.1se, newx=x.valid)
#  nets.mse[i] <- sum((nets.prediction[[i]]-y.valid)^2)/n.valid
}
cbind(nets.a, nets.mse)
```


```{r plot elastic net, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '100%', fig.align="center", fig.pos="H", fig.cap = "\\label{fig:cor}Lasso, Ridge, Elastic Net shrinkage and lambda cross-validation."}

# Plot variable coefficients vs. shrinkage parameter lambda.
###################################
### Display coefficient estimates and MSE, each as a function of the logarithm of parameter lambda
###################################

oldpar <- par(no.readonly=TRUE)
par(mfrow=c(3, 2))

plot(model.lasso, xvar="lambda", label = TRUE)
plot(nets.model[[11]], main="LASSO") # alpha = 1

plot(model.ridge, xvar="lambda", label = TRUE)
plot(nets.model[[1]], main="Ridge") # alpha = 0

plot(model.net, xvar="lambda", label = TRUE)
plot(nets.model[[6]], main="Elastic Net") # alpha = .5

par(oldpar)

```



```{r lasso_prediction, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
pred.valid.lasso <- predict(nets.model[[11]], s="lambda.min", newx=x.valid)

## Changing the predictions to predited labels.
pred.valid.lasso <- ifelse(round(pred.valid.lasso) <= 0.5, 0, 1)

coef(nets.model[[11]], s = "lambda.min")

```


##Model Diagnostics
```{r xtab2, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
## Cross-classification table.
xtab.lasso <- table(valid.ARDS$ECMO_Survival, pred.valid.lasso)
xtab.lasso
## Correct classification rate (CCR).
ccr.lasso <- sum(diag(xtab.lasso)) / sum(xtab.lasso)
err.lasso <- 1 - ccr.lasso

## Class-specific CCRs.
ccr.class.lasso <- diag(xtab.lasso) / rowSums(xtab.lasso)
err.class.lasso <- 1 - ccr.class.lasso

## Precision / Recall
tpr.lasso <- xtab.lasso[1]/(xtab.lasso[1] + xtab.lasso[2]) #sensitivity
fpr.lasso <- xtab.lasso[2]/(xtab.lasso[2] + xtab.lasso[4]) #False Positive Rate
```


\newpage
#K-Nearest Neighbors

```{r KNN, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
library(kknn)

n.valid <- nrow(x.valid)

pred.train.knn <- kknn(formula=formula(ECMO_Survival~.), train=train.ARDS, test=valid.ARDS, k=5, dist=2, ker="rectangular")
pred.valid.knn <- kknn(formula=formula(ECMO_Survival~.), train=train.ARDS, test=valid.ARDS, k=5, dist=2, ker="rectangular")

## Class-specific CCRs.
xtab.knn <- table(pred.valid.knn$fitted.values, valid.ARDS$ECMO_Survival) 
ccr.class.knn <- diag(xtab.knn) / rowSums(xtab.knn)
err.class.knn <- 1-ccr.class.knn

## Calculate and store the test set correct classification rate
ccr.knn <- sum(pred.valid.knn$fitted.values == valid.ARDS$ECMO_Survival) / n.valid
err.knn <- 1 - ccr.knn

## Precision / Recall
tpr.knn <- xtab.knn[1]/(xtab.knn[1] + xtab.knn[2]) #sensitivity
fpr.knn <- xtab.knn[2]/(xtab.knn[2] + xtab.knn[4]) #False Positive Rate
```


```{r continuous, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
## Removing categorical variables because KNN can only handle continuous data
train.data <- train.ARDS %>% 
  dplyr::select(-"Pt_ID",
         -"ECMO_Survival",
         -"Gender",
         -"Indication")
#train.data <- data.matrix(train.data)

train.class <- train.ARDS %>% 
  dplyr::select("ECMO_Survival")
#train.class <- data.matrix(train.class)


valid.data <- valid.ARDS %>% 
  dplyr::select(-"Pt_ID",
         -"ECMO_Survival",
         -"Gender",
         -"Indication")
#valid.data <- data.matrix(valid.data)

valid.class <- valid.ARDS %>% 
  dplyr::select("ECMO_Survival")
#valid.class <- data.matrix(valid.class)
```

```{r KNN2, echo = FALSE, eval = FALSE, warning = FALSE, message = FALSE}

library(class) ## knn function
set.seed(1)
## K-Nearest Neighbours.
n <- 15
k_to_try <- seq(1, n, 2) ## try 10 values of k
n.valid <- nrow(valid.data)
correct.class <- rep(NA, length(k_to_try)) ## vector to store the correct classification rates for different values of k.

count <- 0
for (i in k_to_try) {
  pred.valid.knn <- knn(data.matrix(train.data), data.matrix(valid.data), data.matrix(train.class), k=i)
  ## Calculate and store the test set correct classification rate for k=1.
  correct.class[count] <- sum(pred.valid.knn == data.matrix(valid.class)) / n.valid
  count <- count + 1
}

pred.valid.knn <- knn(data.matrix(train.data), data.matrix(valid.data), data.matrix(train.class), k=11)

xtab.knn <- table(pred.valid.knn, data.matrix(valid.class))
xtab.knn

## Misclassification rate =  1 - correct classifcation rate
ccr.knn <- sum(diag(xtab.knn)) / sum(xtab.knn)
err.knn <- 1 - ccr.knn

## Class-specific CCRs.
ccr.class.knn <- diag(xtab.knn) / rowSums(xtab.knn)
err.class.knn <- 1 - ccr.class.knn

plot(x = k_to_try, y = correct.class, type = "l")
```



\newpage
#CVA

```{r CVA, echo = FALSE, eval = FALSE, warning = FALSE, message = FALSE}

#model.cva <- MASS::lda(train.data, train.class, prior = c(0.5, 0.5)) 
model.cva <- MASS::lda(ECMO_Survival ~ ., train.ARDS, prior = c(0.5, 0.5)) 
model.cva
plot(model.cva) 

## Now we predict the labels based on the model.
#pred.valid.cva <- predict(model.cva, newdata = valid.data) 
pred.valid.cva <- predict(model.cva, valid.ARDS)

## predict produces a list, one element of which is called "class", which equals to predicted classes of the obs., the others are "posterior", which equals to estimated posterior probabilities of each observation belonging to each class and finally "x", which equals to canonical variate scores.
## Note that the linear discriminant is the same as the canonical variates scores here.
names(pred.valid.cva) #[1] "class" "posterior" "x" 

## Creating a cross-classification table comparing the true labels to the predicted labels.
xtab.cva <- table(valid.ARDS$ECMO_Survival, pred.valid.cva$class) 
xtab.cva

## Misclassification rate =  1 - correct classifcation rate
ccr.cva <- sum(diag(xtab.cva)) / sum(xtab.cva)
err.cva <- 1 - ccr.cva

## Class-specific CCRs.
ccr.class.cva <- diag(xtab.cva) / rowSums(xtab.cva)
err.class.cva <- 1 - ccr.class.cva

## Our canonical vector is given by the coefficients of the linear discriminant.
coef(model.cva)
```


\newpage
#LDA
```{r LDA, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}

#model.lda <- MASS::lda(train.data, train.class) 
model.lda <- MASS::lda(ECMO_Survival ~ ., train.ARDS) 
model.lda
plot(model.lda) 

## Now we predict the labels based on the model.
#pred.valid.lda <- predict(model.lda, newdata = valid.data) 
pred.valid.lda <- predict(model.lda, newdata = valid.ARDS) 
## predict produces a list, one element of which is called "class", which equals to predicted classes of the obs., the others are "posterior", which equals to estimated posterior probabilities of each observation belonging to each class and finally "x", which equals to canonical variate scores.
## Note that the linear discriminant is the same as the canonical variates scores here.
names(pred.valid.lda) #[1] "class" "posterior" "x" 

## Creating a cross-classification table comparing the true labels to the predicted labels.
xtab.lda <- table(valid.ARDS$ECMO_Survival, pred.valid.lda$class)
xtab.lda

## Misclassification rate =  1 - correct classifcation rate
ccr.lda <- sum(diag(xtab.lda)) / sum(xtab.lda)
err.lda <- 1 - ccr.lda

## Class-specific CCRs.
ccr.class.lda <- diag(xtab.lda) / rowSums(xtab.lda)
err.class.lda <- 1 - ccr.class.lda

## Precision / Recall
tpr.lda <- xtab.lda[1]/(xtab.lda[1] + xtab.lda[2]) #sensitivity
fpr.lda <- xtab.lda[2]/(xtab.lda[2] + xtab.lda[4]) #False Positive Rate

## Our canonical vector is given by the coefficients of the linear discriminant.
coef(model.lda)
```

\newpage
#QDA
```{r QDA, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}

#model.qda <- MASS::qda(train.data, train.class) 
model.qda <- MASS::qda(ECMO_Survival ~ ., train.ARDS) 
model.qda
#plot(model.qda) 

## Now we predict the labels based on the model.
pred.valid.qda <- predict(model.qda, newdata = valid.ARDS) 

## predict produces a list, one element of which is called "class", which equals to predicted classes of the obs., the others are "posterior", which equals to estimated posterior probabilities of each observation belonging to each class and finally "x", which equals to canonical variate scores.
## Note that the linear discriminant is the same as the canonical variates scores here.
names(pred.valid.qda) #[1] "class" "posterior" "x" 

## Creating a cross-classification table comparing the true labels to the predicted labels.
xtab.qda <- table(valid.ARDS$ECMO_Survival, pred.valid.qda$class) 
xtab.qda

## Taking a look at the posterior probabilities for the observations we got the prediction wrong for.
pred.valid.qda$posterior[valid.ARDS$ECMO_Surviva != pred.valid.qda$class, ]

## Misclassification rate =  1 - correct classifcation rate
ccr.qda <- sum(diag(xtab.qda)) / sum(xtab.qda)
err.qda <- 1 - ccr.qda

## Class-specific CCRs.
ccr.class.qda <- diag(xtab.qda) / rowSums(xtab.qda)
err.class.qda <- 1 - ccr.class.qda

## Precision / Recall
tpr.qda <- xtab.qda[1]/(xtab.qda[1] + xtab.qda[2]) #sensitivity
fpr.qda <- xtab.qda[2]/(xtab.qda[2] + xtab.qda[4]) #False Positive Rate

## Our canonical vector is given by the coefficients of the linear discriminant.
coef(model.qda)
```


\newpage
#Trees

```{r Tree, echo = FALSE, eval = FALSE, warning = FALSE, message = FALSE}
set.seed(126)
library(rpart)

## Here we need to set the argument method to "class" to ensure we get a classification rather than regression tree (since the labels in this case are numbers rather than text so could be interpreted either way).
model.tree <- rpart(ECMO_Survival ~ . , data=train.ARDS[, -1], 
                    method = "class",
                    control = rpart.control(minsplit = 1, 
                                            minbucket = 1, 
                                            cp = 0.1))

## Printing the text version of the tree.
model.tree

## Looking at the graphical version of the tree.
plot(model.tree, margin=0.1)
text(model.tree, cex = 0.5, use.n = TRUE, xpd = TRUE)

## Recall that at each split if the answer to the statement is TRUE we go LEFT, if it is FALSE we go RIGHT.

## We check the complexity parameter versus cross-validated error (xerror) to see if we need to prune the tree.
## We can initially look at a plot of these, if the lowest point is not the farthest right then the tree needs pruning.
plotcp(model.tree)
## Looking at the xerror column in the table we see that the lowest value is at the last line, indicating no need for pruning.
printcp(model.tree)

## We predict the classifications based on the fitted tree using the predict command with the argument type set to "class", otherwise it returns a matrix of posterior probabilities of each obseration belonging to each class.
pred.valid.tree <- predict(model.tree, valid.ARDS, type="class")

## Constructing a cross-classification table of the real versus predicted classifications.
xtab.tree <- table(valid.class, pred.valid.tree)
xtab.tree

## Taking a look at the posterior probabilities for the observations we got the prediction wrong for.
#prob.valid.tree <- predict(model.tree, valid.ARDS)
#prob.valid.tree[valid.class != pred.valid.tree, ]

## Calculating the misclassification rate.
ccr.tree <- sum(diag(xtab.tree)) / sum(xtab.tree)
err.tree <- 1 - ccr.tree

## Class-specific CCRs.
ccr.class.tree <- diag(xtab.tree) / rowSums(xtab.tree)
err.class.tree <- 1 - ccr.class.tree
```


\newpage
#Random Forest

```{r Random Forest, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
set.seed(126)
library(randomForest)

## Here we need to set the argument method to "class" to ensure we get a classification rather than regression tree (since the labels in this case are numbers rather than text so could be interpreted either way).
model.rf <- randomForest(ECMO_Survival ~ . , data=train.ARDS[, -1],
                         proximity = TRUE,
                         importance = TRUE,
                         nodesize = 1,
                         replace = TRUE,
                         ntree = 1000)

## Printing the text version of the tree.
model.rf

## Looking at the graphical version of the tree.
plot(model.rf, margin=0.1)

## Prediction
pred.valid.rf <- predict(model.rf, valid.ARDS, type="class")

## Cross classification
xtab.rf <- table(data.matrix(valid.class), pred.valid.rf)
xtab.rf

## Calculating the misclassification rate.
ccr.rf <- sum(diag(xtab.rf)) / sum(xtab.rf)
err.rf <- 1 - ccr.rf

## Class-specific CCRs.
ccr.class.rf <- diag(xtab.rf) / rowSums(xtab.rf)
err.class.rf <- 1 - ccr.class.rf

## Precision / Recall
tpr.rf <- xtab.rf[1]/(xtab.rf[1] + xtab.rf[2]) #sensitivity
fpr.rf <- xtab.rf[2]/(xtab.rf[2] + xtab.rf[4]) #False Positive Rate
```


\newpage
#Support Vector Machines

```{r SVM, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
library(e1071)
set.seed(126)

model.svm <- svm(ECMO_Survival ~ . , data=train.ARDS[, -1],
                 kernel = "polynomial",
                 cost = 10,
                 decision.values = TRUE)
model.svm

## Prediction
pred.valid.svm <- predict(model.svm, valid.ARDS, type = "class")
fitted.svm  <- attributes(predict(model.svm, valid.ARDS, decision.values = TRUE))

## Cross classification
xtab.svm <- table(valid.class$ECMO_Survival, pred.valid.svm)
xtab.svm

## Class-specific CCRs.
ccr.class.svm <- diag(xtab.svm) / rowSums(xtab.svm)
err.class.svm <- 1 - ccr.class.svm

## Calculating the misclassification rate.
ccr.svm <- sum(diag(xtab.svm)) / sum(xtab.svm)
err.svm <- 1 - ccr.svm

## Precision / Recall
tpr.svm <- xtab.svm[1]/(xtab.svm[1] + xtab.svm[2]) #sensitivity
fpr.svm <- xtab.svm[2]/(xtab.svm[2] + xtab.svm[4]) #False Positive Rate
```







\newpage
#Model Selection
```{r table5, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
ccr <- rbind(ccr.logit, ccr.knn, ccr.lda, ccr.qda, ccr.rf, ccr.svm)
tpr <- rbind(tpr.logit, tpr.knn, tpr.lda, tpr.qda, tpr.rf, tpr.svm)
fpr <- rbind(fpr.logit, fpr.knn, fpr.lda, fpr.qda, fpr.rf, fpr.svm)
ccr <- cbind(ccr, fpr, tpr)

rownames(ccr) <- c("Logistic", "KNN", "LDA", "QDA", "RF", "SVM")
colnames(ccr) <- c("Accuracy", "FPR", "TPR")
ccr <- round(ccr, 3)

ccr %>%
  kable(col.names = c("Accuracy", "FPR", "TPR"),
#        row.names = c("Logistic", "KNN", "LDA", "QDA", "RF", "SVM"),
        caption = 'Accuracy on validation set', booktabs = TRUE, format = "latex") %>%
  kable_styling(font_size = 10, latex_options = "hold_position")

```


##ROC

```{r pROC, echo = FALSE, eval = FALSE, warning = FALSE, message = FALSE}
library(pROC)
ROC.svm <- roc(valid.class, pred.valid.svm)

plot(ROC.svm, col = "blue")

#par(new = TRUE)
#plot(ROC2, col = "green", xaxt = "n", yaxt = "n")
#legend("right", legend = c("RP1", "RP2"), col = c("blue", "green"), lty = 1)
```

```{r ROC, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
#library(caret)
#a <- confusionMatrix(valid.class, pred.valid.svm)
library(ROCR)
data(ROCR.simple)
df <- data.frame(ROCR.simple)
pred <- prediction(df$predictions, df$labels)

#pred <- prediction(as.numeric(pred.valid.cva$class), valid.class$ECMO_Survival)
pred.logit <- prediction(pred.valid.logit, valid.class$ECMO_Survival)
#pred.cva <- prediction(pred.valid.cva$posterior[,2], valid.class$ECMO_Survival)
pred.lda <- prediction(pred.valid.lda$posterior[,2], valid.class$ECMO_Survival)
pred.qda <- prediction(pred.valid.qda$posterior[,1], valid.class$ECMO_Survival)
pred.svm <- prediction(fitted.svm$decision.values, valid.class$ECMO_Survival)

perf.logit <- performance(pred.logit,"tpr","fpr")
perf.svm <- performance(pred.svm,"tpr","fpr")
#perf.cva <- performance(pred.cva,"tpr","fpr")
perf.lda <- performance(pred.lda,"tpr","fpr")
perf.qda <- performance(pred.qda,"tpr","fpr")



plot(perf.logit, col = "green")
#plot(perf.cva, col = "orange", add = TRUE)
plot(perf.lda, col = "red", add = TRUE)
plot(perf.qda, col = "purple", add = TRUE)
plot(perf.svm, col = "blue", add = TRUE)

abline(a=0,b=1,lwd=2,lty=2,col="gray")
```










