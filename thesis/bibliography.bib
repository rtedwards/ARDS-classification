
@article{sinha_latent_2018,
	title = {Latent class analysis of {ARDS} subphenotypes: a secondary analysis of the statins for acutely injured lungs from sepsis ({SAILS}) study},
	volume = {44},
	issn = {1432-1238},
	shorttitle = {Latent class analysis of {ARDS} subphenotypes},
	url = {https://doi.org/10.1007/s00134-018-5378-3},
	doi = {10.1007/s00134-018-5378-3},
	abstract = {PurposeUsing latent class analysis (LCA), we have consistently identified two distinct subphenotypes in four randomized controlled trial cohorts of ARDS. One subphenotype has hyper-inflammatory characteristics and is associated with worse clinical outcomes. Further, within three negative clinical trials, we observed differential treatment response by subphenotype to randomly assigned interventions. The main purpose of this study was to identify ARDS subphenotypes in a contemporary NHLBI Network trial of infection-associated ARDS (SAILS) using LCA and to test for differential treatment response to rosuvastatin therapy in the subphenotypes.MethodsLCA models were constructed using a combination of biomarker and clinical data at baseline in the SAILS study (n = 745). LCA modeling was then repeated using an expanded set of clinical class-defining variables. Subphenotypes were tested for differential treatment response to rosuvastatin.ResultsThe two-class LCA model best fit the population. Forty percent of the patients were classified as the “hyper-inflammatory” subphenotype. Including additional clinical variables in the LCA models did not identify new classes. Mortality at day 60 and day 90 was higher in the hyper-inflammatory subphenotype. No differences in outcome were observed between hyper-inflammatory patients randomized to rosuvastatin therapy versus placebo.ConclusionsLCA using a two-subphenotype model best described the SAILS population. The subphenotypes have features consistent with those previously reported in four other cohorts. Addition of new class-defining variables in the LCA model did not yield additional subphenotypes. No treatment effect was observed with rosuvastatin. These findings further validate the presence of two subphenotypes and demonstrate their utility for patient stratification in ARDS.},
	language = {en},
	number = {11},
	urldate = {2019-06-10},
	journal = {Intensive Care Medicine},
	author = {Sinha, Pratik and Delucchi, Kevin L. and Thompson, B. Taylor and McAuley, Daniel F. and Matthay, Michael A. and Calfee, Carolyn S. and {for the NHLBI ARDS Network}},
	month = nov,
	year = {2018},
	keywords = {ARDS, Latent class analysis, Statins, Subphenotypes},
	pages = {1859--1869},
	file = {Springer Full Text PDF:/Users/Berto/Zotero/storage/VTS8N8J7/Sinha et al. - 2018 - Latent class analysis of ARDS subphenotypes a sec.pdf:application/pdf}
}

@article{calfee_acute_2018,
	title = {Acute respiratory distress syndrome subphenotypes and differential response to simvastatin: secondary analysis of a randomised controlled trial},
	volume = {6},
	abstract = {Background Precision medicine approaches that target patients on the basis of disease subtype have transformed
treatment approaches to cancer, asthma, and other heterogeneous syndromes. Two distinct subphenotypes of acute
respiratory distress syndrome (ARDS) have been identified in three US-based clinical trials, and these subphenotypes
respond differently to positive end-expiratory pressure and fluid management. We aimed to investigate whether these
subphenotypes exist in non-US patient populations and respond differently to pharmacotherapies.
Methods HARP-2 was a multicentre, randomised controlled trial of simvastatin (80 mg) versus placebo done in general
intensive care units (ICUs) at 40 hospitals in the UK and Ireland within 48 h of onset of ARDS. The primary outcome
was ventilator-free days, and secondary outcomes included non-pulmonary organ failure-free days and mortality. In a
secondary analysis of HARP-2, we applied latent class analysis to baseline data without consideration of outcomes to
identify subphenotypes, and we compared clinical outcomes across subphenotypes and treatment groups.
Findings 540 patients were recruited to HARP-2. One patient withdrew consent for the use of their data, so data from
539 patients were analysed. In our secondary analysis, a two-class (two subphenotype) model was an improvement
over a one-class model (p{\textless}0·0001), with 353 (65\%) patients in the hypoinflammatory subphenotype group and
186 (35\%) in the hyperinflammatory subphenotype group. Additional classes did not improve model fit. Clinical and
biological characteristics of the two subphenotypes were similar to previous studies. Patients with the
hyperinflammatory subphenotype had fewer ventilator-free days (median 2 days [IQR 0–17] vs 18 [IQR 0–23];
p{\textless}0·0001), fewer non-pulmonary organ failure-free days (15 [0–25] vs 27 [21–28]; p{\textless}0·0001), and higher 28-day
mortality (73 [39\%] vs 59 [17\%]; p{\textless}0·0001) than did those with the hypoinflammatory subphenotype. Although
HARP-2 found no difference in 28-day survival between placebo and simvastatin, significantly different survival was
identified across patients stratified by treatment and subphenotype (p{\textless}0·0001). Specifically, within the
hyperinflammatory subphenotype, patients treated with simvastatin had significantly higher 28-day survival than did
those given placebo (p=0·008). A similar pattern was observed for 90-day survival.
Interpretation Two subphenotypes of ARDS were identified in the HARP-2 cohort, with distinct clinical and biological
features and disparate clinical outcomes. The hyperinflammatory subphenotype had improved survival with
simvastatin compared with placebo. These findings support further pursuit of predictive enrichment strategies in
critical care clinical trials.
Funding UK Efficacy and Mechanism Evaluation Programme and National Institutes of Health.},
	language = {English},
	number = {9},
	urldate = {2019-06-10},
	journal = {The Lancet Respiratory Medicine},
	author = {Calfee, Carolyn S},
	month = sep,
	year = {2018},
	pages = {691--698}
}

@article{beretta_nearest_2016,
	title = {Nearest neighbor imputation algorithms: a critical evaluation},
	volume = {16},
	issn = {1472-6947},
	shorttitle = {Nearest neighbor imputation algorithms},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959387/},
	doi = {10.1186/s12911-016-0318-z},
	abstract = {Background
Nearest neighbor (NN) imputation algorithms are efficient methods to fill in missing data where each missing value on some records is replaced by a value obtained from related cases in the whole set of records. Besides the capability to substitute the missing data with plausible values that are as close as possible to the true value, imputation algorithms should preserve the original data structure and avoid to distort the distribution of the imputed variable. Despite the efficiency of NN algorithms little is known about the effect of these methods on data structure.

Methods
Simulation on synthetic datasets with different patterns and degrees of missingness were conducted to evaluate the performance of NN with one single neighbor (1NN) and with k neighbors without (kNN) or with weighting (wkNN) in the context of different learning frameworks: plain set, reduced set after ReliefF filtering, bagging, random choice of attributes, bagging combined with random choice of attributes (Random-Forest-like method).

Results
Whatever the framework, kNN usually outperformed 1NN in terms of precision of imputation and reduced errors in inferential statistics, 1NN was however the only method capable of preserving the data structure and data were distorted even when small values of k neighbors were considered; distortion was more severe for resampling schemas.

Conclusions
The use of three neighbors in conjunction with ReliefF seems to provide the best trade-off between imputation error and preservation of the data structure. The very same conclusions can be drawn when imputation experiments were conducted on the single proton emission computed tomography (SPECTF) heart dataset after introduction of missing data completely at random.},
	number = {Suppl 3},
	urldate = {2019-06-11},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Beretta, Lorenzo and Santaniello, Alessandro},
	month = jul,
	year = {2016},
	pmid = {27454392},
	pmcid = {PMC4959387},
	file = {PubMed Central Full Text PDF:/Users/Berto/Zotero/storage/Y6EK8RYX/Beretta and Santaniello - 2016 - Nearest neighbor imputation algorithms a critical.pdf:application/pdf}
}

@inproceedings{caruana_empirical_2006,
	title = {An empirical comparison of supervised learning algorithms},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143865},
	doi = {10.1145/1143844.1143865},
	abstract = {A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90’s. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the eﬀect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods.},
	language = {en},
	urldate = {2019-07-02},
	publisher = {ACM Press},
	author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
	year = {2006},
	pages = {161--168},
	file = {Caruana and Niculescu-Mizil - 2006 - An empirical comparison of supervised learning alg.pdf:/Users/Berto/Zotero/storage/QUHQTQKK/Caruana and Niculescu-Mizil - 2006 - An empirical comparison of supervised learning alg.pdf:application/pdf}
}

@article{calfee_subphenotypes_2014,
	title = {Subphenotypes in acute respiratory distress syndrome: latent class analysis of data from two randomised controlled trials},
	volume = {2},
	issn = {22132600},
	shorttitle = {Subphenotypes in acute respiratory distress syndrome},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2213260014700979},
	doi = {10.1016/S2213-2600(14)70097-9},
	language = {en},
	number = {8},
	urldate = {2019-07-02},
	journal = {The Lancet Respiratory Medicine},
	author = {Calfee, Carolyn S and Delucchi, Kevin and Parsons, Polly E and Thompson, B Taylor and Ware, Lorraine B and Matthay, Michael A},
	month = aug,
	year = {2014},
	pages = {611--620},
	file = {Accepted Version:/Users/Berto/Zotero/storage/CA3Y52SW/Calfee et al. - 2014 - Subphenotypes in acute respiratory distress syndro.pdf:application/pdf}
}

@article{duchi_efcient_nodate,
	title = {Efﬁcient {Learning} using {Forward}-{Backward} {Splitting}},
	abstract = {We describe, analyze, and experiment with a new framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we ﬁrst perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the ﬁrst phase. This yields a simple yet effective algorithm for both batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1. We derive concrete and very simple algorithms for minimization of loss functions with ℓ1, ℓ2, ℓ22, and ℓ∞ regularization. We also show how to construct efﬁcient algorithms for mixed-norm ℓ1/ℓq regularization. We further extend the algorithms and give efﬁcient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in experiments with synthetic and natural datasets.},
	language = {en},
	author = {Duchi, John and Singer, Yoram},
	pages = {15},
	file = {Duchi and Singer - Efﬁcient Learning using Forward-Backward Splitting.pdf:/Users/Berto/Zotero/storage/EXV2K2QG/Duchi and Singer - Efﬁcient Learning using Forward-Backward Splitting.pdf:application/pdf}
}

@inproceedings{alasalmi_classification_2015,
	title = {Classification {Uncertainty} of {Multiple} {Imputed} {Data}},
	doi = {10.1109/SSCI.2015.32},
	abstract = {Every classification model contains uncertainty. This uncertainty can be distributed evenly or into certain areas of feature space. In regular classification tasks, the uncertainty can be estimated from posterior probabilities. On the other hand, if the data set contains missing values, not all classifiers can be used directly. Imputing missing values solves this problem but it suppresses variation in the data leading to underestimation of uncertainty and can also bias the results. Multiple imputation, where several copies of the data set are created, solves these problems but the classical approach for uncertainty estimation does not generalize to this case. Thus in this paper we propose a novel algorithm to estimate classification uncertainty with multiple imputed data. We show that the algorithm performs as well as the benchmark algorithm with a classifier that supports classification with missing values. It also supports the use of any classifier, even if it does not support classification with missing values, as long as it supports the estimation of posterior probabilities.},
	booktitle = {2015 {IEEE} {Symposium} {Series} on {Computational} {Intelligence}},
	author = {Alasalmi, T. and Koskimäki, H. and Suutala, J. and Röning, J.},
	month = dec,
	year = {2015},
	keywords = {ensemble, Analytical models, classification model, classification uncertainty estimation, Correlation, Data handling, Data models, imputed data classification, Machine learning algorithms, multiple imputation, pattern classification, posterior probability estimation, probability, Support vector machines, Uncertainty, classification},
	pages = {151--158},
	file = {IEEE Xplore Abstract Record:/Users/Berto/Zotero/storage/YC8LPLUB/7376605.html:text/html;IEEE Xplore Full Text PDF:/Users/Berto/Zotero/storage/655JS4SQ/Alasalmi et al. - 2015 - Classification Uncertainty of Multiple Imputed Dat.pdf:application/pdf}
}

@misc{van_buuren_flexible_nodate,
	title = {Flexible {Imputation} of {Missing} {Data}, {Second} {Edition}},
	url = {https://www.taylorfrancis.com/books/e/9780429492259},
	abstract = {Missing data pose challenges to real-life data analysis. Simple ad-hoc fixes, like deletion or mean imputation, only work under highly restrictive conditions,},
	language = {en},
	urldate = {2019-08-10},
	journal = {Taylor \& Francis},
	author = {van Buuren, Stef},
	keywords = {multiple imputation, missing data},
	file = {Snapshot:/Users/Berto/Zotero/storage/X2NADXFQ/9780429492259.html:text/html}
}

@book{van_buuren_flexible_2012,
	address = {London},
	edition = {Second Edition},
	title = {Flexible {Imputation} of {Missing} {Data}},
	abstract = {Missing data form a problem in every scientific discipline, yet the techniques required to handle them are complicated and often lacking. One of the great ideas in statistical science—multiple imputation—fills gaps in the data with plausible values, the uncertainty of which is coded in the data itself. It also solves other problems, many of which are missing data problems in disguise.},
	publisher = {Chapman \& Hall},
	author = {van Buuren, Stef},
	year = {2012}
}

@article{van_buuren_mice:_2000,
	title = {{MICE}: {Multivariate} imputation by chained equations ({S} software for missing-data imputation},
	url = {http://web.inter.nl.net/users/S.van.Buuren/mi/},
	author = {van Buuren, Stef and Oudshoom, C. G. M.},
	year = {2000}
}

@article{belanche_handling_2014,
	title = {Handling missing values in kernel methods with application to microbiology data},
	volume = {141},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231214003907},
	doi = {10.1016/j.neucom.2014.01.047},
	abstract = {We discuss several approaches that make possible for kernel methods to deal with missing values for binary variables. The first two are extended kernels able to handle missing values without data preprocessing methods. Another two methods are derived from a sophisticated multiple imputation technique involving logistic regression as local model learner. The performance of these approaches is compared using a binary data set that arises typically in microbiology (the microbial source tracking problem). We also address approaches to the largely neglected problem of prediction with missing values. Our results show that the kernel extensions demonstrate competitive performance in comparison with multiple imputation in terms of predictive accuracy. However, these results are achieved with a simpler and deterministic methodology and entail a much lower computational effort.},
	journal = {Neurocomputing},
	author = {Belanche, Lluís A. and Kobayashi, Vladimer and Aluja, Tomàs},
	month = oct,
	year = {2014},
	keywords = {Support vector machines, Binary variables, Missing values},
	pages = {110--116}
}

@article{shahid_computational_2019,
	title = {Computational intelligence techniques for medical diagnosis and prognosis: {Problems} and current developments},
	volume = {39},
	issn = {0208-5216},
	url = {http://www.sciencedirect.com/science/article/pii/S0208521619300452},
	doi = {10.1016/j.bbe.2019.05.010},
	abstract = {Diagnosis, being the first step in medical practice, is very crucial for clinical decision making. This paper investigates state-of-the-art computational intelligence (CI) techniques applied in the field of medical diagnosis and prognosis. The paper presents the performance of these techniques in diagnosing different diseases along with the detailed description of the data used. This paper includes basic as well as hybrid CI techniques that have been used in recent years so as to know the current trends in medical diagnosis domain. The paper presents the merits and demerits of different techniques in general as well as application specific context. This paper discusses some critical issues related to the medical diagnosis and prognosis such as uncertainties in the medical domain, problems in the medical data especially dealing with time-stamped (temporal) data, and knowledge acquisition. Moreover, this paper also discusses the features of good CI techniques in medical diagnosis. Overall, this review provides new insight for future research requirements in the medical diagnosis domain.},
	number = {3},
	journal = {Biocybernetics and Biomedical Engineering},
	author = {Shahid, Afzal Hussain and Singh, M.P.},
	month = jul,
	year = {2019},
	keywords = {Uncertainty, Computational intelligence, Detection, Disease diagnosis, Medical data, Prediction},
	pages = {638--672}
}

@inproceedings{al_khaldy_performance_2018,
	address = {Cham},
	title = {Performance {Analysis} of {Various} {Missing} {Value} {Imputation} {Methods} on {Heart} {Failure} {Dataset}},
	isbn = {978-3-319-56991-8},
	abstract = {The missing data issue is a fundamental challenge in terms of analyses and classification of data. The classification performance of incomplete data could be affected and produce different accuracy results compared with complete data. In this work we compare six scalable imputation methods, implemented on a Heart Failure dataset. The comparison is done by the performance metrics of three different classification methods namely J48, REPTree, and Random Forest. The aim of the research is to find a classifier that achieves best performance results after imputing the missing data using different imputation methods. The results show that in general, the Random Forest classification achieves the best results in comparison to the decision tree J48 and REP Tree. Furthermore, the performance of classification improved when imputing the missing values by concept most common (CMC) and support vector machine (SVM).},
	booktitle = {Proceedings of {SAI} {Intelligent} {Systems} {Conference} ({IntelliSys}) 2016},
	publisher = {Springer International Publishing},
	author = {Al Khaldy, Mohammad and Kambhampati, Chandrasekhar},
	editor = {Bi, Yaxin and Kapoor, Supriya and Bhatia, Rahul},
	year = {2018},
	pages = {415--425}
}

@article{rubin_inference_1976,
	title = {Inference and missing data},
	volume = {63},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/63.3.581},
	doi = {10.1093/biomet/63.3.581},
	abstract = {When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are ‘missing at random’ and the observed data are ‘observed at random’, but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is ‘distinct’ from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
	number = {3},
	urldate = {2019-08-10},
	journal = {Biometrika},
	author = {RUBIN, DONALD B.},
	month = dec,
	year = {1976},
	pages = {581--592},
	annote = {
MCAR
MAR
MNAR
}
}

@article{yeo_new_2000,
	title = {A new family of power transformations to improve normality or symmetry},
	volume = {87},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/87.4.954},
	doi = {10.1093/biomet/87.4.954},
	abstract = {We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to approximate normality. It has properties similar to those of the Box–Cox transformation for positive variables. The large‐sample properties of the transformation are investigated in the contect of a single random sample.},
	number = {4},
	urldate = {2019-08-10},
	journal = {Biometrika},
	author = {Yeo, In‐Kwon and Johnson, Richard A.},
	month = dec,
	year = {2000},
	keywords = {YeoJohnson Transformation},
	pages = {954--959},
	annote = {The Yeo-Johnson transformation}
}

@article{ben-david_about_2008,
	title = {About the relationship between {ROC} curves and {Cohen}'s kappa},
	volume = {21},
	issn = {0952-1976},
	url = {http://www.sciencedirect.com/science/article/pii/S0952197607001224},
	doi = {10.1016/j.engappai.2007.09.009},
	abstract = {Receiver operating characteristic (ROC) curves are very powerful tools for measuring classifiers’ accuracy in binary-class problems. However, their usefulness in real-world multi-class problems has not been demonstrated yet. In these frequently occurring multi-class cases, simple accuracy meters that do compensate for random successes, such as the kappa statistic, are needed. ROC curves are two-dimensional graphs. Kappa is a scalar. Each comes from an entirely different discipline. This research investigates whether they do have anything in common. A mathematical formulation that links ROC spaces with the kappa statistic is derived here for the first time. The understanding of how these two accuracy meters relate to each other can assist in a better understanding of their respective pros and cons.},
	number = {6},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Ben-David, Arie},
	month = sep,
	year = {2008},
	keywords = {Area under ROC curve (AUC), Classification accuracy, Cohen's kappa, Machine learning, ROC curves},
	pages = {874--882},
	annote = {
Relates Cohen's Kappa to ROC
Explains why Kappa is preferable in many cases
}
}

@article{zavrakidis_combining_2017,
	title = {Combining {Multiple} {Imputation} with cross-validation for calibration and assessment of {Cox} prognostic survival models},
	author = {Zavrakidis, Ioannis},
	year = {2017},
	annote = {How to combine multiple imputation and cross validation
 

Good outline of how to write my thesis
}
}

@article{belanche_handling_2014-1,
	title = {Handling missing values in kernel methods with application to microbiology data},
	volume = {141},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231214003907},
	doi = {10.1016/j.neucom.2014.01.047},
	abstract = {We discuss several approaches that make possible for kernel methods to deal with missing values. The ﬁrst two are extended kernels able to handle missing values without data preprocessing methods. Another two methods are derived from a sophisticated multiple imputation technique involving logistic regression as local model learner. The performance of these approaches is compared using a binary data set that arises typically in microbiology (the microbial source tracking problem). Our results show that the kernel extensions demonstrate competitive performance in comparison with multiple imputation in terms of predictive accuracy. However, these results are achieved with a simpler and deterministic methodology and entail a much lower computational eﬀort.},
	language = {en},
	urldate = {2019-08-15},
	journal = {Neurocomputing},
	author = {Belanche, Lluís A. and Kobayashi, Vladimer and Aluja, Tomàs},
	month = oct,
	year = {2014},
	pages = {110--116},
	annote = {Proposes 2 ways of pooling Multiple Imputation  results
MI1

Concatenate multiply imputed datasets

MI2

Train on separately imputed datasets
},
	file = {Belanche et al. - 2014 - Handling missing values in kernel methods with app.pdf:/Users/Berto/Zotero/storage/A48XAGWJ/Belanche et al. - 2014 - Handling missing values in kernel methods with app.pdf:application/pdf}
}

@incollection{bi_performance_2018,
	address = {Cham},
	title = {Performance {Analysis} of {Various} {Missing} {Value} {Imputation} {Methods} on {Heart} {Failure} {Dataset}},
	volume = {16},
	isbn = {978-3-319-56990-1 978-3-319-56991-8},
	url = {http://link.springer.com/10.1007/978-3-319-56991-8_31},
	abstract = {The missing data issue is a fundamental challenge in terms of analyses and classiﬁcation of data. The classiﬁcation performance of incomplete data could be affected and produce different accuracy results compared with complete data. In this work we compare six scalable imputation methods, implemented on a Heart Failure dataset. The comparison is done by the performance metrics of three different classiﬁcation methods namely J48, REPTree, and Random Forest. The aim of the research is to ﬁnd a classiﬁer that achieves best performance results after imputing the missing data using different imputation methods. The results show that in general, the Random Forest classiﬁcation achieves the best results in comparison to the decision tree J48 and REP Tree. Furthermore, the performance of classiﬁcation improved when imputing the missing values by concept most common (CMC) and support vector machine (SVM).},
	language = {en},
	urldate = {2019-08-15},
	booktitle = {Proceedings of {SAI} {Intelligent} {Systems} {Conference} ({IntelliSys}) 2016},
	publisher = {Springer International Publishing},
	author = {Al Khaldy, Mohammad and Kambhampati, Chandrasekhar},
	editor = {Bi, Yaxin and Kapoor, Supriya and Bhatia, Rahul},
	year = {2018},
	doi = {10.1007/978-3-319-56991-8_31},
	pages = {415--425},
	annote = {Tries various imputation methods on a heart dataset},
	file = {Al Khaldy and Kambhampati - 2018 - Performance Analysis of Various Missing Value Impu.pdf:/Users/Berto/Zotero/storage/JF2JWQYB/Al Khaldy and Kambhampati - 2018 - Performance Analysis of Various Missing Value Impu.pdf:application/pdf}
}

@inproceedings{alasalmi_classification_2015-1,
	title = {Classification {Uncertainty} of {Multiple} {Imputed} {Data}},
	doi = {10.1109/SSCI.2015.32},
	abstract = {Every classification model contains uncertainty. This uncertainty can be distributed evenly or into certain areas of feature space. In regular classification tasks, the uncertainty can be estimated from posterior probabilities. On the other hand, if the data set contains missing values, not all classifiers can be used directly. Imputing missing values solves this problem but it suppresses variation in the data leading to underestimation of uncertainty and can also bias the results. Multiple imputation, where several copies of the data set are created, solves these problems but the classical approach for uncertainty estimation does not generalize to this case. Thus in this paper we propose a novel algorithm to estimate classification uncertainty with multiple imputed data. We show that the algorithm performs as well as the benchmark algorithm with a classifier that supports classification with missing values. It also supports the use of any classifier, even if it does not support classification with missing values, as long as it supports the estimation of posterior probabilities.},
	booktitle = {2015 {IEEE} {Symposium} {Series} on {Computational} {Intelligence}},
	author = {Alasalmi, T. and Koskimäki, H. and Suutala, J. and Röning, J.},
	month = dec,
	year = {2015},
	keywords = {Analytical models, classification model, classification uncertainty estimation, Correlation, Data handling, Data models, imputed data classification, Machine learning algorithms, multiple imputation, pattern classification, posterior probability estimation, probability, Support vector machines, Uncertainty},
	pages = {151--158},
	file = {IEEE Xplore Abstract Record:/Users/Berto/Zotero/storage/NWJLFU3P/7376605.html:text/html}
}

@article{fisher_use_1936,
	title = {{THE} {USE} {OF} {MULTIPLE} {MEASUREMENTS} {IN} {TAXONOMIC} {PROBLEMS}},
	volume = {7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	number = {2},
	journal = {Annals of Eugenics},
	author = {FISHER, R. A.},
	year = {1936},
	pages = {179--188},
	annote = {Linear Discriminant Analysis}
}

@article{cover_geometrical_1965,
	title = {Geometrical and {Statistical} {Properties} of {Systems} of {Linear} {Inequalities} with {Applications} in {Pattern} {Recognition}},
	volume = {EC-14},
	issn = {0367-7508},
	doi = {10.1109/PGEC.1965.264137},
	abstract = {This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces.},
	number = {3},
	journal = {IEEE Transactions on Electronic Computers},
	author = {Cover, T. M.},
	month = jun,
	year = {1965},
	keywords = {Application software, Boolean functions, Geometry, History, Pattern recognition, Vectors},
	pages = {326--334},
	annote = {Quadratic Discriminant Analysis
 
Proposes nonlinear discriminant functions},
	file = {IEEE Xplore Abstract Record:/Users/Berto/Zotero/storage/PP9P5HYG/metrics.html:text/html;IEEE Xplore Full Text PDF:/Users/Berto/Zotero/storage/NBQUNK76/Cover - 1965 - Geometrical and Statistical Properties of Systems .pdf:application/pdf}
}

@article{cohen_coefficient_1960,
	title = {A {Coefficient} of {Agreement} for {Nominal} {Scales}},
	volume = {20},
	url = {https://doi.org/10.1177/001316446002000104},
	doi = {10.1177/001316446002000104},
	number = {1},
	journal = {Educational and Psychological Measurement},
	author = {Cohen, Jacob},
	year = {1960},
	pages = {37--46},
	annote = {Seminal paper for Coehn's Kappa
 }
}

@article{penrose_elementary_1946,
	title = {The {Elementary} {Statistics} of {Majority} {Voting}},
	volume = {109},
	issn = {09528385},
	url = {http://www.jstor.org/stable/2981392},
	doi = {10.2307/2981392},
	number = {1},
	journal = {Journal of the Royal Statistical Society},
	author = {Penrose, L. S.},
	year = {1946},
	pages = {53--57}
}

@article{lam_optimal_1995,
	title = {Optimal combinations of pattern classifiers},
	volume = {16},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/016786559500050Q},
	doi = {10.1016/0167-8655(95)00050-Q},
	abstract = {To improve recognition results, decisions of multiple classifiers can be combined. We study the performance of combination methods that are variations of the majority vote. A Bayesian formulation and a weighted majority vote (with weights obtained through a genetic algorithm) are implemented, and the combined performances of 7 classifiers on a large set of handwritten numerals are analyzed.},
	number = {9},
	journal = {Pattern Recognition Letters},
	author = {Lam, Louisa and Suen, Ching Y.},
	month = sep,
	year = {1995},
	keywords = {Bayesian method, Genetic algorithm, Majority vote, Multiple classifier systems, OCR},
	pages = {945--954}
}

@inproceedings{kittler_combining_1996,
	title = {Combining classifiers},
	volume = {2},
	doi = {10.1109/ICPR.1996.547205},
	booktitle = {Proceedings of 13th {International} {Conference} on {Pattern} {Recognition}},
	author = {Kittler, J. and Hater, M. and Duin, R. P. W.},
	month = aug,
	year = {1996},
	keywords = {classifier combination, compound classification, Decision making, Estimation error, Extraterrestrial measurements, Machinery, pattern classification, Pattern recognition, pattern representations, Physics, sensitivity analysis, Sensitivity analysis, sum rule},
	pages = {897--901 vol.2}
}

@inproceedings{kittler_combining_1996-1,
	title = {Combining classifiers},
	volume = {2},
	doi = {10.1109/ICPR.1996.547205},
	booktitle = {Proceedings of 13th {International} {Conference} on {Pattern} {Recognition}},
	author = {Kittler, J. and Hater, M. and Duin, R. P. W.},
	month = aug,
	year = {1996},
	keywords = {classifier combination, compound classification, Decision making, Estimation error, Extraterrestrial measurements, Machinery, pattern classification, Pattern recognition, pattern representations, Physics, sensitivity analysis, Sensitivity analysis, sum rule},
	pages = {897--901 vol.2},
	annote = {An experimental comparison of various class\$er combinationschemes demonstrates that the combination rule developedunder the most restrictive assumptions - the sum rule- and its derivatives consistently outpevorm other classifiercnmbinutions schemes.}
}

@article{alexandre_combining_2001,
	title = {On combining classifiers using sum and product rules},
	volume = {22},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865501000733},
	doi = {10.1016/S0167-8655(01)00073-3},
	abstract = {This paper presents a comparative study of the performance of arithmetic and geometric means as rules to combine multiple classifiers. For problems with two classes, we prove that these combination rules are equivalent when using two classifiers and the sum of the estimates of the a posteriori probabilities is equal to one. We also prove that the case of a two class problem and a combination of two classifiers is the only one where such equivalence occurs. We present experiments illustrating the equivalence of the rules under the above mentioned assumptions.},
	number = {12},
	journal = {Selected Papers from the 11th Portuguese Conference on Pattern Recognition - RECPAD2000},
	author = {Alexandre, Luís A. and Campilho, Aurélio C. and Kamel, Mohamed},
	month = oct,
	year = {2001},
	keywords = {Arithmetic mean, Classification, Classifier fusion, Combining classifiers, nearest-neighbours, Neural networks, sum rule},
	pages = {1283--1289}
}

@book{james_majority_1998,
	title = {Majority {Vote} {Classifiers}: {Theory} and {Applications}},
	author = {James, Gareth},
	year = {1998},
	annote = {
Good explanations of methods
}
}

@inproceedings{tang_when_2018,
	title = {When do random forests fail?},
	author = {Tang, Cheng and Garreau, Damien and von Luxburg, Ulrike},
	year = {2018},
	annote = {Random forests fail with sparsity and without sufficient subsampling}
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	number = {3},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	pages = {273--297}
}

@misc{hsu_practical_2016,
	title = {A {Practical} {Guide} to {Support} {Vector} {Classification}},
	abstract = {The  support  vector  machine  (SVM)  is  a  popular  classification  technique.However,  beginners  who  are  not  familiar  with  SVM  often  get  unsatisfactoryresults since they miss some easy but significant steps. In this guide, we proposea simple procedure which usually gives reasonable results.},
	publisher = {Department of Computer Science, National Taiwan University},
	author = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen},
	year = {2016},
	annote = {Tuning SVM}
}

@article{franklin_elements_2005,
	title = {The elements of statistical learning: data mining, inference and prediction},
	volume = {27},
	issn = {0343-6993},
	url = {https://doi.org/10.1007/BF02985802},
	doi = {10.1007/BF02985802},
	number = {2},
	journal = {The Mathematical Intelligencer},
	author = {Franklin, James},
	month = mar,
	year = {2005},
	pages = {83--85}
}

@book{hastie_elements_2009,
	series = {Springer series in statistics},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-84884-6},
	url = {https://books.google.co.uk/books?id=eBSgoAEACAAJ},
	publisher = {Springer},
	author = {Hastie, T. and Tibshirani, R. and Friedman, J.H.},
	year = {2009},
	lccn = {2008941148}
}

@inproceedings{song_feature_2010,
	title = {Feature {Selection} {Using} {Principal} {Component} {Analysis}},
	volume = {1},
	doi = {10.1109/ICSEM.2010.14},
	booktitle = {2010 {International} {Conference} on {System} {Science}, {Engineering} {Design} and {Manufacturing} {Informatization}},
	author = {Song, F. and Guo, Z. and Mei, D.},
	month = nov,
	year = {2010},
	keywords = {computer science, covariance matrices, covariance matrix, Databases, eigenvalues and eigenfunctions, eigenvector, Face, face recognition, Face recognition, feature extraction, Feature extraction, feature selection, numerical analysis, principal component analysis, Principal component analysis, transform method, transforms, Transforms},
	pages = {27--30}
}

@inproceedings{wang_feature_2009,
	title = {Feature {Selection} for {Maximizing} the {Area} {Under} the {ROC} {Curve}},
	doi = {10.1109/ICDMW.2009.25},
	booktitle = {2009 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops}},
	author = {Wang, R. and Tang, K.},
	month = dec,
	year = {2009},
	keywords = {ARCO algorithm, area under the ROC curve algorithm, Computational complexity, Computational efficiency, Computer applications, Conferences, Costs, data mining, Data mining, feature selection, Laboratories, learning (artificial intelligence), learning algorithms, Learning systems, minimal redundancy-maximal-relevance method, rank correlation coefficient optimization, Support vector machine classification, Support vector machines},
	pages = {400--405}
}

@article{wood_how_2008,
	title = {How should variable selection be performed with multiply imputed data?},
	volume = {27},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3177},
	doi = {10.1002/sim.3177},
	abstract = {Abstract Multiple imputation is a popular technique for analysing incomplete data. Given the imputed data and a particular model, Rubin's rules (RR) for estimating parameters and standard errors are well established. However, there are currently no guidelines for variable selection in multiply imputed data sets. The usual practice is to perform variable selection amongst the complete cases, a simple but inefficient and potentially biased procedure. Alternatively, variable selection can be performed by repeated use of RR, which is more computationally demanding. An approximation can be obtained by a simple ‘stacked’ method that combines the multiply imputed data sets into one and uses a weighting scheme to account for the fraction of missing data in each covariate. We compare these and other approaches using simulations based around a trial in community psychiatry. Most methods improve on the naïve complete-case analysis for variable selection, but importantly the type 1 error is only preserved if selection is based on RR, which is our recommended approach. Copyright © 2008 John Wiley \& Sons, Ltd.},
	number = {17},
	journal = {Statistics in Medicine},
	author = {Wood, Angela M. and White, Ian R. and Royston, Patrick},
	year = {2008},
	keywords = {multiple imputation, multiply imputed data, stacked data, stepwise, variable selection},
	pages = {3227--3246}
}

@article{kemp_applied_2003,
	title = {Applied {Multiple} {Regression}/{Correlation} {Analysis} for the {Behavioral} {Sciences}},
	volume = {52},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1046/j.1467-9884.2003.t01-2-00383_4.x},
	doi = {10.1046/j.1467-9884.2003.t01-2-00383_4.x},
	number = {4},
	journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
	author = {Kemp, Freda},
	year = {2003},
	pages = {691--691}
}

@article{altman_bootstrap_1989,
	title = {Bootstrap investigation of the stability of a cox regression model},
	volume = {8},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780080702},
	doi = {10.1002/sim.4780080702},
	abstract = {Abstract We describe a bootstrap investigation of the stability of a Cox proportional hazards regression model resulting from the analysis of a clinical trial of azathioprine versus placebo in patients with primary biliary cirrhosis. We have considered stability to refer both to the choice of variables included in the model and, more importantly, to the predictive ability of the model. In stepwise Cox regression analyses of 100 bootstrap samples using 17 candidate variables, the most frequently selected variables were those selected in the original analysis, and no other important variable was identified. Thus there was no reason to doubt the model obtained in the original analysis. For each patient in the trial, bootstrap confidence intervals were constructed for the estimated probability of surviving two years. It is shown graphically that these intervals are markedly wider than those obtained from the original model.},
	number = {7},
	journal = {Statistics in Medicine},
	author = {Altman, Douglas G. and Andersen, Per Kragh},
	year = {1989},
	keywords = {Bootstrap, Cox proportional hazards regression model, Model selection, Prediction, Primary biliary cirrhosis},
	pages = {771--783}
}

@book{altman_practical_1991,
	title = {Practical {Statistics} for {Medical} {Research}},
	volume = {Chapter 12},
	publisher = {Chapman \& Hall: London},
	author = {Altman, DG},
	year = {1991}
}

@article{derksen_backward_1992,
	title = {Backward, forward and stepwise automated subset selection algorithms: {Frequency} of obtaining authentic and noise variables},
	volume = {45},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1992.tb00992.x},
	doi = {10.1111/j.2044-8317.1992.tb00992.x},
	abstract = {The use of automated subset search algorithms is reviewed and issues concerning model selection and selection criteria are discussed. In addition, a Monte Carlo study is reported which presents data regarding the frequency with which authentic and noise variables are selected by automated subset algorithms. In particular, the effects of the correlation between predictor variables, the number of candidate predictor variables, the size of the sample, and the level of significance for entry and deletion of variables were studied for three automated subset algorithms: BACKWARD ELIMINATION, FORWARD SELECTION, and STEPWISE. Results indicated that: (1) the degree of correlation between the predictor variables affected the frequency with which authentic predictor variables found their way into the final model; (2) the number of candidate predictor variables affected the number of noise variables that gained entry to the model; (3) the size of the sample was of little practical importance in determining the number of authentic variables contained in the final model; and (4) the population multiple coefficient of determination could be faithfully estimated by adopting a statistic that is adjusted by the total number of candidate predictor variables rather than the number of variables in the final model.},
	number = {2},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Derksen, Shelley and Keselman, H. J.},
	year = {1992},
	pages = {265--282}
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {58},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x},
	doi = {10.1111/j.2517-6161.1996.tb02080.x},
	abstract = {SUMMARY We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	keywords = {quadratic programming, regression, shrinkage, subset selection},
	pages = {267--288}
}

@article{trendafilov_dalass:_2007,
	title = {{DALASS}: {Variable} selection in discriminant analysis via the {LASSO}},
	volume = {51},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947306005032},
	doi = {https://doi.org/10.1016/j.csda.2006.12.046},
	abstract = {The objective of DALASS is to simplify the interpretation of Fisher's discriminant function coefficients. The DALASS problem—discriminant analysis (DA) modified so that the canonical variates satisfy the LASSO constraint—is formulated as a dynamical system on the unit sphere. Both standard and orthogonal canonical variates are considered. The globally convergent continuous-time algorithms are illustrated numerically and applied to some well-known data sets.},
	number = {8},
	journal = {Computational Statistics \& Data Analysis},
	author = {Trendafilov, Nickolay T. and Jolliffe, Ian T.},
	year = {2007},
	keywords = {Canonical variates, Continuous-time constrained optimization, LASSO constraint, Orthogonal canonical variates, Penalty function, Steepest ascent vector flows on manifolds},
	pages = {3718 -- 3736}
}

@article{clemmensen_sparse_2011,
	title = {Sparse {Discriminant} {Analysis}},
	volume = {53},
	url = {https://doi.org/10.1198/TECH.2011.08118},
	doi = {10.1198/TECH.2011.08118},
	number = {4},
	journal = {Technometrics},
	author = {Clemmensen, Line and Hastie, Trevor and Witten, Daniela and Ersbøll, Bjarne},
	year = {2011},
	pages = {406--413}
}

@article{guyon_gene_2002,
	title = {Gene {Selection} for {Cancer} {Classification} using {Support} {Vector} {Machines}},
	volume = {46},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1012487302797},
	doi = {10.1023/A:1012487302797},
	abstract = {DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues.},
	number = {1},
	journal = {Machine Learning},
	author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
	month = jan,
	year = {2002},
	pages = {389--422},
	annote = {
Recursive Feature Selection
}
}

@article{f.r.s_liii._1901,
	title = {{LIII}. {On} lines and planes of closest fit to systems of points in space},
	volume = {2},
	url = {https://doi.org/10.1080/14786440109462720},
	doi = {10.1080/14786440109462720},
	number = {11},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {F.R.S, Karl Pearson},
	year = {1901},
	pages = {559--572},
	annote = {Principal Component Analysis}
}

@incollection{little_bayes_2014,
	title = {Bayes and {Multiple} {Imputation}},
	isbn = {978-1-119-01356-3},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119013563.ch10},
	abstract = {Summary When sample sizes are small, a useful alternative approach to multiple imputation (ML) is to add a prior distribution for the parameters and compute the posterior distribution of the parameters of interest. As with ML estimation with a general pattern of missing values, Bayes simulation requires iteration. The iterative simulation methods discussed eventually create draws from the posterior distribution of θ. However, the five draws of Ymis can be quite adequate for generating MI inferences, provided the fraction of missing information is modest, as when the fractions of cases with Y1 or Y2 missing are limited.},
	booktitle = {Statistical {Analysis} with {Missing} {Data}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Little, Roderick J. A. and Rubin, Donald B.},
	year = {2014},
	doi = {10.1002/9781119013563.ch10},
	keywords = {Bayes, multiple imputation (ML)},
	pages = {200--220},
	annote = {Mean Imputationdistorts the distribution of the data by reducing the variance of the imputed variables and the correlations between variables}
}

@article{efron_efficiency_1975,
	title = {The {Efficiency} of {Logistic} {Regression} {Compared} to {Normal} {Discriminant} {Analysis}},
	volume = {70},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1975.10480319},
	doi = {10.1080/01621459.1975.10480319},
	number = {352},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	year = {1975},
	pages = {892--898},
	annote = {LDA can perform better than logistic regression when assumptions met.}
}