
@article{sinha_latent_2018,
	title = {Latent class analysis of {ARDS} subphenotypes: a secondary analysis of the statins for acutely injured lungs from sepsis ({SAILS}) study},
	volume = {44},
	issn = {1432-1238},
	shorttitle = {Latent class analysis of {ARDS} subphenotypes},
	url = {https://doi.org/10.1007/s00134-018-5378-3},
	doi = {10.1007/s00134-018-5378-3},
	abstract = {PurposeUsing latent class analysis (LCA), we have consistently identified two distinct subphenotypes in four randomized controlled trial cohorts of ARDS. One subphenotype has hyper-inflammatory characteristics and is associated with worse clinical outcomes. Further, within three negative clinical trials, we observed differential treatment response by subphenotype to randomly assigned interventions. The main purpose of this study was to identify ARDS subphenotypes in a contemporary NHLBI Network trial of infection-associated ARDS (SAILS) using LCA and to test for differential treatment response to rosuvastatin therapy in the subphenotypes.MethodsLCA models were constructed using a combination of biomarker and clinical data at baseline in the SAILS study (n = 745). LCA modeling was then repeated using an expanded set of clinical class-defining variables. Subphenotypes were tested for differential treatment response to rosuvastatin.ResultsThe two-class LCA model best fit the population. Forty percent of the patients were classified as the “hyper-inflammatory” subphenotype. Including additional clinical variables in the LCA models did not identify new classes. Mortality at day 60 and day 90 was higher in the hyper-inflammatory subphenotype. No differences in outcome were observed between hyper-inflammatory patients randomized to rosuvastatin therapy versus placebo.ConclusionsLCA using a two-subphenotype model best described the SAILS population. The subphenotypes have features consistent with those previously reported in four other cohorts. Addition of new class-defining variables in the LCA model did not yield additional subphenotypes. No treatment effect was observed with rosuvastatin. These findings further validate the presence of two subphenotypes and demonstrate their utility for patient stratification in ARDS.},
	language = {en},
	number = {11},
	urldate = {2019-06-10},
	journal = {Intensive Care Medicine},
	author = {Sinha, Pratik and Delucchi, Kevin L. and Thompson, B. Taylor and McAuley, Daniel F. and Matthay, Michael A. and Calfee, Carolyn S. and {for the NHLBI ARDS Network}},
	month = nov,
	year = {2018},
	keywords = {ARDS, Latent class analysis, Statins, Subphenotypes},
	pages = {1859--1869},
	file = {Springer Full Text PDF:/Users/Berto/Zotero/storage/VTS8N8J7/Sinha et al. - 2018 - Latent class analysis of ARDS subphenotypes a sec.pdf:application/pdf}
}

@article{calfee_acute_2018,
	title = {Acute respiratory distress syndrome subphenotypes and differential response to simvastatin: secondary analysis of a randomised controlled trial},
	volume = {6},
	abstract = {Background Precision medicine approaches that target patients on the basis of disease subtype have transformed
treatment approaches to cancer, asthma, and other heterogeneous syndromes. Two distinct subphenotypes of acute
respiratory distress syndrome (ARDS) have been identified in three US-based clinical trials, and these subphenotypes
respond differently to positive end-expiratory pressure and fluid management. We aimed to investigate whether these
subphenotypes exist in non-US patient populations and respond differently to pharmacotherapies.
Methods HARP-2 was a multicentre, randomised controlled trial of simvastatin (80 mg) versus placebo done in general
intensive care units (ICUs) at 40 hospitals in the UK and Ireland within 48 h of onset of ARDS. The primary outcome
was ventilator-free days, and secondary outcomes included non-pulmonary organ failure-free days and mortality. In a
secondary analysis of HARP-2, we applied latent class analysis to baseline data without consideration of outcomes to
identify subphenotypes, and we compared clinical outcomes across subphenotypes and treatment groups.
Findings 540 patients were recruited to HARP-2. One patient withdrew consent for the use of their data, so data from
539 patients were analysed. In our secondary analysis, a two-class (two subphenotype) model was an improvement
over a one-class model (p{\textless}0·0001), with 353 (65\%) patients in the hypoinflammatory subphenotype group and
186 (35\%) in the hyperinflammatory subphenotype group. Additional classes did not improve model fit. Clinical and
biological characteristics of the two subphenotypes were similar to previous studies. Patients with the
hyperinflammatory subphenotype had fewer ventilator-free days (median 2 days [IQR 0–17] vs 18 [IQR 0–23];
p{\textless}0·0001), fewer non-pulmonary organ failure-free days (15 [0–25] vs 27 [21–28]; p{\textless}0·0001), and higher 28-day
mortality (73 [39\%] vs 59 [17\%]; p{\textless}0·0001) than did those with the hypoinflammatory subphenotype. Although
HARP-2 found no difference in 28-day survival between placebo and simvastatin, significantly different survival was
identified across patients stratified by treatment and subphenotype (p{\textless}0·0001). Specifically, within the
hyperinflammatory subphenotype, patients treated with simvastatin had significantly higher 28-day survival than did
those given placebo (p=0·008). A similar pattern was observed for 90-day survival.
Interpretation Two subphenotypes of ARDS were identified in the HARP-2 cohort, with distinct clinical and biological
features and disparate clinical outcomes. The hyperinflammatory subphenotype had improved survival with
simvastatin compared with placebo. These findings support further pursuit of predictive enrichment strategies in
critical care clinical trials.
Funding UK Efficacy and Mechanism Evaluation Programme and National Institutes of Health.},
	language = {English},
	number = {9},
	urldate = {2019-06-10},
	journal = {The Lancet Respiratory Medicine},
	author = {Calfee, Carolyn S},
	month = sep,
	year = {2018},
	pages = {691--698}
}

@article{beretta_nearest_2016,
	title = {Nearest neighbor imputation algorithms: a critical evaluation},
	volume = {16},
	issn = {1472-6947},
	shorttitle = {Nearest neighbor imputation algorithms},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959387/},
	doi = {10.1186/s12911-016-0318-z},
	abstract = {Background
Nearest neighbor (NN) imputation algorithms are efficient methods to fill in missing data where each missing value on some records is replaced by a value obtained from related cases in the whole set of records. Besides the capability to substitute the missing data with plausible values that are as close as possible to the true value, imputation algorithms should preserve the original data structure and avoid to distort the distribution of the imputed variable. Despite the efficiency of NN algorithms little is known about the effect of these methods on data structure.

Methods
Simulation on synthetic datasets with different patterns and degrees of missingness were conducted to evaluate the performance of NN with one single neighbor (1NN) and with k neighbors without (kNN) or with weighting (wkNN) in the context of different learning frameworks: plain set, reduced set after ReliefF filtering, bagging, random choice of attributes, bagging combined with random choice of attributes (Random-Forest-like method).

Results
Whatever the framework, kNN usually outperformed 1NN in terms of precision of imputation and reduced errors in inferential statistics, 1NN was however the only method capable of preserving the data structure and data were distorted even when small values of k neighbors were considered; distortion was more severe for resampling schemas.

Conclusions
The use of three neighbors in conjunction with ReliefF seems to provide the best trade-off between imputation error and preservation of the data structure. The very same conclusions can be drawn when imputation experiments were conducted on the single proton emission computed tomography (SPECTF) heart dataset after introduction of missing data completely at random.},
	number = {Suppl 3},
	urldate = {2019-06-11},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Beretta, Lorenzo and Santaniello, Alessandro},
	month = jul,
	year = {2016},
	pmid = {27454392},
	pmcid = {PMC4959387},
	file = {PubMed Central Full Text PDF:/Users/Berto/Zotero/storage/Y6EK8RYX/Beretta and Santaniello - 2016 - Nearest neighbor imputation algorithms a critical.pdf:application/pdf}
}

@inproceedings{caruana_empirical_2006,
	title = {An empirical comparison of supervised learning algorithms},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143865},
	doi = {10.1145/1143844.1143865},
	abstract = {A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90’s. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the eﬀect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods.},
	language = {en},
	urldate = {2019-07-02},
	publisher = {ACM Press},
	author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
	year = {2006},
	pages = {161--168},
	file = {Caruana and Niculescu-Mizil - 2006 - An empirical comparison of supervised learning alg.pdf:/Users/Berto/Zotero/storage/QUHQTQKK/Caruana and Niculescu-Mizil - 2006 - An empirical comparison of supervised learning alg.pdf:application/pdf}
}

@article{calfee_subphenotypes_2014,
	title = {Subphenotypes in acute respiratory distress syndrome: latent class analysis of data from two randomised controlled trials},
	volume = {2},
	issn = {22132600},
	shorttitle = {Subphenotypes in acute respiratory distress syndrome},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2213260014700979},
	doi = {10.1016/S2213-2600(14)70097-9},
	language = {en},
	number = {8},
	urldate = {2019-07-02},
	journal = {The Lancet Respiratory Medicine},
	author = {Calfee, Carolyn S and Delucchi, Kevin and Parsons, Polly E and Thompson, B Taylor and Ware, Lorraine B and Matthay, Michael A},
	month = aug,
	year = {2014},
	pages = {611--620},
	file = {Accepted Version:/Users/Berto/Zotero/storage/CA3Y52SW/Calfee et al. - 2014 - Subphenotypes in acute respiratory distress syndro.pdf:application/pdf}
}

@article{duchi_efcient_nodate,
	title = {Efﬁcient {Learning} using {Forward}-{Backward} {Splitting}},
	abstract = {We describe, analyze, and experiment with a new framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we ﬁrst perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the ﬁrst phase. This yields a simple yet effective algorithm for both batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1. We derive concrete and very simple algorithms for minimization of loss functions with ℓ1, ℓ2, ℓ22, and ℓ∞ regularization. We also show how to construct efﬁcient algorithms for mixed-norm ℓ1/ℓq regularization. We further extend the algorithms and give efﬁcient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in experiments with synthetic and natural datasets.},
	language = {en},
	author = {Duchi, John and Singer, Yoram},
	pages = {15},
	file = {Duchi and Singer - Efﬁcient Learning using Forward-Backward Splitting.pdf:/Users/Berto/Zotero/storage/EXV2K2QG/Duchi and Singer - Efﬁcient Learning using Forward-Backward Splitting.pdf:application/pdf}
}

@inproceedings{alasalmi_classification_2015,
	title = {Classification {Uncertainty} of {Multiple} {Imputed} {Data}},
	doi = {10.1109/SSCI.2015.32},
	abstract = {Every classification model contains uncertainty. This uncertainty can be distributed evenly or into certain areas of feature space. In regular classification tasks, the uncertainty can be estimated from posterior probabilities. On the other hand, if the data set contains missing values, not all classifiers can be used directly. Imputing missing values solves this problem but it suppresses variation in the data leading to underestimation of uncertainty and can also bias the results. Multiple imputation, where several copies of the data set are created, solves these problems but the classical approach for uncertainty estimation does not generalize to this case. Thus in this paper we propose a novel algorithm to estimate classification uncertainty with multiple imputed data. We show that the algorithm performs as well as the benchmark algorithm with a classifier that supports classification with missing values. It also supports the use of any classifier, even if it does not support classification with missing values, as long as it supports the estimation of posterior probabilities.},
	booktitle = {2015 {IEEE} {Symposium} {Series} on {Computational} {Intelligence}},
	author = {Alasalmi, T. and Koskimäki, H. and Suutala, J. and Röning, J.},
	month = dec,
	year = {2015},
	keywords = {ensemble, Analytical models, classification model, classification uncertainty estimation, Correlation, Data handling, Data models, imputed data classification, Machine learning algorithms, multiple imputation, pattern classification, posterior probability estimation, probability, Support vector machines, Uncertainty, classification},
	pages = {151--158},
	file = {IEEE Xplore Abstract Record:/Users/Berto/Zotero/storage/YC8LPLUB/7376605.html:text/html;IEEE Xplore Full Text PDF:/Users/Berto/Zotero/storage/655JS4SQ/Alasalmi et al. - 2015 - Classification Uncertainty of Multiple Imputed Dat.pdf:application/pdf}
}

@misc{van_buuren_flexible_nodate,
	title = {Flexible {Imputation} of {Missing} {Data}, {Second} {Edition}},
	url = {https://www.taylorfrancis.com/books/e/9780429492259},
	abstract = {Missing data pose challenges to real-life data analysis. Simple ad-hoc fixes, like deletion or mean imputation, only work under highly restrictive conditions,},
	language = {en},
	urldate = {2019-08-10},
	journal = {Taylor \& Francis},
	author = {van Buuren, Stef},
	keywords = {multiple imputation, missing data},
	file = {Snapshot:/Users/Berto/Zotero/storage/X2NADXFQ/9780429492259.html:text/html}
}

@book{van_buuren_flexible_2012,
	address = {London},
	edition = {Second Edition},
	title = {Flexible {Imputation} of {Missing} {Data}},
	abstract = {Missing data form a problem in every scientific discipline, yet the techniques required to handle them are complicated and often lacking. One of the great ideas in statistical science—multiple imputation—fills gaps in the data with plausible values, the uncertainty of which is coded in the data itself. It also solves other problems, many of which are missing data problems in disguise.},
	publisher = {Chapman \& Hall},
	author = {van Buuren, Stef},
	year = {2012}
}

@article{van_buuren_mice:_2000,
	title = {{MICE}: {Multivariate} imputation by chained equations ({S} software for missing-data imputation},
	url = {http://web.inter.nl.net/users/S.van.Buuren/mi/},
	author = {van Buuren, Stef and Oudshoom, C. G. M.},
	year = {2000}
}

@article{belanche_handling_2014,
	title = {Handling missing values in kernel methods with application to microbiology data},
	volume = {141},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231214003907},
	doi = {10.1016/j.neucom.2014.01.047},
	abstract = {We discuss several approaches that make possible for kernel methods to deal with missing values for binary variables. The first two are extended kernels able to handle missing values without data preprocessing methods. Another two methods are derived from a sophisticated multiple imputation technique involving logistic regression as local model learner. The performance of these approaches is compared using a binary data set that arises typically in microbiology (the microbial source tracking problem). We also address approaches to the largely neglected problem of prediction with missing values. Our results show that the kernel extensions demonstrate competitive performance in comparison with multiple imputation in terms of predictive accuracy. However, these results are achieved with a simpler and deterministic methodology and entail a much lower computational effort.},
	journal = {Neurocomputing},
	author = {Belanche, Lluís A. and Kobayashi, Vladimer and Aluja, Tomàs},
	month = oct,
	year = {2014},
	keywords = {Support vector machines, Binary variables, Missing values},
	pages = {110--116}
}

@article{shahid_computational_2019,
	title = {Computational intelligence techniques for medical diagnosis and prognosis: {Problems} and current developments},
	volume = {39},
	issn = {0208-5216},
	url = {http://www.sciencedirect.com/science/article/pii/S0208521619300452},
	doi = {10.1016/j.bbe.2019.05.010},
	abstract = {Diagnosis, being the first step in medical practice, is very crucial for clinical decision making. This paper investigates state-of-the-art computational intelligence (CI) techniques applied in the field of medical diagnosis and prognosis. The paper presents the performance of these techniques in diagnosing different diseases along with the detailed description of the data used. This paper includes basic as well as hybrid CI techniques that have been used in recent years so as to know the current trends in medical diagnosis domain. The paper presents the merits and demerits of different techniques in general as well as application specific context. This paper discusses some critical issues related to the medical diagnosis and prognosis such as uncertainties in the medical domain, problems in the medical data especially dealing with time-stamped (temporal) data, and knowledge acquisition. Moreover, this paper also discusses the features of good CI techniques in medical diagnosis. Overall, this review provides new insight for future research requirements in the medical diagnosis domain.},
	number = {3},
	journal = {Biocybernetics and Biomedical Engineering},
	author = {Shahid, Afzal Hussain and Singh, M.P.},
	month = jul,
	year = {2019},
	keywords = {Uncertainty, Computational intelligence, Detection, Disease diagnosis, Medical data, Prediction},
	pages = {638--672}
}

@inproceedings{al_khaldy_performance_2018,
	address = {Cham},
	title = {Performance {Analysis} of {Various} {Missing} {Value} {Imputation} {Methods} on {Heart} {Failure} {Dataset}},
	isbn = {978-3-319-56991-8},
	abstract = {The missing data issue is a fundamental challenge in terms of analyses and classification of data. The classification performance of incomplete data could be affected and produce different accuracy results compared with complete data. In this work we compare six scalable imputation methods, implemented on a Heart Failure dataset. The comparison is done by the performance metrics of three different classification methods namely J48, REPTree, and Random Forest. The aim of the research is to find a classifier that achieves best performance results after imputing the missing data using different imputation methods. The results show that in general, the Random Forest classification achieves the best results in comparison to the decision tree J48 and REP Tree. Furthermore, the performance of classification improved when imputing the missing values by concept most common (CMC) and support vector machine (SVM).},
	booktitle = {Proceedings of {SAI} {Intelligent} {Systems} {Conference} ({IntelliSys}) 2016},
	publisher = {Springer International Publishing},
	author = {Al Khaldy, Mohammad and Kambhampati, Chandrasekhar},
	editor = {Bi, Yaxin and Kapoor, Supriya and Bhatia, Rahul},
	year = {2018},
	pages = {415--425}
}

@article{rubin_inference_1976,
	title = {Inference and missing data},
	volume = {63},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/63.3.581},
	doi = {10.1093/biomet/63.3.581},
	abstract = {When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are ‘missing at random’ and the observed data are ‘observed at random’, but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is ‘distinct’ from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
	number = {3},
	urldate = {2019-08-10},
	journal = {Biometrika},
	author = {RUBIN, DONALD B.},
	month = dec,
	year = {1976},
	pages = {581--592},
	annote = {
MCAR
MAR
MNAR
}
}

@article{yeo_new_2000,
	title = {A new family of power transformations to improve normality or symmetry},
	volume = {87},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/87.4.954},
	doi = {10.1093/biomet/87.4.954},
	abstract = {We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to approximate normality. It has properties similar to those of the Box–Cox transformation for positive variables. The large‐sample properties of the transformation are investigated in the contect of a single random sample.},
	number = {4},
	urldate = {2019-08-10},
	journal = {Biometrika},
	author = {Yeo, In‐Kwon and Johnson, Richard A.},
	month = dec,
	year = {2000},
	keywords = {YeoJohnson Transformation},
	pages = {954--959},
	annote = {The Yeo-Johnson transformation}
}

@article{ben-david_about_2008,
	title = {About the relationship between {ROC} curves and {Cohen}'s kappa},
	volume = {21},
	issn = {0952-1976},
	url = {http://www.sciencedirect.com/science/article/pii/S0952197607001224},
	doi = {10.1016/j.engappai.2007.09.009},
	abstract = {Receiver operating characteristic (ROC) curves are very powerful tools for measuring classifiers’ accuracy in binary-class problems. However, their usefulness in real-world multi-class problems has not been demonstrated yet. In these frequently occurring multi-class cases, simple accuracy meters that do compensate for random successes, such as the kappa statistic, are needed. ROC curves are two-dimensional graphs. Kappa is a scalar. Each comes from an entirely different discipline. This research investigates whether they do have anything in common. A mathematical formulation that links ROC spaces with the kappa statistic is derived here for the first time. The understanding of how these two accuracy meters relate to each other can assist in a better understanding of their respective pros and cons.},
	number = {6},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Ben-David, Arie},
	month = sep,
	year = {2008},
	keywords = {Area under ROC curve (AUC), Classification accuracy, Cohen's kappa, Machine learning, ROC curves},
	pages = {874--882},
	annote = {
Relates Cohen's Kappa to ROC
Explains why Kappa is preferable in many cases
}
}

@article{zavrakidis_combining_2017,
	title = {Combining {Multiple} {Imputation} with cross-validation for calibration and assessment of {Cox} prognostic survival models},
	author = {Zavrakidis, Ioannis},
	month = jul,
	year = {2017},
	annote = {How to combine multiple imputation and cross validation
 

Good outline of how to write my thesis
}
}

@article{belanche_handling_2014-1,
	title = {Handling missing values in kernel methods with application to microbiology data},
	volume = {141},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231214003907},
	doi = {10.1016/j.neucom.2014.01.047},
	abstract = {We discuss several approaches that make possible for kernel methods to deal with missing values. The ﬁrst two are extended kernels able to handle missing values without data preprocessing methods. Another two methods are derived from a sophisticated multiple imputation technique involving logistic regression as local model learner. The performance of these approaches is compared using a binary data set that arises typically in microbiology (the microbial source tracking problem). Our results show that the kernel extensions demonstrate competitive performance in comparison with multiple imputation in terms of predictive accuracy. However, these results are achieved with a simpler and deterministic methodology and entail a much lower computational eﬀort.},
	language = {en},
	urldate = {2019-08-15},
	journal = {Neurocomputing},
	author = {Belanche, Lluís A. and Kobayashi, Vladimer and Aluja, Tomàs},
	month = oct,
	year = {2014},
	pages = {110--116},
	annote = {Proposes 2 ways of pooling Multiple Imputation  results
MI1

Concatenate multiply imputed datasets

MI2

Train on separately imputed datasets
},
	file = {Belanche et al. - 2014 - Handling missing values in kernel methods with app.pdf:/Users/Berto/Zotero/storage/A48XAGWJ/Belanche et al. - 2014 - Handling missing values in kernel methods with app.pdf:application/pdf}
}

@incollection{bi_performance_2018,
	address = {Cham},
	title = {Performance {Analysis} of {Various} {Missing} {Value} {Imputation} {Methods} on {Heart} {Failure} {Dataset}},
	volume = {16},
	isbn = {978-3-319-56990-1 978-3-319-56991-8},
	url = {http://link.springer.com/10.1007/978-3-319-56991-8_31},
	abstract = {The missing data issue is a fundamental challenge in terms of analyses and classiﬁcation of data. The classiﬁcation performance of incomplete data could be affected and produce different accuracy results compared with complete data. In this work we compare six scalable imputation methods, implemented on a Heart Failure dataset. The comparison is done by the performance metrics of three different classiﬁcation methods namely J48, REPTree, and Random Forest. The aim of the research is to ﬁnd a classiﬁer that achieves best performance results after imputing the missing data using different imputation methods. The results show that in general, the Random Forest classiﬁcation achieves the best results in comparison to the decision tree J48 and REP Tree. Furthermore, the performance of classiﬁcation improved when imputing the missing values by concept most common (CMC) and support vector machine (SVM).},
	language = {en},
	urldate = {2019-08-15},
	booktitle = {Proceedings of {SAI} {Intelligent} {Systems} {Conference} ({IntelliSys}) 2016},
	publisher = {Springer International Publishing},
	author = {Al Khaldy, Mohammad and Kambhampati, Chandrasekhar},
	editor = {Bi, Yaxin and Kapoor, Supriya and Bhatia, Rahul},
	year = {2018},
	doi = {10.1007/978-3-319-56991-8_31},
	pages = {415--425},
	annote = {Tries various imputation methods on a heart dataset},
	file = {Al Khaldy and Kambhampati - 2018 - Performance Analysis of Various Missing Value Impu.pdf:/Users/Berto/Zotero/storage/JF2JWQYB/Al Khaldy and Kambhampati - 2018 - Performance Analysis of Various Missing Value Impu.pdf:application/pdf}
}

@inproceedings{alasalmi_classification_2015-1,
	title = {Classification {Uncertainty} of {Multiple} {Imputed} {Data}},
	doi = {10.1109/SSCI.2015.32},
	abstract = {Every classification model contains uncertainty. This uncertainty can be distributed evenly or into certain areas of feature space. In regular classification tasks, the uncertainty can be estimated from posterior probabilities. On the other hand, if the data set contains missing values, not all classifiers can be used directly. Imputing missing values solves this problem but it suppresses variation in the data leading to underestimation of uncertainty and can also bias the results. Multiple imputation, where several copies of the data set are created, solves these problems but the classical approach for uncertainty estimation does not generalize to this case. Thus in this paper we propose a novel algorithm to estimate classification uncertainty with multiple imputed data. We show that the algorithm performs as well as the benchmark algorithm with a classifier that supports classification with missing values. It also supports the use of any classifier, even if it does not support classification with missing values, as long as it supports the estimation of posterior probabilities.},
	booktitle = {2015 {IEEE} {Symposium} {Series} on {Computational} {Intelligence}},
	author = {Alasalmi, T. and Koskimäki, H. and Suutala, J. and Röning, J.},
	month = dec,
	year = {2015},
	keywords = {Analytical models, classification model, classification uncertainty estimation, Correlation, Data handling, Data models, imputed data classification, Machine learning algorithms, multiple imputation, pattern classification, posterior probability estimation, probability, Support vector machines, Uncertainty},
	pages = {151--158},
	file = {IEEE Xplore Abstract Record:/Users/Berto/Zotero/storage/NWJLFU3P/7376605.html:text/html}
}

@article{fisher_use_1936,
	title = {{THE} {USE} {OF} {MULTIPLE} {MEASUREMENTS} {IN} {TAXONOMIC} {PROBLEMS}},
	volume = {7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	number = {2},
	journal = {Annals of Eugenics},
	author = {FISHER, R. A.},
	year = {1936},
	pages = {179--188},
	annote = {Linear Discriminant Analysis}
}

@article{cover_geometrical_1965,
	title = {Geometrical and {Statistical} {Properties} of {Systems} of {Linear} {Inequalities} with {Applications} in {Pattern} {Recognition}},
	volume = {EC-14},
	issn = {0367-7508},
	doi = {10.1109/PGEC.1965.264137},
	abstract = {This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces.},
	number = {3},
	journal = {IEEE Transactions on Electronic Computers},
	author = {Cover, T. M.},
	month = jun,
	year = {1965},
	keywords = {Application software, Boolean functions, Geometry, History, Pattern recognition, Vectors},
	pages = {326--334},
	annote = {Quadratic Discriminant Analysis
 
Proposes nonlinear discriminant functions},
	file = {IEEE Xplore Abstract Record:/Users/Berto/Zotero/storage/PP9P5HYG/metrics.html:text/html;IEEE Xplore Full Text PDF:/Users/Berto/Zotero/storage/NBQUNK76/Cover - 1965 - Geometrical and Statistical Properties of Systems .pdf:application/pdf}
}

@article{cohen_coefficient_1960,
	title = {A {Coefficient} of {Agreement} for {Nominal} {Scales}},
	volume = {20},
	url = {https://doi.org/10.1177/001316446002000104},
	doi = {10.1177/001316446002000104},
	number = {1},
	journal = {Educational and Psychological Measurement},
	author = {Cohen, Jacob},
	year = {1960},
	pages = {37--46},
	annote = {Seminal paper for Coehn's Kappa
 }
}

@article{penrose_elementary_1946,
	title = {The {Elementary} {Statistics} of {Majority} {Voting}},
	volume = {109},
	issn = {09528385},
	url = {http://www.jstor.org/stable/2981392},
	doi = {10.2307/2981392},
	number = {1},
	journal = {Journal of the Royal Statistical Society},
	author = {Penrose, L. S.},
	year = {1946},
	pages = {53--57}
}

@article{lam_optimal_1995,
	title = {Optimal combinations of pattern classifiers},
	volume = {16},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/016786559500050Q},
	doi = {10.1016/0167-8655(95)00050-Q},
	abstract = {To improve recognition results, decisions of multiple classifiers can be combined. We study the performance of combination methods that are variations of the majority vote. A Bayesian formulation and a weighted majority vote (with weights obtained through a genetic algorithm) are implemented, and the combined performances of 7 classifiers on a large set of handwritten numerals are analyzed.},
	number = {9},
	journal = {Pattern Recognition Letters},
	author = {Lam, Louisa and Suen, Ching Y.},
	month = sep,
	year = {1995},
	keywords = {Bayesian method, Genetic algorithm, Majority vote, Multiple classifier systems, OCR},
	pages = {945--954}
}

@inproceedings{kittler_combining_1996,
	title = {Combining classifiers},
	volume = {2},
	doi = {10.1109/ICPR.1996.547205},
	booktitle = {Proceedings of 13th {International} {Conference} on {Pattern} {Recognition}},
	author = {Kittler, J. and Hater, M. and Duin, R. P. W.},
	month = aug,
	year = {1996},
	keywords = {pattern classification, Pattern recognition, classifier combination, compound classification, Decision making, Estimation error, Extraterrestrial measurements, Machinery, pattern representations, Physics, sensitivity analysis, Sensitivity analysis, sum rule},
	pages = {897--901 vol.2},
	annote = {An experimental comparison of various class\$er combinationschemes demonstrates that the combination rule developedunder the most restrictive assumptions - the sum rule- and its derivatives consistently outpevorm other classifiercnmbinutions schemes.}
}

@article{alexandre_combining_2001,
	title = {On combining classifiers using sum and product rules},
	volume = {22},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865501000733},
	doi = {10.1016/S0167-8655(01)00073-3},
	abstract = {This paper presents a comparative study of the performance of arithmetic and geometric means as rules to combine multiple classifiers. For problems with two classes, we prove that these combination rules are equivalent when using two classifiers and the sum of the estimates of the a posteriori probabilities is equal to one. We also prove that the case of a two class problem and a combination of two classifiers is the only one where such equivalence occurs. We present experiments illustrating the equivalence of the rules under the above mentioned assumptions.},
	number = {12},
	journal = {Selected Papers from the 11th Portuguese Conference on Pattern Recognition - RECPAD2000},
	author = {Alexandre, Luís A. and Campilho, Aurélio C. and Kamel, Mohamed},
	month = oct,
	year = {2001},
	keywords = {sum rule, Classification, Classifier fusion, Combining classifiers, nearest-neighbours, Neural networks, Arithmetic mean},
	pages = {1283--1289}
}

@book{james_majority_1998,
	title = {Majority {Vote} {Classifiers}: {Theory} and {Applications}},
	author = {James, Gareth},
	year = {1998},
	annote = {
Good explanations of methods
}
}

@inproceedings{tang_when_2018,
	title = {When do random forests fail?},
	author = {Tang, Cheng and Garreau, Damien and von Luxburg, Ulrike},
	year = {2018},
	annote = {Random forests fail with sparsity and without sufficient subsampling}
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	number = {3},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	pages = {273--297}
}

@misc{hsu_practical_2016,
	title = {A {Practical} {Guide} to {Support} {Vector} {Classification}},
	abstract = {The  support  vector  machine  (SVM)  is  a  popular  classification  technique.However,  beginners  who  are  not  familiar  with  SVM  often  get  unsatisfactoryresults since they miss some easy but significant steps. In this guide, we proposea simple procedure which usually gives reasonable results.},
	publisher = {Department of Computer Science, National Taiwan University},
	author = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen},
	year = {2016},
	annote = {Tuning SVM}
}

@book{hastie_elements_2009,
	series = {Springer series in statistics},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-84884-6},
	url = {https://books.google.co.uk/books?id=eBSgoAEACAAJ},
	publisher = {Springer},
	author = {Hastie, T. and Tibshirani, R. and Friedman, J.H.},
	year = {2009},
	lccn = {2008941148}
}

@inproceedings{song_feature_2010,
	title = {Feature {Selection} {Using} {Principal} {Component} {Analysis}},
	volume = {1},
	doi = {10.1109/ICSEM.2010.14},
	booktitle = {2010 {International} {Conference} on {System} {Science}, {Engineering} {Design} and {Manufacturing} {Informatization}},
	author = {Song, F. and Guo, Z. and Mei, D.},
	month = nov,
	year = {2010},
	keywords = {computer science, covariance matrices, covariance matrix, Databases, eigenvalues and eigenfunctions, eigenvector, Face, face recognition, Face recognition, feature extraction, Feature extraction, feature selection, numerical analysis, principal component analysis, Principal component analysis, transform method, transforms, Transforms},
	pages = {27--30}
}

@inproceedings{wang_feature_2009,
	title = {Feature {Selection} for {Maximizing} the {Area} {Under} the {ROC} {Curve}},
	doi = {10.1109/ICDMW.2009.25},
	booktitle = {2009 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops}},
	author = {Wang, R. and Tang, K.},
	month = dec,
	year = {2009},
	keywords = {Support vector machines, feature selection, ARCO algorithm, area under the ROC curve algorithm, Computational complexity, Computational efficiency, Computer applications, Conferences, Costs, data mining, Data mining, Laboratories, learning (artificial intelligence), learning algorithms, Learning systems, minimal redundancy-maximal-relevance method, rank correlation coefficient optimization, Support vector machine classification},
	pages = {400--405}
}

@article{wood_how_2008,
	title = {How should variable selection be performed with multiply imputed data?},
	volume = {27},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3177},
	doi = {10.1002/sim.3177},
	abstract = {Abstract Multiple imputation is a popular technique for analysing incomplete data. Given the imputed data and a particular model, Rubin's rules (RR) for estimating parameters and standard errors are well established. However, there are currently no guidelines for variable selection in multiply imputed data sets. The usual practice is to perform variable selection amongst the complete cases, a simple but inefficient and potentially biased procedure. Alternatively, variable selection can be performed by repeated use of RR, which is more computationally demanding. An approximation can be obtained by a simple ‘stacked’ method that combines the multiply imputed data sets into one and uses a weighting scheme to account for the fraction of missing data in each covariate. We compare these and other approaches using simulations based around a trial in community psychiatry. Most methods improve on the naïve complete-case analysis for variable selection, but importantly the type 1 error is only preserved if selection is based on RR, which is our recommended approach. Copyright © 2008 John Wiley \& Sons, Ltd.},
	number = {17},
	journal = {Statistics in Medicine},
	author = {Wood, Angela M. and White, Ian R. and Royston, Patrick},
	year = {2008},
	keywords = {multiple imputation, multiply imputed data, stacked data, stepwise, variable selection},
	pages = {3227--3246}
}

@article{kemp_applied_2003,
	title = {Applied {Multiple} {Regression}/{Correlation} {Analysis} for the {Behavioral} {Sciences}},
	volume = {52},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1046/j.1467-9884.2003.t01-2-00383_4.x},
	doi = {10.1046/j.1467-9884.2003.t01-2-00383_4.x},
	number = {4},
	journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
	author = {Kemp, Freda},
	year = {2003},
	pages = {691--691}
}

@article{altman_bootstrap_1989,
	title = {Bootstrap investigation of the stability of a cox regression model},
	volume = {8},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780080702},
	doi = {10.1002/sim.4780080702},
	abstract = {Abstract We describe a bootstrap investigation of the stability of a Cox proportional hazards regression model resulting from the analysis of a clinical trial of azathioprine versus placebo in patients with primary biliary cirrhosis. We have considered stability to refer both to the choice of variables included in the model and, more importantly, to the predictive ability of the model. In stepwise Cox regression analyses of 100 bootstrap samples using 17 candidate variables, the most frequently selected variables were those selected in the original analysis, and no other important variable was identified. Thus there was no reason to doubt the model obtained in the original analysis. For each patient in the trial, bootstrap confidence intervals were constructed for the estimated probability of surviving two years. It is shown graphically that these intervals are markedly wider than those obtained from the original model.},
	number = {7},
	journal = {Statistics in Medicine},
	author = {Altman, Douglas G. and Andersen, Per Kragh},
	year = {1989},
	keywords = {Prediction, Bootstrap, Cox proportional hazards regression model, Model selection, Primary biliary cirrhosis},
	pages = {771--783}
}

@book{altman_practical_1991,
	title = {Practical {Statistics} for {Medical} {Research}},
	volume = {Chapter 12},
	publisher = {Chapman \& Hall: London},
	author = {Altman, DG},
	year = {1991}
}

@article{derksen_backward_1992,
	title = {Backward, forward and stepwise automated subset selection algorithms: {Frequency} of obtaining authentic and noise variables},
	volume = {45},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1992.tb00992.x},
	doi = {10.1111/j.2044-8317.1992.tb00992.x},
	abstract = {The use of automated subset search algorithms is reviewed and issues concerning model selection and selection criteria are discussed. In addition, a Monte Carlo study is reported which presents data regarding the frequency with which authentic and noise variables are selected by automated subset algorithms. In particular, the effects of the correlation between predictor variables, the number of candidate predictor variables, the size of the sample, and the level of significance for entry and deletion of variables were studied for three automated subset algorithms: BACKWARD ELIMINATION, FORWARD SELECTION, and STEPWISE. Results indicated that: (1) the degree of correlation between the predictor variables affected the frequency with which authentic predictor variables found their way into the final model; (2) the number of candidate predictor variables affected the number of noise variables that gained entry to the model; (3) the size of the sample was of little practical importance in determining the number of authentic variables contained in the final model; and (4) the population multiple coefficient of determination could be faithfully estimated by adopting a statistic that is adjusted by the total number of candidate predictor variables rather than the number of variables in the final model.},
	number = {2},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Derksen, Shelley and Keselman, H. J.},
	year = {1992},
	pages = {265--282}
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {58},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x},
	doi = {10.1111/j.2517-6161.1996.tb02080.x},
	abstract = {SUMMARY We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	keywords = {quadratic programming, regression, shrinkage, subset selection},
	pages = {267--288}
}

@article{trendafilov_dalass:_2007,
	title = {{DALASS}: {Variable} selection in discriminant analysis via the {LASSO}},
	volume = {51},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947306005032},
	doi = {https://doi.org/10.1016/j.csda.2006.12.046},
	abstract = {The objective of DALASS is to simplify the interpretation of Fisher's discriminant function coefficients. The DALASS problem—discriminant analysis (DA) modified so that the canonical variates satisfy the LASSO constraint—is formulated as a dynamical system on the unit sphere. Both standard and orthogonal canonical variates are considered. The globally convergent continuous-time algorithms are illustrated numerically and applied to some well-known data sets.},
	number = {8},
	journal = {Computational Statistics \& Data Analysis},
	author = {Trendafilov, Nickolay T. and Jolliffe, Ian T.},
	year = {2007},
	keywords = {Canonical variates, Continuous-time constrained optimization, LASSO constraint, Orthogonal canonical variates, Penalty function, Steepest ascent vector flows on manifolds},
	pages = {3718 -- 3736}
}

@article{clemmensen_sparse_2011,
	title = {Sparse {Discriminant} {Analysis}},
	volume = {53},
	url = {https://doi.org/10.1198/TECH.2011.08118},
	doi = {10.1198/TECH.2011.08118},
	number = {4},
	journal = {Technometrics},
	author = {Clemmensen, Line and Hastie, Trevor and Witten, Daniela and Ersbøll, Bjarne},
	year = {2011},
	pages = {406--413}
}

@article{guyon_gene_2002,
	title = {Gene {Selection} for {Cancer} {Classification} using {Support} {Vector} {Machines}},
	volume = {46},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1012487302797},
	doi = {10.1023/A:1012487302797},
	abstract = {DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues.},
	number = {1},
	journal = {Machine Learning},
	author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
	month = jan,
	year = {2002},
	pages = {389--422},
	annote = {
Recursive Feature Selection
}
}

@article{f.r.s_liii._1901,
	title = {{LIII}. {On} lines and planes of closest fit to systems of points in space},
	volume = {2},
	url = {https://doi.org/10.1080/14786440109462720},
	doi = {10.1080/14786440109462720},
	number = {11},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {F.R.S, Karl Pearson},
	year = {1901},
	pages = {559--572},
	annote = {Principal Component Analysis}
}

@incollection{little_bayes_2014,
	title = {Bayes and {Multiple} {Imputation}},
	isbn = {978-1-119-01356-3},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119013563.ch10},
	abstract = {Summary When sample sizes are small, a useful alternative approach to multiple imputation (ML) is to add a prior distribution for the parameters and compute the posterior distribution of the parameters of interest. As with ML estimation with a general pattern of missing values, Bayes simulation requires iteration. The iterative simulation methods discussed eventually create draws from the posterior distribution of θ. However, the five draws of Ymis can be quite adequate for generating MI inferences, provided the fraction of missing information is modest, as when the fractions of cases with Y1 or Y2 missing are limited.},
	booktitle = {Statistical {Analysis} with {Missing} {Data}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Little, Roderick J. A. and Rubin, Donald B.},
	year = {2014},
	doi = {10.1002/9781119013563.ch10},
	keywords = {Bayes, multiple imputation (ML)},
	pages = {200--220},
	annote = {Mean Imputationdistorts the distribution of the data by reducing the variance of the imputed variables and the correlations between variables}
}

@article{efron_efficiency_1975,
	title = {The {Efficiency} of {Logistic} {Regression} {Compared} to {Normal} {Discriminant} {Analysis}},
	volume = {70},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1975.10480319},
	doi = {10.1080/01621459.1975.10480319},
	number = {352},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	year = {1975},
	pages = {892--898},
	annote = {LDA can perform better than logistic regression when assumptions met.}
}

@article{mackinnon_use_2010,
	title = {The use and reporting of multiple imputation in medical research – a review},
	volume = {268},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2796.2010.02274.x},
	doi = {10.1111/j.1365-2796.2010.02274.x},
	abstract = {Abstract. Mackinnon A (Centre for Youth Mental Health, University of Melbourne, Parkville, Victoria, Australia) The use and reporting of multiple imputation in medical research – a review. J Intern Med 2010; 268: 586–593. Background. Multiple imputation (MI) is an advanced, principled method of dealing with missing data in statistical analyses, a common problem in medical research. This paper sought to document the use of MI in general medical journals and to evaluate the information provided to readers about the application of the procedure in studies. Methods. Research articles using MI in analyses published in JAMA, New England Journal of Medicine, BMJ and the Lancet were identified using full text searches from the earliest date each journal offered such searches until the end of 2008. Ninety-nine articles were found. Studies were classified according to their design. Results. Multiple imputation was used in 49 RCTs and 50 other types of studies. A third of the articles (n = 33) reported no details of the procedure used. In a third of these (n = 11), it was not possible to infer the approach used from references cited or software used. The nature of the imputation model was rarely reported. MI was frequently used as a secondary analysis (n = 40) either to justify reporting a simpler approach or as a form of sensitivity analysis. Conclusions. Whilst still relatively uncommon, the use of MI has risen substantially, particularly in trials. MI is rarely adequately reported, leading to doubt about its appropriateness in some cases. This gives rise to uncertainty about conclusions reached and poses a barrier to attempts to replicate analyses. Guidelines for the reporting of MI should be developed.},
	number = {6},
	journal = {Journal of Internal Medicine},
	author = {Mackinnon, A.},
	year = {2010},
	keywords = {multiple imputation, biostatistics, missing data handling, randomized controlled trials, reporting standards},
	pages = {586--593},
	annote = {
MI methods not adequately reported
May be used innappropriately
}
}

@article{hayati_rezvan_rise_2015,
	title = {The rise of multiple imputation: a review of the reporting and implementation of the method in medical research},
	volume = {15},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-015-0022-1},
	doi = {10.1186/s12874-015-0022-1},
	abstract = {Missing data are common in medical research, which can lead to a loss in statistical power and potentially biased results if not handled appropriately. Multiple imputation (MI) is a statistical method, widely adopted in practice, for dealing with missing data. Many academic journals now emphasise the importance of reporting information regarding missing data and proposed guidelines for documenting the application of MI have been published. This review evaluated the reporting of missing data, the application of MI including the details provided regarding the imputation model, and the frequency of sensitivity analyses within the MI framework in medical research articles.},
	number = {1},
	journal = {BMC Medical Research Methodology},
	author = {Hayati Rezvan, Panteha and Lee, Katherine J. and Simpson, Julie A.},
	month = apr,
	year = {2015},
	pages = {30},
	annote = {
MI reporting not sufficient
}
}

@article{karahalios_review_2012,
	title = {A review of the reporting and handling of missing data in cohort studies with repeated assessment of exposure measures},
	volume = {12},
	issn = {1471-2288},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/22784200},
	doi = {10.1186/1471-2288-12-96},
	abstract = {BACKGROUND: Retaining participants in cohort studies with multiple follow-up waves is difficult. Commonly, researchers are faced with the problem of missing data, which may introduce biased results as well as a loss of statistical power and precision. The STROBE guidelines von Elm et al. (Lancet, 370:1453-1457, 2007); Vandenbroucke et al. (PLoS Med, 4:e297, 2007) and the guidelines proposed by Sterne et al. (BMJ, 338:b2393, 2009) recommend that cohort studies report on the amount of missing data, the reasons for non-participation and non-response, and the method used to handle missing data in the analyses. We have conducted a review of publications from cohort studies in order to document the reporting of missing data for exposure measures and to describe the statistical methods used to account for the missing data. METHODS: A systematic search of English language papers published from January 2000 to December 2009 was carried out in PubMed. Prospective cohort studies with a sample size greater than 1,000 that analysed data using repeated measures of exposure were included. RESULTS: Among the 82 papers meeting the inclusion criteria, only 35 (43\%) reported the amount of missing data according to the suggested guidelines. Sixty-eight papers (83\%) described how they dealt with missing data in the analysis. Most of the papers excluded participants with missing data and performed a complete-case analysis (n=54, 66\%). Other papers used more sophisticated methods including multiple imputation (n=5) or fully Bayesian modeling (n=1). Methods known to produce biased results were also used, for example, Last Observation Carried Forward (n=7), the missing indicator method (n=1), and mean value substitution (n=3). For the remaining 14 papers, the method used to handle missing data in the analysis was not stated. CONCLUSIONS: This review highlights the inconsistent reporting of missing data in cohort studies and the continuing use of inappropriate methods to handle missing data in the analysis. Epidemiological journals should invoke the STROBE guidelines as a framework for authors so that the amount of missing data and how this was accounted for in the analysis is transparent in the reporting of cohort studies.},
	language = {eng},
	journal = {BMC medical research methodology},
	author = {Karahalios, Amalia and Baglietto, Laura and Carlin, John B and English, Dallas R and Simpson, Julie A},
	month = jul,
	year = {2012},
	keywords = {*Cohort Studies, *Observer Variation, *Research Design/standards, Bayes Theorem, Data Interpretation, Statistical, Follow-Up Studies, Guidelines as Topic/standards, Humans, Patient Dropouts, Patient Participation, Periodicals as Topic/standards, Publishing/*standards/statistics \& numerical data},
	pages = {96--96}
}

@article{karahalios_review_2012-1,
	title = {A review of the reporting and handling of missing data in cohort studies with repeated assessment of exposure measures},
	volume = {12},
	issn = {1471-2288},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/22784200},
	doi = {10.1186/1471-2288-12-96},
	abstract = {BACKGROUND: Retaining participants in cohort studies with multiple follow-up waves is difficult. Commonly, researchers are faced with the problem of missing data, which may introduce biased results as well as a loss of statistical power and precision. The STROBE guidelines von Elm et al. (Lancet, 370:1453-1457, 2007); Vandenbroucke et al. (PLoS Med, 4:e297, 2007) and the guidelines proposed by Sterne et al. (BMJ, 338:b2393, 2009) recommend that cohort studies report on the amount of missing data, the reasons for non-participation and non-response, and the method used to handle missing data in the analyses. We have conducted a review of publications from cohort studies in order to document the reporting of missing data for exposure measures and to describe the statistical methods used to account for the missing data. METHODS: A systematic search of English language papers published from January 2000 to December 2009 was carried out in PubMed. Prospective cohort studies with a sample size greater than 1,000 that analysed data using repeated measures of exposure were included. RESULTS: Among the 82 papers meeting the inclusion criteria, only 35 (43\%) reported the amount of missing data according to the suggested guidelines. Sixty-eight papers (83\%) described how they dealt with missing data in the analysis. Most of the papers excluded participants with missing data and performed a complete-case analysis (n=54, 66\%). Other papers used more sophisticated methods including multiple imputation (n=5) or fully Bayesian modeling (n=1). Methods known to produce biased results were also used, for example, Last Observation Carried Forward (n=7), the missing indicator method (n=1), and mean value substitution (n=3). For the remaining 14 papers, the method used to handle missing data in the analysis was not stated. CONCLUSIONS: This review highlights the inconsistent reporting of missing data in cohort studies and the continuing use of inappropriate methods to handle missing data in the analysis. Epidemiological journals should invoke the STROBE guidelines as a framework for authors so that the amount of missing data and how this was accounted for in the analysis is transparent in the reporting of cohort studies.},
	language = {eng},
	journal = {BMC medical research methodology},
	author = {Karahalios, Amalia and Baglietto, Laura and Carlin, John B and English, Dallas R and Simpson, Julie A},
	month = jul,
	year = {2012},
	keywords = {*Cohort Studies, *Observer Variation, *Research Design/standards, Bayes Theorem, Data Interpretation, Statistical, Follow-Up Studies, Guidelines as Topic/standards, Humans, Patient Dropouts, Patient Participation, Periodicals as Topic/standards, Publishing/*standards/statistics \& numerical data},
	pages = {96--96}
}

@article{powney_review_2014,
	title = {A review of the handling of missing longitudinal outcome data in clinical trials},
	volume = {15},
	issn = {1745-6215},
	url = {https://doi.org/10.1186/1745-6215-15-237},
	doi = {10.1186/1745-6215-15-237},
	abstract = {The aim of this review was to establish the frequency with which trials take into account missingness, and to discover what methods trialists use for adjustment in randomised controlled trials with longitudinal measurements. Failing to address the problems that can arise from missing outcome data can result in misleading conclusions. Missing data should be addressed as a means of a sensitivity analysis of the complete case analysis results. One hundred publications of randomised controlled trials with longitudinal measurements were selected randomly from trial publications from the years 2005 to 2012. Information was extracted from these trials, including whether reasons for dropout were reported, what methods were used for handing the missing data, whether there was any explanation of the methods for missing data handling, and whether a statistician was involved in the analysis. The main focus of the review was on missing data post dropout rather than missing interim data. Of all the papers in the study, 9 (9\%) had no missing data. More than half of the papers included in the study failed to make any attempt to explain the reasons for their choice of missing data handling method. Of the papers with clear missing data handling methods, 44 papers (50\%) used adequate methods of missing data handling, whereas 30 (34\%) of the papers used missing data methods which may not have been appropriate. In the remaining 17 papers (19\%), it was difficult to assess the validity of the methods used. An imputation method was used in 18 papers (20\%). Multiple imputation methods were introduced in 1987 and are an efficient way of accounting for missing data in general, and yet only 4 papers used these methods. Out of the 18 papers which used imputation, only 7 displayed the results as a sensitivity analysis of the complete case analysis results. 61\% of the papers that used an imputation explained the reasons for their chosen method. Just under a third of the papers made no reference to reasons for missing outcome data. There was little consistency in reporting of missing data within longitudinal trials.},
	number = {1},
	journal = {Trials},
	author = {Powney, Matthew and Williamson, Paula and Kirkham, Jamie and Kolamunnage-Dona, Ruwanthi},
	month = jun,
	year = {2014},
	pages = {237}
}

@article{wood_are_2004,
	title = {Are missing outcome data adequately handled? {A} review of published randomized controlled trials in major medical journals},
	volume = {1},
	url = {https://doi.org/10.1191/1740774504cn032oa},
	doi = {10.1191/1740774504cn032oa},
	abstract = {Background Randomized controlled trials almost always have some individuals with missing outcomes. Inadequate handling of these missing data in the analysis can cause substantial bias in the treatment effect estimates. We examine how missing outcome data are handled in randomized controlled trials in order to assess whether adequate steps have been taken to reduce nonresponse bias and to identify ways to improve procedures for missing data.Methods We reviewed all randomized trials published between July and December 2001 in BMJ, JAMA, Lancet and New England Journal of Medicine, excluding trials in which the primary outcome was described as a time-to-event. We focused on trial designs, how missing outcome data were described and the statistical methods used to deal with the missing outcome data, including sensitivity analyses.Results We identified 71 trials of which 63 (89\%) reported having partly missing outcome data: 13 trials had more than 20\% of patients with missing outcomes. In 26 trials that measured the outcome at a single time point, 92\% performed a complete case analysis and 8\% imputed the missing outcomes using baseline values or the worst case value. In 37 trials with repeated measures of the outcome, 46\% performed complete case analyses, potentially excluding individuals with some follow-up data, while 14\% performed a repeated measures analysis, 19\% used the last observation carried forward, 11\% imputed with the worst case value and 2\% imputed using regression predictions. Thirteen (21\%) of trials with missing data reported a sensitivity analysis.Conclusions Our review shows that missing outcome data are a common problem in randomized controlled trials, and are often inadequately handled in the statistical analysis in the top tier medical journals. Authors should explicitly state the assumptions underlying the handling of the missing outcomes and justify them through data descriptions and sensitivity analyses.},
	number = {4},
	journal = {Clinical Trials},
	author = {Wood, Angela M. and White, Ian R. and Thompson, Simon G.},
	year = {2004},
	pmid = {16279275},
	pages = {368--376}
}

@article{schafer_missing_2002,
	title = {Missing data: {Our} view of the state of the art.},
	volume = {7},
	issn = {1939-1463(Electronic),1082-989X(Print)},
	doi = {10.1037/1082-989X.7.2.147},
	abstract = {Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Methods},
	author = {Schafer, Joseph L. and Graham, John W.},
	year = {2002},
	keywords = {*Maximum Likelihood, *Methodology, *Statistical Data, Statistical Estimation},
	pages = {147--177}
}

@article{rubenfeld_epidemiology_2007,
	title = {Epidemiology and {Outcomes} of {Acute} {Lung} {Injury}},
	volume = {131},
	issn = {0012-3692},
	url = {http://www.sciencedirect.com/science/article/pii/S0012369215483448},
	doi = {https://doi.org/10.1378/chest.06-1976},
	abstract = {Acute lung injury (ALI) and its presentation with more severe hypoxemia, the ARDS, is a challenging entity for clinical investigation because, like many critical illness syndromes, it lacks an accepted diagnostic test and relies on a constellation of clinical findings for diagnosis. Despite these barriers, there have been important advances in the clinical and population epidemiology of ALI. This article will review recent studies of the incidence, diagnosis, etiologic and prognostic factors, relevant disease subsets, mortality, and long-term outcomes of ALI. A detailed understanding of the epidemiology and outcomes of ALI is essential for future research on mechanisms of both the acute presentation and long-term sequelae, for designing studies to identify genetic risk factors for developing ALI, and to develop strategies to treat or prevent the morbidity encountered by survivors.},
	number = {2},
	journal = {Chest},
	author = {Rubenfeld, Gordon D. and Herridge, Margaret S.},
	year = {2007},
	keywords = {acute lung injury, diagnosis, epidemiology, incidence, mortality, prognosis, sequelae},
	pages = {554 -- 562}
}

@article{bellani_epidemiology_2016,
	title = {Epidemiology, {Patterns} of {Care}, and {Mortality} for {Patients} {With} {Acute} {Respiratory} {Distress} {Syndrome} in {Intensive} {Care} {Units} in 50 {CountriesTrends} in {Acute} {Respiratory} {Distress} {Syndrome} in 50 {CountriesTrends} in {Acute} {Respiratory} {Distress} {Syndrome} in 50 {Countries}},
	volume = {315},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2016.0291},
	doi = {10.1001/jama.2016.0291},
	abstract = {Limited information exists about the epidemiology, recognition, management, and outcomes of patients with the acute respiratory distress syndrome (ARDS).To evaluate intensive care unit (ICU) incidence and outcome of ARDS and to assess clinician recognition, ventilation management, and use of adjuncts—for example prone positioning—in routine clinical practice for patients fulfilling the ARDS Berlin Definition.The Large Observational Study to Understand the Global Impact of Severe Acute Respiratory Failure (LUNG SAFE) was an international, multicenter, prospective cohort study of patients undergoing invasive or noninvasive ventilation, conducted during 4 consecutive weeks in the winter of 2014 in a convenience sample of 459 ICUs from 50 countries across 5 continents.Acute respiratory distress syndrome.The primary outcome was ICU incidence of ARDS. Secondary outcomes included assessment of clinician recognition of ARDS, the application of ventilatory management, the use of adjunctive interventions in routine clinical practice, and clinical outcomes from ARDS.Of 29 144 patients admitted to participating ICUs, 3022 (10.4\%) fulfilled ARDS criteria. Of these, 2377 patients developed ARDS in the first 48 hours and whose respiratory failure was managed with invasive mechanical ventilation. The period prevalence of mild ARDS was 30.0\% (95\% CI, 28.2\%-31.9\%); of moderate ARDS, 46.6\% (95\% CI, 44.5\%-48.6\%); and of severe ARDS, 23.4\% (95\% CI, 21.7\%-25.2\%). ARDS represented 0.42 cases per ICU bed over 4 weeks and represented 10.4\% (95\% CI, 10.0\%-10.7\%) of ICU admissions and 23.4\% of patients requiring mechanical ventilation. Clinical recognition of ARDS ranged from 51.3\% (95\% CI, 47.5\%-55.0\%) in mild to 78.5\% (95\% CI, 74.8\%-81.8\%) in severe ARDS. Less than two-thirds of patients with ARDS received a tidal volume 8 of mL/kg or less of predicted body weight. Plateau pressure was measured in 40.1\% (95\% CI, 38.2-42.1), whereas 82.6\% (95\% CI, 81.0\%-84.1\%) received a positive end-expository pressure (PEEP) of less than 12 cm H2O. Prone positioning was used in 16.3\% (95\% CI, 13.7\%-19.2\%) of patients with severe ARDS. Clinician recognition of ARDS was associated with higher PEEP, greater use of neuromuscular blockade, and prone positioning. Hospital mortality was 34.9\% (95\% CI, 31.4\%-38.5\%) for those with mild, 40.3\% (95\% CI, 37.4\%-43.3\%) for those with moderate, and 46.1\% (95\% CI, 41.9\%-50.4\%) for those with severe ARDS.Among ICUs in 50 countries, the period prevalence of ARDS was 10.4\% of ICU admissions. This syndrome appeared to be underrecognized and undertreated and associated with a high mortality rate. These findings indicate the potential for improvement in the management of patients with ARDS.clinicaltrials.gov Identifier: NCT02010073},
	number = {8},
	journal = {JAMA},
	author = {Bellani, Giacomo and Laffey, John G. and Pham, Tài and Fan, Eddy and Brochard, Laurent and Esteban, Andres and Gattinoni, Luciano and van Haren, Frank and Larsson, Anders and McAuley, Daniel F. and Ranieri, Marco and Rubenfeld, Gordon and Thompson, B. Taylor and Wrigge, Hermann and Slutsky, Arthur S. and Pesenti, Antonio and Investigators, for the LUNG SAFE and Group, the ESICM Trials},
	year = {2016},
	pages = {788--800}
}

@article{fan_acute_2018,
	title = {Acute {Respiratory} {Distress} {Syndrome}: {Advances} in {Diagnosis} and {TreatmentAcute} {Respiratory} {Distress} {SyndromeAcute} {Respiratory} {Distress} {Syndrome}},
	volume = {319},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2017.21907},
	doi = {10.1001/jama.2017.21907},
	abstract = {Acute respiratory distress syndrome (ARDS) is a life-threatening form of respiratory failure that affects approximately 200 000 patients each year in the United States, resulting in nearly 75 000 deaths annually. Globally, ARDS accounts for 10\% of intensive care unit admissions, representing more than 3 million patients with ARDS annually.To review advances in diagnosis and treatment of ARDS over the last 5 years.We searched MEDLINE, EMBASE, and the Cochrane Database of Systematic Reviews from 2012 to 2017 focusing on randomized clinical trials, meta-analyses, systematic reviews, and clinical practice guidelines. Articles were identified for full text review with manual review of bibliographies generating additional references.After screening 1662 citations, 31 articles detailing major advances in the diagnosis or treatment of ARDS were selected. The Berlin definition proposed 3 categories of ARDS based on the severity of hypoxemia: mild (200 mm Hg\&lt;Pao2/Fio2≤300 mm Hg), moderate (100 mm Hg\&lt;Pao2/Fio2≤200 mm Hg), and severe (Pao2/Fio2 ≤100 mm Hg), along with explicit criteria related to timing of the syndrome’s onset, origin of edema, and the chest radiograph findings. The Berlin definition has significantly greater predictive validity for mortality than the prior American-European Consensus Conference definition. Clinician interpretation of the origin of edema and chest radiograph criteria may be less reliable in making a diagnosis of ARDS. The cornerstone of management remains mechanical ventilation, with a goal to minimize ventilator-induced lung injury (VILI). Aspirin was not effective in preventing ARDS in patients at high-risk for the syndrome. Adjunctive interventions to further minimize VILI, such as prone positioning in patients with a Pao2/Fio2 ratio less than 150 mm Hg, were associated with a significant mortality benefit whereas others (eg, extracorporeal carbon dioxide removal) remain experimental. Pharmacologic therapies such as β2 agonists, statins, and keratinocyte growth factor, which targeted pathophysiologic alterations in ARDS, were not beneficial and demonstrated possible harm. Recent guidelines on mechanical ventilation in ARDS provide evidence-based recommendations related to 6 interventions, including low tidal volume and inspiratory pressure ventilation, prone positioning, high-frequency oscillatory ventilation, higher vs lower positive end-expiratory pressure, lung recruitment maneuvers, and extracorporeal membrane oxygenation.The Berlin definition of acute respiratory distress syndrome addressed limitations of the American-European Consensus Conference definition, but poor reliability of some criteria may contribute to underrecognition by clinicians. No pharmacologic treatments aimed at the underlying pathology have been shown to be effective, and management remains supportive with lung-protective mechanical ventilation. Guidelines on mechanical ventilation in patients with acute respiratory distress syndrome can assist clinicians in delivering evidence-based interventions that may lead to improved outcomes.},
	number = {7},
	urldate = {2019-08-28},
	journal = {JAMA},
	author = {Fan, Eddy and Brodie, Daniel and Slutsky, Arthur S.},
	month = feb,
	year = {2018},
	pages = {698--710}
}

@article{paolone_extracorporeal_2017,
	title = {Extracorporeal {Membrane} {Oxygenation} ({ECMO}) for {Lung} {Injury} in {Severe} {Acute} {Respiratory} {Distress} {Syndrome} ({ARDS}): {Review} of the {Literature}},
	volume = {26},
	url = {https://doi.org/10.1177/1054773816677808},
	doi = {10.1177/1054773816677808},
	abstract = {Despite advances in mechanical ventilation, severe acute respiratory distress syndrome (ARDS) is associated with high morbidity and mortality rates ranging from 26\% to 58\%. Extracorporeal membrane oxygenation (ECMO) is a modified cardiopulmonary bypass circuit that serves as an artificial membrane lung and blood pump to provide gas exchange and systemic perfusion for patients when their own heart and lungs are unable to function adequately. ECMO is a complex network that provides oxygenation and ventilation and allows the lungs to rest and recover from respiratory failure while minimizing iatrogenic ventilator-induced lung injury. In critical care settings, ECMO is proven to improve survival rates and outcomes in patients with severe ARDS. This review defines severe ARDS; describes the ECMO circuit; and discusses recent research, optimal use of the ECMO circuit, limitations of therapy including potential complications, economic impact, and logistical factors; and discusses future research considerations.},
	number = {6},
	journal = {Clinical Nursing Research},
	author = {Paolone, Summer},
	year = {2017},
	pages = {747--762}
}

@article{sahetya_survival_2018,
	title = {Survival of {Patients} {With} {Severe} {Acute} {Respiratory} {Distress} {Syndrome} {Treated} {Without} {Extracorporeal} {Membrane} {Oxygenation}},
	volume = {27},
	url = {http://ajcc.aacnjournals.org/content/27/3/220.abstract},
	doi = {10.4037/ajcc2018515},
	abstract = {Background Case series have reported favorable outcomes with extracorporeal membrane oxygenation (ECMO) in patients with severe acute respiratory distress syndrome. However, those patients were generally young, with few comorbid conditions.Objective To characterize the clinical features and survival rates of patients with severe acute respiratory distress syndrome who met criteria for ECMO but were managed without it.Methods Patients who met the study criteria were identified prospectively. Inclusion criteria for ECMO included severe hypoxemia, uncompensated hypercapnia, or elevated end-inspiratory plateau pressures despite low tidal volume ventilation. Predicted survival rates with ECMO were calculated using the Respiratory ECMO Survival Prediction score.Results Of the 46 patients who met the criteria for severe acute respiratory distress syndrome and ECMO consideration, 5 received ECMO and 16 patients had at least 1 contraindication to it. The remaining 25 patients met ECMO criteria but did not receive the treatment. The patients’ mean age was 53.5 (SD, 14.3) years; 84\% had at least 1 major comorbid condition. The median predicted survival rate with ECMO was 57\%. The actual hospital discharge survival rate without ECMO was 56\%.Conclusions The general medical intensive care patient population with severe acute respiratory distress syndrome is older and sicker than patients reported in prior case series in which patients were treated with ECMO. In this study, the survival rate without ECMO was similar to predicted survival rates with ECMO.},
	number = {3},
	journal = {American Journal of Critical Care},
	author = {Sahetya, Sarina K. and Brower, Roy G. and Stephens, R. Scott},
	year = {2018},
	pages = {220--227}
}

@article{schafer_missing_2002-1,
	title = {Missing data: our view of the state of the art.},
	volume = {7},
	issn = {1082-989X},
	abstract = {Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art.},
	language = {English (US)},
	number = {2},
	journal = {Psychological Methods},
	author = {Schafer, Joseph L. and Graham, John W.},
	year = {2002},
	pages = {147--177}
}

@article{breiman_submodel_1992,
	title = {Submodel {Selection} and {Evaluation} in {Regression}. {The} {X}-{Random} {Case}},
	volume = {60},
	issn = {03067734, 17515823},
	url = {http://www.jstor.org/stable/1403680},
	doi = {10.2307/1403680},
	abstract = {Often, in a regression situation with many variables, a sequence of submodels is generated containing fewer variables by using such methods as stepwise addition or deletion of variables, or 'best subsets'. The question is which of this sequence of submodels is 'best', and how can submodel performance be evaluated. This was explored in Breiman (1988) for a fixed X-design. This is a sequel exploring the case of random X-designs. Analytical results are difficult, if not impossible. This study involved an extensive simulation. The basis of the study is the theoretical definition of prediction error (PE) as the expected squared error produced by applying a prediction equation to the distributional universe of (y, x) values. This definition is used throughout to compare various submodels. There can be startling differences between the x-fixed and x-random situations and different PE estimates are appropriate. Non-resampling estimates such as CP, adjusted R2, etc. turn out to be highly biased methods for submodel selection. The two best methods are cross-validation and bootstrap. One surprise is that 5 fold cross-validation (leave out 20\% of the data) is better at submodel selection and evaluation than leave-one-out cross-validation. There are a number of other surprises. /// Dans l'analyse de probl\&\#xe8;mes de r\&\#xe9;gression \&\#xe0; plusieurs variables (ind\&\#xe9;pendantes), on produit souvent une s\&\#xe9;rie de sous-mod\&\#xe8;les constitu\&\#xe9;s d'un sous-ensemble des variables par des m\&\#xe9;thodes telles que l'addition par \&\#xe9;tape, le retrait par \&\#xe9;tape et la m\&\#xe9;thode du meilleur sous-ensemble. Le probl\&\#xe8;me est de d\&\#xe9;terminer lequel de ces sous-mod\&\#xe8;les est le meilleur et d'\&\#xe9;valuer sa performance. Ce probl\&\#xe8;me fut explor\&\#xe9; dans Breiman (1988) pour le cas d'une matrice X fixe. Dans ce qui suit on consid\&\#xe8;re le cas o\&\#xf9; la matrice X est al\&\#xe9;atoire. La d\&\#xe9;termination de r\&\#xe9;sultats analytiques est difficile, sinon impossible. Notre \&\#xe9;tude a utilis\&\#xe9; des simulations de grande envergure. Elle se base sur la d\&\#xe9;finition th\&\#xe9;orique de l'erreur de pr\&\#xe9;diction (EP) comme \&\#xe9;tant l'esp\&\#xe9;rance du carr\&\#xe9; de l'erreur produite en applicant une \&\#xe9;quation de pr\&\#xe9;diction \&\#xe0; l'univers distributional des valeurs (y, x). La d\&\#xe9;finition est utilis\&\#xe9;e dans toute l'\&\#xe9;tude \&\#xe0; fin de comparer divers sous-mod\&\#xe8;les. Il y a une diff\&\#xe9;rence \&\#xe9;tonnante entre le cas o\&\#xf9; la matrice X est fix\&\#xe9;e et celui o\&\#xf9; elle est al\&\#xe9;atoire. Diff\&\#xe9;rents estimateurs de la EP sont \&\#xe0; propos. Les estimateurs n'utilisant pas de r\&\#xe9;-\&\#xe9;chantillonage, tels que le Cp et le R2 ajust\&\#xe9;, produisent des m\&\#xe9;thodes de s\&\#xe9;lection ayant grand biais. Les deux meilleures m\&\#xe9;thodes sont la validation crois\&\#xe9;e et l'autoamor\&\#xe7;age. Une surprise est que la validation crois\&\#xe9;e quintuple est meilleure que la validation crois\&\#xe9;e tous sauf un. Il y a plusieurs autres r\&\#xe9;sultats surprenants.},
	number = {3},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Breiman, Leo and Spector, Philip},
	year = {1992},
	pages = {291--319}
}

@inproceedings{kohavi_study_1995,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'95},
	title = {A {Study} of {Cross}-validation and {Bootstrap} for {Accuracy} {Estimation} and {Model} {Selection}},
	isbn = {1-55860-363-8},
	url = {http://dl.acm.org/citation.cfm?id=1643031.1643047},
	booktitle = {Proceedings of the 14th {International} {Joint} {Conference} on {Artificial} {Intelligence} - {Volume} 2},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Kohavi, Ron},
	year = {1995},
	note = {event-place: Montreal, Quebec, Canada},
	pages = {1137--1143}
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	number = {1},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	pages = {5--32}
}

@article{biau_analysis_2012,
	title = {Analysis of a {Random} {Forests} {Model}},
	volume = {13},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=2188385.2343682},
	journal = {J. Mach. Learn. Res.},
	author = {Biau, Gérard},
	month = apr,
	year = {2012},
	keywords = {consistency, dimension reduction, random forests, randomization, rate of convergence, sparsity},
	pages = {1063--1095}
}

@article{biau_random_2016,
	title = {A random forest guided tour},
	volume = {25},
	issn = {1863-8260},
	url = {https://doi.org/10.1007/s11749-016-0481-7},
	doi = {10.1007/s11749-016-0481-7},
	abstract = {The random forest algorithm, proposed by L. Breiman in 2001, has been extremely successful as a general-purpose classification and regression method. The approach, which combines several randomized decision trees and aggregates their predictions by averaging, has shown excellent performance in settings where the number of variables is much larger than the number of observations. Moreover, it is versatile enough to be applied to large-scale problems, is easily adapted to various ad hoc learning tasks, and returns measures of variable importance. The present article reviews the most recent theoretical and methodological developments for random forests. Emphasis is placed on the mathematical forces driving the algorithm, with special attention given to the selection of parameters, the resampling mechanism, and variable importance measures. This review is intended to provide non-experts easy access to the main ideas.},
	number = {2},
	journal = {TEST},
	author = {Biau, Gérard and Scornet, Erwan},
	month = jun,
	year = {2016},
	pages = {197--227}
}

@article{park_extracorporeal_2011,
	title = {Extracorporeal {Membrane} {Oxygenation} in {Adult} {Acute} {Respiratory} {Distress} {Syndrome}},
	volume = {27},
	issn = {0749-0704},
	url = {https://doi.org/10.1016/j.ccc.2011.05.009},
	doi = {10.1016/j.ccc.2011.05.009},
	number = {3},
	urldate = {2019-08-29},
	journal = {Critical Care Clinics},
	author = {Park, Pauline K. and Napolitano, Lena M. and Bartlett, Robert H.},
	month = jul,
	year = {2011},
	pages = {627--646},
	annote = {doi: 10.1016/j.ccc.2011.05.009}
}

@article{wallace_ave_2010,
	title = {Ave, {CESAR}, morituri te salutant! ({Hail}, {CESAR}, those who are about to die salute you!)},
	volume = {14},
	issn = {1364-8535},
	url = {https://doi.org/10.1186/cc8946},
	doi = {10.1186/cc8946},
	number = {2},
	journal = {Critical Care},
	author = {Wallace, David J. and Milbrandt, Eric B. and Boujoukos, Arthur},
	month = apr,
	year = {2010},
	pages = {308}
}

@article{rubin_statistical_1986,
	title = {Statistical {Matching} {Using} {File} {Concatenation} {With} {Adjusted} {Weights} and {Multiple} {Imputations}},
	volume = {4},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/07350015.1986.10509497},
	doi = {10.1080/07350015.1986.10509497},
	number = {1},
	journal = {Journal of Business \& Economic Statistics},
	author = {Rubin, Donald B.},
	year = {1986},
	pages = {87--94}
}

@article{little_missing-data_1988,
	title = {Missing-{Data} {Adjustments} in {Large} {Surveys}},
	volume = {6},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/07350015.1988.10509663},
	doi = {10.1080/07350015.1988.10509663},
	number = {3},
	journal = {Journal of Business \& Economic Statistics},
	author = {Little, Roderick J. A.},
	year = {1988},
	pages = {287--296}
}

@misc{husson_factominer:_2019,
	title = {{FactoMineR}: {Multivariate} {Exploratory} {Data} {Analysis} and {Data} {Mining}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {{FactoMineR}},
	url = {https://CRAN.R-project.org/package=FactoMineR},
	abstract = {Exploratory data analysis methods to summarize, visualize and describe datasets. The main principal component methods are available, those with the largest potential in terms of applications: principal component analysis (PCA) when variables are quantitative, correspondence analysis (CA) and multiple correspondence analysis (MCA) when variables are categorical, Multiple Factor Analysis when variables are structured in groups, etc. and hierarchical cluster analysis. F. Husson, S. Le and J. Pages (2017).},
	urldate = {2019-08-29},
	author = {Husson, Francois and Josse, Julie and Le, Sebastien and Mazet, Jeremy},
	month = jul,
	year = {2019},
	keywords = {Multivariate, Psychometrics}
}

@misc{buuren_mice:_2019,
	title = {mice: {Multivariate} {Imputation} by {Chained} {Equations}},
	copyright = {GPL-2 {\textbar} GPL-3},
	shorttitle = {mice},
	url = {https://CRAN.R-project.org/package=mice},
	abstract = {Multiple imputation using Fully Conditional Specification (FCS) implemented by the MICE algorithm as described in Van Buuren and Groothuis-Oudshoorn (2011) {\textless}doi:10.18637/jss.v045.i03{\textgreater}. Each variable has its own imputation model. Built-in imputation models are provided for continuous data (predictive mean matching, normal), binary data (logistic regression), unordered categorical data (polytomous logistic regression) and ordered categorical data (proportional odds). MICE can also impute continuous two-level data (normal model, pan, second-level variables). Passive imputation can be used to maintain consistency between variables. Various diagnostic plots are available to inspect the quality of the imputations.},
	urldate = {2019-08-29},
	author = {Buuren, Stef van and Groothuis-Oudshoorn, Karin and Robitzsch, Alexander and Vink, Gerko and Doove, Lisa and Jolani, Shahab and Schouten, Rianne and Gaffert, Philipp and Meinfelder, Florian and Gray, Bernie},
	month = jul,
	year = {2019},
	keywords = {MissingData, Multivariate, OfficialStatistics, SocialSciences}
}

@misc{team_micemd:_2019,
	title = {micemd: {Multiple} {Imputation} by {Chained} {Equations} with {Multilevel} {Data}},
	copyright = {GPL-2 {\textbar} GPL-3},
	shorttitle = {micemd},
	url = {https://CRAN.R-project.org/package=micemd},
	abstract = {Addons for the 'mice' package to perform multiple imputation using chained equations with two-level data. Includes imputation methods dedicated to sporadically and systematically missing values. Imputation of continuous, binary or count variables are available. Following the recommendations of Audigier, V. et al (2018) {\textless}doi:10.1214/18-STS646{\textgreater}, the choice of the imputation method for each variable can be facilitated by a default choice tuned according to the structure of the incomplete dataset. Allows parallel calculation and overimputation for 'mice'.},
	urldate = {2019-08-29},
	author = {team), Vincent Audigier (CNAM MSDMA and team), Matthieu Resche-Rigon (INSERM ECSTRA},
	month = jul,
	year = {2019},
	keywords = {MissingData}
}

@misc{wing_caret:_2019,
	title = {caret: {Classification} and {Regression} {Training}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {caret},
	url = {https://CRAN.R-project.org/package=caret},
	abstract = {Misc functions for training and plotting classification and regression models.},
	urldate = {2019-08-29},
	author = {Wing, Max Kuhn Contributions from Jed and Weston, Steve and Williams, Andre and Keefer, Chris and Engelhardt, Allan and Cooper, Tony and Mayer, Zachary and Kenkel, Brenton and Team, the R. Core and Benesty, Michael and Lescarbeau, Reynald and Ziem, Andrew and Scrucca, Luca and Tang, Yuan and Candan, Can and Hunt, {and} Tyler},
	month = apr,
	year = {2019},
	keywords = {HighPerformanceComputing, MachineLearning, Multivariate}
}