
---
title: "Combinging Multiple Imputation and Cross-Validation for Predicting Survival of ECMO Treatment in ARDS Patients"

thesis: MASTER THESIS
major: Biostatistics
output:
  pdf_document:
    citation_package: natbib
    number_sections: yes
    fig_caption: yes
    fig_crop: no
    keep_tex: yes
#    toc: yes
#    toc_depth: 2
#    theme: united  
    highlight: tango  
#    highlight: haddock
  latex_engine: xelatex
  includes:
    in_header: preamble-latex.tex
    before_body: before_body.tex
    

tables: true
mainfont: Calibri Light
fontsize: 12pt
geometry: "left=1in,right=1in,top=1in,bottom=1in"

bibliography: bibliography.bib
link-citations: yes
csl: chicago-author-date.csl
# csl: nature.csl


header-includes: 
  \usepackage[bottom]{footmisc}
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{color}
  \usepackage[table]{xcolor}
  \usepackage{multirow}
  \usepackage{caption}
  \captionsetup[table]{skip=5pt, font=footnotesize} 
  \usepackage[font=footnotesize]{caption} 
  \usepackage{algorithm2e}
  \usepackage{amsmath}
---

```{r setup, include=FALSE, echo = FALSE, eval = TRUE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      comment = NA, 
                      cache = TRUE,
                      fig.pos = 'H', 
                      fig.align = 'center',
                      fig.path = 'figure/graphics-',
                      cache.path = 'cache/graphics-'
                      )
options()
```

```{r libraries, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
source("../project/_settings/libraries.R")
source("../project/_settings/functions.R")

load("../project/data/processed-data.RData")
```


\vspace{2cm}
\begin{center}
Robert Edwards 

\vspace{0.125cm}
2416963E


\vspace{1cm}
MASTER THESIS 

\vspace{0.125cm}
Biostatistics

\vspace{10cm}
  \includegraphics[height = 1.5cm]{images/GUlogo.png}
\end{center}


\newpage
\begin{center}
~
 
\vspace{5cm}
Acknowledgements 

\vspace{3cm}
To my peers, alone we were sinking but together we swam.

\vspace{1cm}
To my family, for keeping me sane in the bipolar Scottish weather. 

\vspace{1cm}
To Google, you da best. 

\end{center}


\newpage 
\setcounter{tocdepth}{2}
\tableofcontents  




\newpage
#Introduction


##Discussion of the Context

+ Description of Acute Respiratory Syndrome
+ Description of ECMO treatment


##Aims of the Proposed Research

Prediction in medical data can often be difficult; imbalanced class distributions and poor predictive covariates.  If the sample size is small, then prediction becomes even more difficult.  Some of these issues arise from the experimental design of the study but little can be rememdied post-hoc.  Missing values in the data complicate analysis even further and are often handled either by dropping missing observations or filling in the missing value by the mean.  Both methods can be valid if certain assumptions hold, but useful information is either lost to the analysis or the natural distribution of the data is effected.  

Multiple imputation is another method for handling missing data that is not yet common in analysis of medical datasets **(Citation)**.  This method both allows retention of observations in the analysis as well as accounts for the uncertainty of the imputed value.  The advantages come at the cost of complexity and increased computation time.  Multiple datasets must be imputed and results somehow pooled.  This paper investigates the use of multiple imputation in increasing prediction performance in a medical dataset.  


##Questions of Interest
The three main questions of interest this paper aims to answer are:

1. Can ECMO treatment survival (`ECMO_Survival`) be accurately predicted by PreECMO biomedical markers?
2. What is the future expected performance of predictions?
3. Which biomedical markers are needed for accurate prediction and which can be dropped?


##Study Population & Data Description

+ Description of the study and variables invovled

The dataset is composed of 450 observations on patients with Acute Respiratory Distress Syndrome who underwent ECMO treatment.  The response variable, `ECMO_Survival`, is a binary categorical variable for survival indication with levels "Y" and "N".  33 covariates are included in the analysis, two of which are categorical, and 31 continuous.  The binary categorical variable `Gender` has two levels for "m", "f" and `Indication` is a seven level disease indicator.  `Age` is a contnuous variable included in the analysis.  The remaining variables are biomedical markers from hospital measurements.   




\newpage
#Methodology


```{r ensemble-imputation, echo = FALSE, eval = TRUE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:ensemble-imputation}Outline of the algorithm used to pool predictions from multiple imputation.  (a) Step 1. (b) Step 2. (c) Step 3. (d) Step 4.  (e) Step 5.  (f) Step 6."}
knitr::include_graphics("images/ensemble-imputation.png")
```

##PreProcessing

+ Standardizing - only on continuous data
  + Mean-centering
  + Scaling
  + Yeo-Johnson Transformation 

##Validation & Cross-validation

When building a classification model, it is important to asses its ability to produce valid predictions.  If there are ample number of observations, one way to asses model performance is to randomly split the dataset into training, validation, and test sets.  The training set is used to fit the model, which is then used to predict the classes for the observations in the validation set;  the validation set is used to estimate prediction error and tune hyperparameters for model selection; the test set is used to estimate future prediction performance for the model/hyperparameters chosen.  To simulate the model predicting on future, unseen data, the test set should be kept isolated.  The model can  overfit the data if feature manipulation and hyperparamter tuning are done before randomly splitting the data.  If standarzation and transformation of the covariates is done on the entire dataset, information from the training set can "leak" into the test set and the true test error will be underestimated. 

If there is insufficient data to split into three parts then a suitable alternative is $K$-fold cross-validation.  It is one of the simplest and most widely used method for estimating prediction error [@hastie_elements_2009].  The data is randomly split into $K$ folds, where the $K^{th}$ fold is taken as the validation set and the the remaining $K-1$ folds are used for training the model.  The procedure is then repeated $K$ times and the prediction error averaged.  $K$-fold cross validation is most useful on sparse datasets as it allows more observations to be used in training the model.  The choice of $K$ can effect the variability of the prediction error; if $K=1$, the model will overfit the data and prediction error will be highly variable and if $K=n$ (the number of observation in the dataset), the model is fit with no validation set for training parameters.  Typical values used are $K=5$ & $10$ [@hastie_elements_2009]. 

a training and a test set, respectively, preserving class proportions using the `createDataPartition()` from the **caret** package.  


##Models

There are many classification methods, some perform well on many types of data and others perform better on certain types of data.  A variety of classification methods are explored toward the aim of predicting survival of ECMO treatment, including parametric methods with many assumptions and high bias as well as non-parametric methods with higher variability.  The five explored on the ARDS dataset in this paper are: Logistic Regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, K-Nearest Neighbors, and Random Forests.  


###Logistic Regression

Logistic regression is a widely used approach in machine learning and medicine for binary classification.  It is a generalisation of linear regression that models the posterior probabilities of the $Y$ classes.   A logit link is used to ensure the posterior probabilities sum to one and are bounded by [0,1].  For two classes, the model has the form

$$
\text{logit} \Big( \text{Pr}(Y \vert X) \Big) = \text{log} \frac{ \text{Pr}(Y=1 \vert X=x) }{ \text{Pr}(Y=2 \vert X=x) }  = \mathbf{x}^T_i\boldsymbol{\beta}  
$$

The posterior probabilities are estimated by maximizing the log-likelihood function to find the parameter estimates, $\hat{\boldsymbol{\beta}}$, to obtain estimates of the probabilities:

$$
\text{Pr}(Y=1 \vert X) = \frac{ \text{exp}(\mathbf{x}^T_1 \hat{\boldsymbol{\beta}}) }{ 1 + \sum^2_{i=1} \text{exp}(\mathbf{x}^T_i \hat{\boldsymbol{\beta}}) }
$$



###LDA and QDA

Discriminant Analysis is a widely used set of classification methods.  A generlization of Fisher's Linear Discriminant [@fisher_use_1936],  discriminant functions are created through a combination of the explanatory variables that characterize the classes.  

Let $p(X \vert Y)$ be the densities of distributions of the observations for each class and let $\pi_Y$ denote the prior probabilities of the classes; that is, the prior probability that a randomly sampled observation belongs to the $Y^{th}$ class based on the class proportions.  The posterior probabilities may be written using Bayes Theorem as:

$$
p(Y \vert X) = \frac{p(X \vert Y) ~\pi_Y}{p(X)} \propto p(X \vert Y) ~\pi_Y   \tag{1}
$$

Suppose the class distribution for class $Y$ is Multivariate Normal with mean $\mu_Y$ and covariance matrix $\Sigma_Y$, so that:

$$
p(X \vert Y) = \frac{1}{(2 \pi_Y)^{p/2} \vert\boldsymbol{\Sigma}_Y\vert ^{1/2}} \text{exp} \left[-\frac{1}{2}(X - \mu_Y)^T \boldsymbol{\Sigma}^{-1}_Y(X - \mu_Y)  \right]  \tag{2}
$$

In comparing two classes, it is sufficient to look at the log-ratio:
$$
\text{log} \frac{\text{Pr}(Y=1 \vert X=x)}{\text{Pr}(Y=2 \vert X=x)} = \text{log}\frac{p(X \vert Y=1)}{p(X \vert Y=2)} + \text{log}\frac{\pi_1}{\pi_2}   \tag{3}
$$

and using Bayes Discriminant Rule stating that *an observation should be allocated to the class with the largest posterior probability*.  From Equation (1), the posterior probability may be written as
$$
p(Y \vert X) \propto \text{exp} \left( Q_Y \right)    \tag{4}
$$

where

$$
Q_Y = (X - \mu_Y) \Sigma^{-1}_Y (X - \mu_Y)^T + \text{log} \vert \Sigma_Y \vert - 2\text{log} ~\pi_Y   \tag{5}
$$

defines the Quadratic Discriminant Function for class $Y$.  The Bayes Discriminant Rule is then: *allocated the observation to the class with the largest QDF*.  This method of classification is called *Quadratic Discriminant Analysis* (QDA) because the decision boundaries between classes are elliptical and defined by $Q_Y$, an equation quadratic in $X$.  If the covariance matrix, $\Sigma_Y$ is assumed to be equal for each class then

$$
L_Y = X \Sigma^{-1}_Y \mu_Y^T -\frac{1}{2}\mu_Y \Sigma^{-1}_Y \mu_Y^T  - \text{log} ~\pi_Y     \tag{6}
$$
defines the *Linear Discriminant Function*. This method has linear decision boundaries between classes defined by $L_Y$, an equation linear in $X$, and is known ad *Linear Discriminant Analysis* (LDA).  The Bayes Discriminant Rule is then: *allocated the observation to the class with the largest LDF*. 

There is a bias-variance trade-off; both assume the covariates are normally distributed, there is no multicollinearity, and the observations are independent [@cover_geometrical_1965].  LDA additionally assumes equal class covariances.  Discriminant Analysis can only utilize continuous covariates with no missing observations.  The bias from simple linear or quadratic class boundaries can be acceptable because  it is estimated with less variance.  Despite the many assumptions and limitations, both LDA and QDA are widely used and perform well on on a diverse set of classification tasks [@hastie_elements_2009], even when the classes are not normally distributed.  


###K-Nearest Neighbors

$K$-Nearest Neighbors (KNN) is a commonly used non-parametric classification method.  To predict the class of a new observation, a distance matrix is constructed between all observations and the K nearest labelled observations to the new observation are considered.  The new observation is then assigned the class label that the majority of its neighbors share.  In case of only two classes, ties in class assignments are avoided by using odd values of K.  

In the event of a tie, a class can be chosen at random.  Various distance metrics may be used but it is common to use Euclidean distance to determine the closest training points, though it is advisable to scale variables so that one direction does not dominate the classification. 

KNN is sensitive to the local sturcture of the data.  As $K$ increases, the variability of the classification tends to decrease at the expense of increased bias. 


###Random Forests

Random forests (Brieman, 2001) are one of the most successful general-purpose modern algorithms (Biau and Scornet, 2016).  They are an ensemble learning method that can be applied to a wide range of tasks, namely classification and regression.  A random forest is created by building multiple decision trees, where randomness is introduced during the construction of each tree.  Predictions are made by classifying a new observation to the mode of the multiple decisions tree classifications.  Random forests often make accurate and robust predictions, even for very high-dimensional problems (Biau, 2012).  See **(Appendix X)** for an explanation of the random forests algorithm. 

+ **State why random forests are good predictors**



##Accuracy Metrics

These are the default metrics used to evaluate algorithms on binary and multi-class classification datasets in caret.

###Accuracy, Sensitivity, and Specificity

Accuracy is the percentage of correctly classifies instances out of all instances. It is more useful on a binary classification than multi-class classification problems because it can be less clear exactly how the accuracy breaks down across those classes (e.g. you need to go deeper with a confusion matrix). Learn more about Accuracy here.

Don’t use accuracy (or error rate) to evaluate your classifier! There are two significant problems with it. Accuracy applies a naive 0.50 threshold to decide between classes, and this is usually wrong when the classes are imbalanced. Second, classification accuracy is based on a simple count of the errors, and you should know more than this. You should know which classes are being confused and where (top end of scores, bottom end, throughout?)

```{r confusion-matrix, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '100%', fig.align="center", fig.pos="H"}

xtab <- matrix(c("a", "b", "c", "d"), nrow = 2, byrow = TRUE)
response <- matrix(c("N", "Y"), nrow=2, byrow = TRUE)
predicted <- matrix(c("Predicted", "Predicted"), nrow=2, byrow = TRUE)
xtab <- cbind(predicted, response, xtab)
colnames(xtab) <- c(" ", " ", "N", "Y")

xtab %>%
  kable(format = "latex", 
        align = c("l", "c", "|c", "c"), 
        booktabs = TRUE,
        caption = '\\label{tab:confusion-matrix} Confusion matrix for two classes.') %>%
    kable_styling(font_size = 12, 
                  latex_options = c("hold_position", "striped")) %>%
  column_spec(1, bold = T) %>%
  collapse_rows(columns = 1:2, valign = "middle", latex_hline = "none") %>%
  add_header_above(c(" " = 2, "Observed" = 2), bold = FALSE) 
  
```

For the two class confusion matrix in Table \ref{tab:confusion-matrix} accuracy metrics are defined as:

$$
\begin{aligned}
\text{sensitivity} &= \frac{a}{a+c} \\
\text{specificity} &= \frac{d}{b+d} \\
\text{accuracy} &= \frac{a+d}{a+b+c+d}
\end{aligned}
$$
where sensitivity is a measure of how accurately non-survival is predicted,  specificity is a measure of how accurately survival is predicted, and accuracy is a measure of how well both survival and non-survival are predicted.  While sensitivity and specificity state the accuracy each class prediction, accuracy is a poor measure for model performance in an imbalanced dataset.  On the ARDS datasets, for example, if `ECMO_Survival` is predicted to be "Y" for all cases, then the accuracy is 75% but the prediction is no better than the baseline likelihood of the class percentages.  


###Cohen's Kappa

Kappa or Cohen’s Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset. It is a more useful measure to use on problems that have an imbalance in the classes.  Let $p_o$ be the accuracy, the relative observed agreement between observed and predicted classes and let $p_e$ be the probability of chance agreement based on the class probabilities.  Cohen's Kappa is defined as:

$$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$

If all the observations are predicted correctly then $\kappa=1$.  If the observations are predicted no better than expected by the class probabilities, $p_e$ then $\kappa=0$.  If all the observations are predicted incorrectly, then $\kappa=-1$.  A positive $\kappa$ indicates that the model predicts better than would be expected by chance whereas a negative $\kappa$ indicates that the model predicts worse than would be expected by chance.  

$$
p_o = \frac{a+d}{a+b+c+d}
$$
$$
p_e = p_{o,Y} + p_{o,N} 
$$


$$
p_{o,Y} = \frac{a+d}{a+b+c+d} ~\cdot~ \frac{a+c}{a+b+c+d}
$$

$$
p_{o,N} = \frac{c+d}{a+b+c+d} ~\cdot~ \frac{b+d}{a+b+c+d}
$$



##Missing Data

Missing data is a common problem that must be dealt with in machine learning, statistics, and medicine.  Understanding the missing mechanism for the missing observations is important in the analysis.  [@rubin_inference_1976] defined three types of missing data mechanisms: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR).  The data are said to be missing completely at random (MCAR) if the probability of being missing is the same for all cases.  This implies the causes of the missing data are unrelated to the data itself.  While MCAR is conveinient because it allows many complexities that arise because data are missing to be ignored, it is typically an unrealistic assumption [@van_buuren_flexible_2012].  The data is said to be MAR if the probability of being missing is the same only within groups defined by the observed data.  MAR is a more general and more realistic assumption than MCAR.  If neither MCAR nor MAR applies, then the probability of being missing depends on an unknown mechanism and said to be MNAR.  Most simple approaches to dealing with missing data are only valid under MCAR assumption.  Modern methods to dealing with missing data begin from the MAR assumption.  

##Imputation Methods

###Complete Case Analysis
Complete case canalysis is a convenient method for handling missing data and is the default method in many statistical packages.  If there is a missing value in an observation, it is dropped from the analysis.  This is often a poor appraoch as complete cases analysis assumes MCAR.  In sparse datasets a complete case analysis can cause an analysis to be underpowered and if MCAR does not hold, can severely bias estimates of means, regression coefficients, and correlations [@van_buuren_flexible_2012].  

The ARDS dataset considered in this paper has `r sum(!complete.cases(data_clean.df))`/`r nrow(data_clean.df)` observations with missing data.


###Mean Imputation
Another common method for handling missing data is mean imputation; the missing value is replaced by the mean of the observed values (the mode for categorical data).  This approach is satisfactory for a moderate amount of MCAR-generated  missing  values.  However, it  distorts  the distribution of the data by reducing the variance of the imputed variables and the correlations between variables **(Little and Rubin 2002)**.  [@van_buuren_flexible_2012] suggests mean imputation should only be used only when there are few missing values, and should be generally avoided.  

+ Can further distort the distribution if the variable is not normally distributed.  
+ Mean imputation is implemented in this paper because although it is often a poor method of choice for imputing missing values, it is commonly done.  
+ Variables are transformed via Yeo-Johnson Transformation to reduce distributional distortions.  


###Fully Conditional Specification

###Predictive Mean Matching

Predictive Mean Matching (PMM) is a semi-parametric imputation approach. It is similar to the regression method except that for each missing value, it fills in a value randomly from among the a observed donor values from an observation whose regression-predicted values are closest to the regression-predicted value for the missing value from the simulated regression model (Heitjan and Little 1991; Schenker and Taylor 1996).
The PMM method ensures that imputed values are plausible; it might be more appropriate than the regression method (which assumes a joint multivariate normal distribution) if the normality assumption is violated (Horton and Lipsitz 2001, p. 246). PMM is fairly robust to transformations of the target variables [@van_buuren_flexible_2012], yielding similar results for a Yeo-Johnson transformation or no transformation. 


###Multiple Imputation
The aim when imputing data is to recreate the dataset and recreate the missing data as if it were never missing.  Multiple imputation is a method that accounts for the uncertainty in the imputed values.  The analysis begins with the observed, incomplete dataset.  The dataset is imputed multiple times to create $m>1$ complete datasets.  The imputed values are drawn from a distribution specifically modeled for each missing entry.  The $m$ datasets are analyzed using the same method that would have been used had the data been complete.  The results will differ because of the variation in the input data caused by the uncertainty in the imputed values.  

Multiple imputation can handle data that is both MAR and MNAR.  

There is uncertainty as to the true value of the unseen data, and that uncertainty should be included in the analysis.  Multiple imputation is a method created by Donald Rubin wherein multiple datasets are imputed, the analysis is conducted on each dataset, and the results are pooled using "Rubin's Rules" [@rubin_inference_1976].  

+ Details of the **MICE** algorithm can be found in Appendix B.  



##Ensemble Multiple Imputation

Two approaches have been proposed for pooling results from several SVMs **(Belache et al. 2014)** and Cox regression **(Zavrakidis 2017)** from multiply imputed datasets.  The method is to concatenate the $m$ imputed datasets and fit a classifier, and optimize, to the resulting set; this accounts for the variability of the parameter estimates as well as the variability of the training observations in relation to the imputed values **(Belache et al. 2014)**.  The second proceedure fits separate classifiers to each imputed data set and get the pooled (i.e. avereaged) performance of the $m$ classifiers.  Results from both studies either show similar results between appraoches **(Zavrakidis 2017)** or slightly better performance with the first approach **(Belache et al. 2014)**.  For simplicity and the sake of computational costs, this paper, only considers the first approach.    

The steps in the ensemble approach for multiply imputed data in k-fold cross-validation are as follows:

1. Randomly partition the training data into $k$ folds
2. Define the $k^{th}$ as the test set and the remaining $k-1$ folds as the training set
3. Impute the training set $m$ times, with the response variable `ECMO_Survival` included, to create $m$ imputed training sets
4. Concatenate the $m$ imputed training sets into one extended training set
5. A model is fitted to the extended training set
6. The test set is concatenated with the extended training set
7. Impute the combined test and extended training set, with the response variable `ECMO_Survival` excluded, to create $m$ imputed combined test and extended training sets
8. Extract the $m$ test sets
9. Make $m$ predictions on the $m$ imputed test sets
10. Take the majority vote of the $m$ predictions as the prediction for the fitted model
11. Validate the prediction against the test set by calculating Cohen's Kappa (note there are no missing values for the response variable in the data)
12. Repeat steps 2-11 $k$ times and validate the fitted model on each training set against the test set for each fold
13. Average the $k$ calculated Cohen's Kappas as the estimated in-sample accuracy metric



###Number of Imputations

**Flexible Imputation Book**  
"The classic advice is to use a low number of imputation, somewhere between 3 and 5 for moderate amounts of missing information. Several authors investigated the influence of m on various aspects of the results. The picture emerging from this work is that it is often beneficial to set m higher, somewhere in the range of 20-100 imputations. 

"Rubin's Rules" provide a simple method for pooling parameters estimates from multiple imputation for linear and generalized linear models but to the author's knowledge, there has been insufficient work on estimating the required number of imputations for estimating posterior probabilities in classification problems.  In his book, **(Van Buuren)** states

> "Theoretically it is always better to use higher m, but this involves more computation and storage. Setting m very high (say m=200) may be useful for low-level estimands that are very uncertain, and for which we want to approximate the full distribution, or for parameters that are notoriously different to estimates, like variance components. On the other hand, setting m high may not be worth the extra wait if the primary interest is on the point estimates (and not on standard errors, p-values, and so on). In that case using m=5-20 will be enough under moderate missingness."


##Voting

There has been sufficient exploration into pooling of posterior probabilities resulting from classification problems **(Citation 1)** **(Citation 2)** **Citation 3)**.  But not all statistical methods produce posterior probabilities and the comparison of pooled models from multiple imputation is an area ripe for more analysis.  Indeed, others have pooled predictions from various machine learning methods by taking the majority vote **(Zavrakidis)** **(Citation 2)**, and comparing prediction accuracy.  

###Majority Vote

The combination can be implemented using a variety of strategies, among which majority vote is by far the simplest, yet it has been found to be just as effective as more complicated schemes [@lam_optimal_1995].

**(Alexandre et al. 2001)**
There has been some interest on the comparative performance of the sum and product rules (or the arithmetic and geometric means) (Kittler et al., 1996; Tax et al., 1997; Kittler et al., 1998). The arithmetic mean is one of the most frequently used combination rules since it is easy to implement and normally produces good results.

In (Kittler et al., 1998), the authors show that for combination rules based on the sum, such as the arithmetic mean, and for the case of classifiers working in different feature spaces, the arithmetic mean is less sensitive to errors than geometric mean.

In fact (Alexandre et al. 2001) show that for classification problems with two classes, that give estimates of the a posteriori probabilities that sum to one the combination rules arithmetic mean (or the sum) and the geometric mean (or the product) are equivalent. 


##Feature Selection

One of the goals of this analysis is to identify the variables most useful for accurate prediction.  There are various methods that can be used for feature selection: stepwise selection, Recursive Feature Elimination (RFE), LASSO regularization, and Principal Component Analysis.  However, some of these methods are highly criticized, not applicable to all classifications methods considered, or cannot be integrated into the ensemble cross-validation approach used.  Stepwise selection, while very common, is only applicable to regression models and it is often criticised [@kemp_applied_2003]; problems include falsely narrow confidence intervals for effects and predicted values [@altman_bootstrap_1989] and multiple hypothesis testing inflating risks of capitalising on chance features of the data [@altman_practical_1991], such as noise covariates gaining entry into the model when the number of candidate variables is large [@derksen_backward_1992].  

RFE is an iterative procedure analogous of backward feature selection [@guyon_gene_2002].  

Both logistic regression with LASSO regularization [@tibshirani_regression_1996] and the analogous Sparse Discriminant Analysis [@clemmensen_sparse_2011] are embedded feature selection methods that are dependent on the classification method.  

+ PCA for feature selection
+ Quick description of PCA
+ Maximizing the Area Under the ROC Curve











\newpage
#Results

## Exploratory Data Analysis

  
To get an idea of the distribution of the data, the following summary statistics were obtained for the categorical variable `ECMO_Survival` (Table \ref{tab:ECMO_Survival-table}) and for the continuous variables (Table \ref{tab:gender-table}).

```{r , echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
data_clean.df %>%
  group_by(ECMO_Survival) %>%
  summarise(n(), round(100*n()/nrow(data_clean.df), 2)) %>%
  kable(col.names = c("ECMO_Survival", "n", "Percent %"),
        caption = '\\label{tab:ECMO_Survival-table} Numbers of survivors and nonsurvivors of ECMO treatment.', booktabs = TRUE, format = "latex") %>%
  kable_styling(font_size = 10, latex_options = "hold_position")
```

Table \ref{tab:ECMO_Survival-table} shows that out of the `r nrow(data_clean.df)` individuals, only `r round(100*sum(data_clean.df$ECMO_Survival=="Y")/nrow(data_clean.df), 2)`% of the individuals in the study sample survived ECMO treatment (`r sum(data_clean.df$ECMO_Survival=="Y")` survived vs `r sum(data_clean.df$ECMO_Survival=="N")` did not survive).  


```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
data_clean.df %>%
  group_by(Gender) %>%
  summarise(n(), round(100*n()/nrow(data_clean.df), 2)) %>%
  kable(col.names = c("Gender", "n", "Percent %"),
        caption = '\\label{tab:gender-table} Number of males and females.', booktabs = TRUE, format = "latex") %>%
  kable_styling(font_size = 10, latex_options = "hold_position")
```

Table \ref{tab:gender-table} shows that out of the `r nrow(data_clean.df)` individuals, only `r round(100*sum(data_clean.df$Gender == "m")/nrow(data_clean.df), 2)`% of the individuals in the study sample are male (`r sum(data_clean.df$Gender == "m")` male vs `r sum(data_clean.df$Gender == "w")` female).  

```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
data_clean.df %>%
  group_by(Indication) %>%
  summarise(n(), round(100*n()/nrow(data_clean.df), 2)) %>%
  kable(col.names = c("Indication", "n", "Percent %"),
        caption = '\\label{tab:indications-table} Number of each disease type indication.', booktabs = TRUE, format = "latex") %>%
  kable_styling(font_size = 10, latex_options = "hold_position")
```

Table \ref{tab:indications-table} shows the distribution of each disease type indication. 


```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, fig.ext='png', out.width = '100%', fig.align="center", fig.pos="H", fig.cap = "\\label{fig:violin-standardized}Violin plot of continuous variables."}
feature_names <- colnames(data_clean.df[, 4:ncol(data_clean.df)])
#data_standard.df <- data_clean.df %>%
#  dplyr::select(5:ncol(data_clean.df)) %>%
#  mutate_each_(funs(scale), vars=feature_names)

data_standard.df <- data_clean.df %>% 
  preProcess( method = c("center", 
                       "scale"
#                       "YeoJohnson"  ## Transformation method
                        )) %>%
  predict(data_clean.df) ## Generate new dataframe

data_standard.df %>% 
  select(-ECMO_Survival, -Gender, -Indication) %>%
  gather(key = Feature, value = Value) %>% 
  ggplot(aes(x = Feature, y = Value, fill = Feature)) +
    geom_violin(scale = "width", 
                draw_quantiles = TRUE,
                trim = TRUE,
                show.legend = FALSE) +
    geom_boxplot(width = 0.2,
                 outlier.size = 0.5) +
    xlab("Features") + 
    ylab("Standardized Value") +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5),
          legend.position = "none",
          axis.text.x = element_text(angle = 60, hjust = 1))  
```


##Missing Data Patterns

Before imputation, and indeed multiple imputation, it is important to inspect the missingness patterns in the data and check assumptions.  Figure \ref{fig:missing-data} shows the missingness patterns in the dataset, where a black bar represents a missing value.  Table \ref{tab:missing-statistics} provides some measures about variable dependence  in the dataset.  The first row shows the probability of observed values for each variable.  The following are coefficients that give insight into how the variables are connected in terms of missingness.  $\mathbf{Influx}$ is the ratio of the number of variables pairs $(Y_j, ~Y_k)$ with $Y_j$ missing and $Y_k$ observed, divided by the total number of observed data.  For a variable that is entirely missing, influx is 1, and 0 for if the variable is complete.  $\mathbf{Outflux}$ is defined in the opposit manner, by dividing the number of pairs $(Y_j, ~Y_k)$ with $Y_j$ observed and $Y_k$ missing, by the total number of complete cells.  For a completely observed variable, outflux will have a value of 1 and 0 if completely missing.  Outflux gives an indication of how useful the variable will be for imputing other variables in the dataset, while influx is an indicator for how easily the variable can be imputed.  Table \ref{tab:missing-patterns} shows that all variables will be useful during impuation except `PreECMO_Albumin`.  A high outflux variable might turn out to be useless for the imputation procedure if it is unrelated to the incomplete variables, while the usefulness of a highly predictive variables is severely limited by a low outflux value (Van Buuren 2012).  **Mention \ref{fig:missing-data})**


```{r missing-data, echo = FALSE, eval = TRUE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig.ensemble-imputation}Visual representation of missing observations in the ARDS dataset."}
#knitr::include_graphics("images/missing_data_visualization.png")
load("../project/data/processed-data.RData")
vis_miss(data_clean.df, sort_miss = TRUE) +
  theme(  plot.margin = ggplot2::margin(0.125, 2, 0.5, 1, "cm") ) #top, right, bottom, left
```

+ It can be difficult or impossible to determine if the data are MCAR.  Figure \ref{fig:missing-data} shows that many missing values occur in observations with other missing values.  Missing values could be conditionally dependent on other variables, in which case the data would be MAR.  The missing values could also be due to some unknown mechanism at the time of recording (*i.e.* a failure of the measurement device) that happens to effect multiple readings (the biomarkers are measured from blood samples and measurements are likely done in batches).  In this case the data would be MCAR.  Without more information, this analysis assumes the data is MCAR.  

```{r missing-patterns, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '100%', fig.align="center", fig.pos="H"}

flux <- flux(data_clean.df)

flux %>%
  round(2) %>%
  select(pobs, influx, outflux) %>%
  kable(col.names = c("Proportion", "Influx", "Outflux"), 
        caption = '\\label{tab:missing-patterns} Missing pattern statistics for variables in dataset.',
        booktabs = TRUE, 
        format = "latex") %>%
  kable_styling(font_size = 10, 
                latex_options = "hold_position") %>%
  row_spec(23, bold = TRUE)
```


## Experiments and Results

The experimentation phase of this study involved three methods for handling missing data: (a) complete case analysis with the variable `PreECMO_Albumin` dropped from the analysis due to `r round( sum(is.na(data_clean.df$PreECMO_Albumin))/450 * 100, 2)`% missingness, (b) mean imputation on variables with missing values, (c) imputation via the MICE algorithm implemented with PMM.  


```{r cv-kappa, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '100%'}
load("../project/_trained-models/trained-models-complete-case.RData")
kappa_cc <- as.data.frame(cbind(table_logit[[2]], table_lda[[2]], table_qda[[2]], table_knn$xtabs[[2]], table_rf$xtabs[[2]]) )

load("../project/_trained-models/trained-models-mean.RData")
kappa_mean <- as.data.frame(cbind(table_logit[[2]], table_lda[[2]], table_qda[[2]], table_knn$xtabs[[2]], table_rf$xtabs[[2]]) )

load("../project/_trained-models/trained-models-pmm.RData")
kappa_pmm <- as.data.frame(cbind(table_logit[[2]], table_lda[[2]], table_qda[[2]], table_knn$xtabs[[2]], table_rf$xtabs[[2]]) )

kappa_table <- rbind(kappa_cc, kappa_mean, kappa_pmm)
kappa_table <- round(kappa_table, 3)
colnames(kappa_table) <- c("Logit", "LDA", "QDA", "KNN", "RF")
rownames(kappa_table) <- c("Complete Case", "Mean", "PMM")

cap <- paste0('\\label{tab:cv-kappa} Averaged Cohen\'s Kappa for each model fitted in cross-validation.  The tuned parameters for K-Nearest Neighbors and Random Forests are K=', table_knn$kmax, ' and mtry=', table_rf$mtry, ', respectively.')

kappa_table %>%
  kable(#col.names = c("Variables", "Test", "df", "p.value"), 
        caption = cap,
        booktabs = TRUE, 
        format = "latex") %>%
  kable_styling(font_size = 10, 
                latex_options = "hold_position")
```

Table \ref{tab:cv-kappa} shows the averaged Kappa from each analysis in 10-fold cross-validation.  In complete case analysis and mean imputaion, LDA is the highest performer.  While for predictive mean-matching with $m=9$ logistic regression has the higest averaged Kappa.  


###Validation on Test Set

Using the parameters values learned obtained from 10-fold cross-validation in Table \ref{tab:cv-kappa}, models were fit to the full training set and validated against the test set.  K-nearest neighbors and random forests were fit with parameters $K=5$ and $mtry=13$, respectively.  


```{r echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '50%'}
load("../project/_metrics/metrics-complete-case.RData")
load("../project/_trained-models/trained-models-complete-case.RData")

cap <- paste0('\\label{tab:cc-metrics} Complete case analysis accuracy metrics.  The tuned hyperparameters for K-Nearest Neighbors and Random Forests are K=', table_knn$kmax, ' and mtry=', table_rf$mtry, ', respectively.')

metrics_table %>%
  round(3) %>%
  select(sensitivity, specificity, accuracy, kappa) %>%
  kable(col.names = c("Sensitivity", "Specificity", "Accuracy", "Kappa"), 
        caption = cap,
        booktabs = TRUE, 
        format = "latex") %>%
  row_spec(4, bold = T) %>%
  kable_styling(font_size = 10, 
                latex_options = "hold_position")
```


```{r echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '50%'}
load("../project/_metrics/metrics-mean.RData")
load("../project/_trained-models/trained-models-mean.RData")

cap <- paste0('\\label{tab:mean-metrics} Mean imputation accuracy metrics (m=1).  The tuned hyperparameters for K-Nearest Neighbors and Random Forests are K=', table_knn$kmax, ' and mtry=', table_rf$mtry, ', respectively.')

metrics_table %>%
  round(3) %>%
  select(sensitivity, specificity, accuracy, kappa) %>%
  kable(col.names = c("Sensitivity", "Specificity", "Accuracy", "Kappa"), 
        caption = cap,
        booktabs = TRUE, 
        format = "latex") %>%
  row_spec(5, bold = T) %>%
  kable_styling(font_size = 10, 
                latex_options = "hold_position")
```


```{r echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '50%'}
load("../project/_metrics/metrics-pmm.RData")
load("../project/_trained-models/trained-models-pmm.RData")

cap <- paste0('\\label{tab:pmm-metrics} MICE via predictive mean matching accuracy metrics (m=91).  The tuned hyperparameters for K-Nearest Neighbors and Random Forests are K=', table_knn$kmax, ' and mtry=', table_rf$mtry, ', respectively.')

metrics_table %>%
  round(3) %>%
  select(sensitivity, specificity, accuracy, kappa) %>%
  kable(col.names = c("Sensitivity", "Specificity", "Accuracy", "Kappa"), 
        caption = cap,
        booktabs = TRUE, 
        format = "latex") %>%
  row_spec(1, bold = T) %>%
  kable_styling(font_size = 10, 
                latex_options = "hold_position")
```


```{r echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '50%'}
load("../project/_metrics/metrics-pmm99.RData")
load("../project/_trained-models/trained-models-pmm99.RData")

cap <- paste0('\\label{tab:pmm99-metrics} MICE via predictive mean matching accuracy metrics (m=99).  The tuned hyperparameters for K-Nearest Neighbors and Random Forests are K=', table_knn$kmax, ' and mtry=', table_rf$mtry, ', respectively.')

metrics_table %>%
  round(3) %>%
  select(sensitivity, specificity, accuracy, kappa) %>%
  kable(col.names = c("Sensitivity", "Specificity", "Accuracy", "Kappa"), 
        caption = cap,
        booktabs = TRUE, 
        format = "latex") %>%
  row_spec(1, bold = T) %>%
  kable_styling(font_size = 10, 
                latex_options = "hold_position")
```


\newpage
#Discussion



##Model Performance

**Logistic Regression**  
For complete-case analysis, mean imputation, and predictive mean-matching, logistic regression does not meet the "one in ten rule", a rule of thumb stating that a logistic regression models give stable estimates for the covariates if there are at least 10 observations of the least frequent class per covariate.  


**LDA**  
Can perform better than logistic regression when the covariates are normally distributed **(CITATION)**, which they are in this case after Yeo-Johnson transformation.  

**QDA**


**K-Nearest Neighbors**


**Random Forests Fails**  

+ Sparsity - When the data are very sparse, it's very plausible that for some node, the bootstrapped sample and the random subset of features will collaborate to produce an invariant feature space. There's no productive split to be had, so it's unlikely that the children of this node will be at all helpful.

+  One surprising consequence is that trees that work well for nearest-neighbor search problems can be bad candidates for forests without sufficient subsampling, due toa lack of diversity.  **(Tang et al. 2018)**

+ Data are not axis-aligned - Suppose that there is a diagonal decision boundary in the space of two features, $x_1$ or $x_2$.  Even if this is the only relevant dimension to your data, it will take an ordinary random forest model many splits to describes that diagonal boundary.  This is because each split is oriented perpendicular to the axis of either $x_1$ or $x_2$.  

+ XGBoost, Rotation forest (PCA rotation) may do better


##Important Features for Prediction

+ Mention poor model performance 
+ Correlation heatmap
+ Results from PCA


+ **Refer to Figure \ref{fig:feature-importance} in Appendix A for feature importance plots for the logit model**


```{r , echo = FALSE, eval = FALSE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:feature-importance-pca-lda}Ordered feature importance from LDA model"}
# PCA with function PCA
require(FactoMineR) 

#scale all the features,  ncp: number of dimensions kept in the results (by default 5)
pca <- PCA(data_clean.df[, 4:ncol(data_clean.df)], graph=FALSE, ncp=20)
#print(pca)

#This line of code will sort the variables the most linked to each PC. It is very useful when you have many variables.
#dimdesc(pca)


## Interpreting PCA
## The amount of variation retained by each PC is called eigenvalues. 
## The first PC corresponds to the direction with the maximum amount of variation in the data set.
eigenvalues <- pca$eig
head(eigenvalues, 20)

## Scree Plot
fviz_screeplot(pca, ncp=20)

### Coordinates of variables
head(pca$var$coord)

## Visualization of the variables on the factor map
## Correlation circle can help to visualize the most correlated variables (i.e, variables that group together).
fviz_pca_var(pca)

## Squared Loadings of variables
## The squared loadings for variables are called cos2 ( = cor * cor = coord * coord).
## The sum of the cos2 for variables on the principal components is equal to one.
## If a variable is perfectly represented by only two components, the sum of the cos2 is equal to one. 
## In this case the variables will be positioned on the circle of correlations.
## For some of the variables, more than 2 components are required to perfectly represent the data. 
## In this case the variables are positioned inside the circle of correlations.
head(pca$var$cos2)


## The graph below shows the plot of variables on the components. Variables are colored according to the values of the squared cosine :
fviz_pca_var(pca, col.var="cos2") +
scale_color_gradient2(low="white", mid="blue", 
                    high="red", midpoint=0.5) + theme_minimal()



## CONTRIBUTIONS OF VARIABLES TO THE PRINCIPAL COMPONENTS
## The contributions of variables in accounting for the variability in a given 
## principal component are (in percentage) : (variable.cos2 * 100) / (total cos2 of the component)
## The larger the value of the contribution, the more the variable contributes to the component
head(pca$var$contrib)

## CONTRIBUTIONS OF VARIABLES ON PC1
## If the contribution of the variables were uniform, the expected value would be 1/length(variables) = 1/10 = 10%.
## The red dashed line on the graph above indicates the expected average contribution. 
## For a given component, a variable with a contribution larger than this cutoff could 
## be considered as important in contributing to the component.
fviz_pca_contrib(pca, choice = "var", axes = 1)

## CONTRIBUTIONS OF VARIABLES ON PC2
fviz_pca_contrib(pca, choice = "var", axes = 2)

## CONTRIBUTIONS OF VARIABLES ON PC1 AND PC2
fviz_pca_contrib(pca, choice = "var", axes = 1:2)

## Control variable colors using their contributions
fviz_pca_var(pca, col.var="contrib") +
scale_color_gradient2(low="white", mid="blue", 
                  high="red", midpoint=5) + theme_minimal()


## TO IDENTIFY THE MOST CORRELATED VARIABLES WITH A GIVEN PRINCIPAL COMPONENT
## res : an object of class PCA
## axes : a numeric vector specifying the dimensions to be described
## prob : the significance level
dimdesc(pca, axes = 1:3, proba = 0.05)


## QUALITY OF THE REPRESENTATION FOR INDIVIDUALS ON THE PRINCIPAL COMPONENTS
fviz_pca_ind(pca, col.ind="cos2") +
scale_color_gradient2(low="white", mid="blue", 
   high="red", midpoint=0.4) + theme_minimal()


# Contributions of TOP 10 individuals to PC1
fviz_pca_contrib(pca, choice = "ind", axes = 1, top = 10)
```




##Conclusion

+ Summary of proceedure
+ Summary of results
+ Possible improvements and future work




\newpage
#Appendices

##A. Additional Exploratory Data Analysis


```{r , echo = FALSE, eval = TRUE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:heatmap-standardized}Heatmap of standardized and transformed variables."}
# Correlation matrix plot
ggcorr(data_standard.df[, 1:ncol(data_standard.df)], 
       hjust = 0.95, 
       size = 2, 
       label = TRUE, 
       label_size = 2, 
       label_alpha = TRUE, 
       layout.exp = 5, 
       legend.position = "bottom", 
       legend.size = 8) +
  guides(fill = guide_colorbar(barwidth = 10, 
                               barheight = 0.5, 
                               title.vjust = 0.75)) 
#  theme(  plot.margin = margin(1, 1, 1, 1, "cm") ) #top, right, bottom, left
```


```{r , echo = FALSE, eval = FALSE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:density}Density plots of standardized variables."}
data_clean.df %>% 
  na.omit() %>%
  select(-Gender, -Indication) %>%
#  gather(key = Feature, value = Value) %>% 
#  ggplot(aes(x = Feature, y = Value, fill = ECMO_Survival)) +
  gather(type, value, 1:20) %>%
  ggplot(aes(x = value, fill = ECMO_Survival)) + 
    geom_density(alpha=0.3) + 
#    plotTheme() + 
    facet_wrap(~ type, scales = "free") + 
    theme(axis.text.x = element_text(angle = 90, vjust = 1)) + 
    labs(title = "Density Plots of Data across Variables")
```

```{r , echo = FALSE, eval = TRUE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:rfe-logit}Ordered feature importance from Logit model"}
## Recursive Feature Elimination
## Create feature importance plots
# Imputation method: Complete Case
load("../project/_metrics/metrics-complete-case.RData")

importance <- varImp(logit_model, scale = T)
imp_df1 <- importance$importance
imp_df1$group <- rownames(imp_df1)

imp1 <- imp_df1 %>%
  ggplot( aes(x=reorder(group, Overall), y = Overall), size=2 ) + 
  geom_bar(stat = "identity", position = "dodge") + 
  theme(axis.text.y = element_text(vjust=1, angle=0, size=4),
        axis.title.y = element_text(size = 5),
        plot.margin=unit(c(0,0,0,0), "cm")) +  #top, right, bottom, left
  coord_flip() +
  labs(x = "",
       y = "", 
       title = "Complete Case")

# Imputation method: Mean m=1
load("../project/_metrics/metrics-mean.RData")

importance <- varImp(logit_model, scale = T)
imp_df1 <- importance$importance
imp_df1$group <- rownames(imp_df1)

imp2 <- imp_df1 %>%
  ggplot( aes(x=reorder(group, Overall), y = Overall), size=2 ) + 
  geom_bar(stat = "identity", position = "dodge") + 
  theme(axis.text.y = element_text(vjust=1, angle=0, size=4),
        axis.title.y = element_text(size = 5),
        plot.margin=unit(c(0,0,0,0), "cm")) +  #top, right, bottom, left
  coord_flip() +
  labs(x = "",
       y = "", 
       title = "Mean") 

# Imputation method: PMM  m=9
load("../project/_metrics/metrics-pmm.RData")

importance <- varImp(logit_model, scale = T)
imp_df1 <- importance$importance
imp_df1$group <- rownames(imp_df1)

imp3 <- imp_df1 %>%
  ggplot( aes(x=reorder(group, Overall), y = Overall), size=2 ) + 
  geom_bar(stat = "identity", position = "dodge") + 
  theme(axis.text.y = element_text(vjust=1, angle=0, size=4),
        axis.title.y = element_text(size = 5),
        plot.margin=unit(c(0,0,0,0), "cm")) + 
  coord_flip() +
  labs(x = "",
       y = "", 
       title = "PMM m=9")

# Imputation method: PMM  m=99
load("../project/_metrics/metrics-pmm99.RData")

importance <- varImp(logit_model, scale = T)
imp_df1 <- importance$importance
imp_df1$group <- rownames(imp_df1)

imp4 <- imp_df1 %>%
  ggplot( aes(x=reorder(group, Overall), y = Overall), size=2 ) +
  geom_bar(stat = "identity", position = "dodge") + 
  theme(axis.text.y = element_text(vjust=1, angle=0, size=4 ),
        axis.title.y.left = element_text(size = 5),
        plot.margin=unit(c(0,0,0,0), "cm")) +
  coord_flip() +
  labs(x = "", 
       y = "",
       title = "PMM m=99") 
        
## Create Multiplot
require(gridExtra)
grid.arrange(imp1, imp2, imp3, imp4, ncol = 2, top = "Scaled Feature Importance")

```

```{r , echo = FALSE, eval = FALSE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:feature-importance}Ordered feature importance from LDA model"}

```

```{r , echo = FALSE, eval = FALSE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:feature-importance}Ordered feature importance from LDA model fit to imputed dataset method=pmm, m=9"}
load("../project/_metrics/metrics-pmm.RData")

importance <- varImp(lda_model, scale = T)
imp_df1 <- importance$importance
imp_df1$group <- rownames(imp_df1)

imp_df1 %>%
  ggplot( aes(x=reorder(group, Y), y = Y), size=2 ) + 
  geom_bar(stat = "identity") + theme(axis.text.x = element_text(vjust=1,angle=90)) + 
  coord_flip() +
  labs(x = "Variable", 
       y = "Overall Importance", 
       title = "Scaled Feature Importance")
```



##B. Algorithms

###Random Forests Algorithm

The random forests algorithm depicted is adapted from [@hastie_elements_2009].  

\begin{algorithm}[H]

\caption{Random Forest Classifier}
\DontPrintSemicolon
\SetAlgoLined
\BlankLine

\begin{enumerate}
  \item For ($b=1$ to B):
    \begin{enumerate}
      \item Draw a bootstrap sample $\mathbf{Z*}$ of the size $N$ from the training data.
      \item Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repreating the following steps for each terminal node of the tree, until the minimum node size $n_{min}$ is reached.
      \begin{enumerate}
        \item Select $mtry$ variables at random from the $p$ covariates. 
        \item Pick the best covariate/split-point among the $mtry$. 
        \item Split the node into two daughter nodes. 
      \end{enumerate}
    \end{enumerate}
  \item Output the ensemble of trees $\{T_B\}^B_1$
\end{enumerate}
\BlankLine

Let $\hat{Y}_b(x)$ be the class prediction of the $b^{\text{th}}$ random-forest tree.  Then a new observation, $x$, is classified as:

$$\hat{Y}^B_{\text{rf}}(x) = \text{majority vote } \left\{ \hat{Y}_b(x) \right\}^B_1$$

\end{algorithm} 


###MICE Algorithm

The MICE algorithm is adapted from [@van_buuren_flexible_2012].

\begin{algorithm}[H]

\caption{Multiple Imputation via Chained Equations}
\DontPrintSemicolon
\SetAlgoLined
\BlankLine

\begin{enumerate}
  \item Specify an imputation model $P(Y^{\text{mis}}_j \vert Y^{\text{obs}}_j, Y_{-j}, R)$ for variable $Y_j$ with $j=1,...,p$
  \item For each $j$, fill in starting imputation $Y^0_j$ by random draws from $Y^{\text{obs}}_j$
  \item Repeat for $t=1,...,T:$
  \item Repeat for $j=1,...,p:$
  \item Define $Y^t_{-j} = (Y^t_1,...,T^t_{j-1}, Y^{t-1}_{j+1},..., Y^{t-1}_p)$ as the currently complete data except $Y_j$ 
  \item Draw $\phi^t_j \sim P(\phi^t_j \vert Y^{\text{obs}}_j, Y^t_{-j}, R)$.
  \item Draw imputations from $Y^t_j \sim P(Y^{ \text{mis} }_j \vert Y^{ \text{obs} }_j, Y^t_{-j}, R, \phi^t_j)$.
  \item End repeat $j$.
  \item End repeat $t$.

\end{enumerate}
\BlankLine

\end{algorithm} 





##C. Additional Missing Data Diagnostics

```{r , echo = FALSE, eval = TRUE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:missing-data-patterns}Missing data patterns.  Each row corresponds to a missing data pattern (1=observed, 0=missing).  Rows and columns are sorted in increasing amounts of missing information.  The last column and row contain row and column counts, respectively."}
missing_data_patterns <- md.pattern(data_clean.df, plot = TRUE, rotate.names = TRUE) +
  theme_bw()
#  theme(  text = element_text(size = 1),
#          plot.margin = margin(4, 1, 1, 1, "cm") ) #top, right, bottom, left
```


```{r , echo = FALSE, eval = TRUE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:missing-data-patterns}Missing data patterns.  Each row corresponds to a missing data pattern (1=observed, 0=missing).  Rows and columns are sorted in increasing amounts of missing information.  The last column and row contain row and column counts, respectively."}
missing_data_patterns2 <- aggr(data_clean.df, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(data_clean.df), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern")) +
  theme_bw()
```

###Visual Insepction of Imputations

```{r , echo = FALSE, eval = TRUE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:xyplot-mean}Scatterplot of each imputed dataset"}
## Impute concatenated train and test sets
data_standard.df <- data_clean.df %>% 
  preProcess( method = c("center", 
                       "scale",
                       "YeoJohnson"  ## Transformation method
                        )) %>%
  predict(data_clean.df) ## Generate new dataframe

imp_mean <- micemd::mice.par(data_standard.df, 
                         nnodes = 8,
                         meth = "mean", 
                         m = 5,
                         maxit = 5, 
                         seed = 123, 
                         printFlag = FALSE
                         )

#  imp <- complete(imp6, action = "all") # create list of imputed datasets

xyplot(imp_mean, PreECMO_Albumin ~ PreECMO_ATIII | .imp, pch = 20, cex = 0.5)
```

```{r , echo = FALSE, eval = TRUE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:xyplot-mean}Scatterplot of each imputed dataset"}
## Impute concatenated train and test sets
imp_pmm <- micemd::mice.par(data_standard.df, 
                         nnodes = 8,
                         meth = "pmm", 
                         m = 5,
                         maxit = 5, 
                         seed = 123, 
                         printFlag = FALSE
                         )

#  imp <- complete(imp6, action = "all") # create list of imputed datasets

xyplot(imp_pmm, PreECMO_Albumin ~ PreECMO_ATIII | .imp, pch = 20, cex = 0.5)
```

+ **xyplot checking distributions of original and imputed data for MEAN imputation**
+ **xyplot checking distributions of original and imputed data for PMM imputation**

+ **density plot of original and imputed data for MEAN imputation**
+ **density plot of original and imputed data for PMM imputation**

This plot compares the density of observed data with the ones of imputed data. We expect them
to be similar (though not identical) under MAR assumption.


###Convergence Monitoring

+ **Plot of convergence**


##D. Code Structure

The code organization is described in Figure \ref{fig:r-code-chart}.  `libraries.R` contains all the libraries used in the analysis.  `functions.R` contains functions used in `training.R` and `model-evaluation.R`.  The ensemble cross-validation algorithm is done in the `crossValidation()` function.  The data is initially cleaned and split into test and training sets in `preprocess.R`.  The cleaned datasets are saved to `processed-data.RData` for use in `training.R` and in creating tables and figures in the thesis rmarkdown.  The training data is loaded into `training.R` where each of the five classification methods are trained via ensemble cross-validation.  This is done for the four imputation methods: complete case analysis, mean imputation, MICE using PMM for $m=9$, and MICE using PMM for $m=99$ imputed datasets.  The trained models for each imputation method are saved into separate `trained-models.RData`.  The methods are then then fit to the full training set in `model-evaluation.R` using the trained parameters found in `training.R`.  The final fitted models are evaluated on the test set and the fitted models and performance metrics are saved to `metrics.RData`.  

```{r , echo = FALSE, eval = TRUE, fig.pos="H", fig.align = 'center', out.width = '100%', fig.cap = "\\label{fig:r-code-chart}Flowchart of code structure."}
knitr::include_graphics("images/r-code-chart.png")
```





\newpage
