\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Calibri Light}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Combinging Multiple Imputation and Cross-Validation for Predicting Survival of ECMO Treatment in ARDS Patients},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{biblatex}

\addbibresource{bibliography.bib}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Combinging Multiple Imputation and Cross-Validation for Predicting
Survival of ECMO Treatment in ARDS Patients}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  
\usepackage[bottom]{footmisc} \usepackage{float}
\floatplacement{figure}{H} \usepackage{color} \usepackage[table]{xcolor}
\usepackage{multirow} \usepackage{caption}
\captionsetup[table]{skip=5pt, font=footnotesize}
\usepackage[font=footnotesize]{caption} \usepackage{algorithm2e}
\usepackage{amsmath}
\newcommand{\appendixA}{ \setcounter{table}{0} \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{A\arabic{figure}} }
\newcommand{\appendixB}{ \setcounter{table}{0} \renewcommand{\thetable}{B\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{B\arabic{figure}} }
\newcommand{\appendixC}{ \setcounter{table}{0} \renewcommand{\thetable}{C\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{C\arabic{figure}} }
\newcommand{\appendixD}{ \setcounter{table}{0} \renewcommand{\thetable}{D\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{D\arabic{figure}} }
\newcommand{\appendixE}{ \setcounter{table}{0} \renewcommand{\thetable}{E\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{E\arabic{figure}} }

\begin{document}
\maketitle

\vspace{4cm}

\begin{center}
Robert Edwards 

\vspace{0.125cm}
2416963E


\vspace{1cm}
MASTERS THESIS 

\vspace{0.125cm}
Biostatistics

\vspace{9cm}
  \includegraphics[height = 1.5cm]{images/GUlogo.png}
\end{center}

\newpage

\begin{center}
~
 
\vspace{5cm}
Acknowledgements 

\vspace{3cm}
To my peers, alone we sink but together we swim.

\vspace{1cm}
To my family, for keeping me sane in the bipolar Scottish weather. 

\vspace{1cm}
To my friends, for your unbiased indulgence in my regressive statistical puns. 

\vspace{1cm}
To my weird friends, you provided much needed relief from the objectively Normal days. 

\end{center}

\newpage 

\setcounter{tocdepth}{2} \tableofcontents  

\newpage

\section{Introduction}\label{introduction}

Prediction in medical data can often be difficult due to a low number of
observations and poor predictive covariates. If the response class
distributions are imbalanced, then prediction becomes even more
difficult. Some of these issues arise from the experimental design of
the study or for reasons beyond the control of the researcher but little
can be rememdied post-hoc. Missing values in the data complicate the
analysis even further and are often handled either by dropping missing
observations or filling in the missing value by the mean. Both methods
can be valid if certain assumptions hold, but useful information is lost
and bias estimates of means, regression coefficients, and correlations
{[}\textcite{van_buuren_flexible_2012}, pp.8; schafer\_missing\_2002{]}.
Several statistical methods have been proposed for handling missing data
\autocite{schafer_missing_2002}; simple proceedures include complete
case analysis (CC) wherein all observations with missing data are
excluded, single imputation methods that simply use the mean to replace
the missing value (SIM), and more principled methods such as multiple
imputation (MI). Multiple imputation is a widely used flexible method in
datasets with missing values. MI is commonly used in medical studies
\autocites{powney_review_2014}{karahalios_review_2012}{wood_are_2004}
but how it is implemented in often not stated
\autocites{mackinnon_use_2010}{hayati_rezvan_rise_2015}.

\subsection{Study Population \& Data
Description}\label{study-population-data-description}

This paper investigates predicting survival of patients diagnosed with
Acute Respiratory Distress Syndrom (ARDS) after treatment with
Extracorporeal Membrane Oxygenation (ECMO). ARDS is a common and often
fatal cause of respiratory failure among patients who are critically ill
with an estimated global prevalence of 10\% and a mortality of 25-40\%
\autocites{bellani_epidemiology_2016}{rubenfeld_epidemiology_2007}{fan_acute_2018}.
ARDS is syndrome with many disease paths characterized by rapid onset of
widespread inflammation in the lungs \autocite{fan_acute_2018}. ECMO is
a treatment used in \_\_\_ that is thought to help patients with
ARDS{[}@{]}. Studies have reported favorable outcome in young ARDS
patients treated with ECMO but not older \autocite{Sahetya01052018}.
ECMO treatment is a complicated and invasive proceedure with risks that
lead to death \autocite{paolone_extracorporeal_2017}. Identifying
mortality risk of patients before treatment is crucial. Two ARDS
subphenotypes have been identified with distinct clinical and biological
features that are thought to support predictive strategies
\autocites{calfee_acute_2018}{sinha_latent_2018}.

The dataset is composed of 450 observations on patients with Acute
Respiratory Distress Syndrome who underwent ECMO treatment. The response
variable, \texttt{ECMO\_Survival}, is a binary categorical variable for
survival indication with levels ``Y'' and ``N''. There are 33 covariates
included in the analysis, two of which are categorical, and 31
continuous. The categorical variable \texttt{Gender} has two levels,
``m'' and ``f'', and \texttt{Indication} a seven level nominal
categorical indicator of disease type. The continuous variable
\texttt{Age} is also included in the analysis with a minimum age of 18
and a maximum of 83 with a median age of 53. The remaining variables are
biomedical markers from hospital measurements.

\subsection{Aims of the Proposed
Research}\label{aims-of-the-proposed-research}

The main questions of interest investigated in this paper are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Can ECMO treatment survival (\texttt{ECMO\_Survival}) be accurately
  predicted by PreECMO biomedical markers?
\item
  What is the future expected performance of predictions?
\item
  Which biomedical markers are needed for accurate prediction and which
  can be dropped?
\end{enumerate}

\subsection{Layout}\label{layout}

To further the goals of this paper multiple imputation is investigated
for increasing prediction performance on ECMO treatment survival. This
method both allows retention of observations in the analysis as well as
accounts for the uncertainty of the imputed value. The advantages come
at the cost of complexity and increased computation time; multiple
datasets are be imputed and results pooled.

This paper begins by explaining how the data are cleaned and an
explanation of the proceedure to pool results from MI in
cross-validation. An explanantion of imputaiton methods considers
follows. Finally, an explanation of the considered classification
methods and how each is implemented then follows. Lastly, a discussion
of the results from predictions on each imputed dataset.

\newpage

\section{Methodology}\label{methodology}

\subsection{PreProcessing}\label{preprocessing}

Before analysis the data are standardized by mean-centering and scaling
so the standard deviation is 1. The standardizing of variables in
important in classification because variables measured at different
scales do not contribute equally to the analysis. For example, the
K-Nearest Neighbors method uses a distance metric to distinguish
classes; a variable on a scale of 0 to 100 will be analyzed differently
than a variable with a range of 0 to 1.

In addition to standardizing, the continuous variables are also
transformed so the distributional form of the data is multivariate
normal. Some nonparametric classification methods assume the data is
multivariate normally distributed and can have better prediction
performance if the assumption is true. Van Buuren
\autocite*[pp.106-107]{van_buuren_flexible_2012} also suggests
transforming the data toward normailty for multiple imputation. The data
are transformed using the Yeo-Johnson transformation
\autocite{yeo_new_2000}. The Yeo-Johnson transformation is similar to a
Box-Cox transformation except it can accomodate covariate with zero
and/or negative values.

\subsection{Validation \&
Cross-validation}\label{validation-cross-validation}

When building a classification model, it is important to asses its
ability to produce valid predictions. If there are ample number of
observations, one way to asses model performance is to randomly split
the dataset into training, validation, and test sets. The training set
is used to fit the model, which is then used to predict the classes for
the observations in the validation set; the validation set is used to
estimate prediction error and tune hyperparameters for model selection;
the test set is used to estimate future prediction performance for the
model/hyperparameters chosen. To simulate the model predicting on
future, unseen data, the test set should be kept isolated. The model can
overfit the data if feature manipulation and hyperparamter tuning are
done before randomly splitting the data. If standarzation and
transformation of the covariates is done on the entire dataset,
information from the training set can ``leak'' into the test set and the
true test error will be underestimated.

If there is insufficient data to split into three parts then a suitable
alternative is \(K\)-fold cross-validation. It is one of the simplest
and most widely used method for estimating prediction error
\autocite{hastie_elements_2009}. The data is randomly split into \(K\)
folds, where the \(K^{th}\) fold is taken as the validation set and the
the remaining \(K-1\) folds are used for training the model. The
procedure is then repeated \(K\) times and the prediction error
averaged. \(K\)-fold cross validation is most useful on sparse datasets
as it allows more observations to be used in training the model. The
choice of \(K\) can effect the variability of the prediction error; if
\(K=1\), the model will overfit the data and prediction error will be
highly variable and if \(K=n\) (the number of observation in the
dataset), the model is fit with no validation set for training
parameters. Typical values used are \(K=5\) \& \(10\)
\autocites{hastie_elements_2009}{breiman_submodel_1992}{kohavi_study_1995}.

\subsection{Models}\label{models}

There are many classification methods, some perform well on many types
of data and others perform better on certain types of data. A variety of
classification methods are explored toward the aim of predicting
survival of ECMO treatment, including parametric methods with many
assumptions and high bias as well as non-parametric methods with higher
variability. The five explored on the ARDS dataset in this paper are:
Logistic Regression, Linear Discriminant Analysis, Quadratic
Discriminant Analysis, K-Nearest Neighbors, and Random Forests.

\emph{Logistic Regression:}\\
Logistic regression is a widely used approach in machine learning and
medicine for binary classification. It is a generalisation of linear
regression that models the posterior probabilities of the \(Y\) classes.
A logit link is used to ensure the posterior probabilities sum to one
and are bounded by {[}0,1{]}. For two classes, let \(Y_i\) be
independent Bernoulli random variables, then the model has the form

\[
\text{logit} \Big( \text{Pr}(Y_i \vert X_i) \Big) = \text{log} \frac{ \text{Pr}(Y_i=1 \vert X_i) }{ \text{Pr}(Y_i=2 \vert X) }  = \mathbf{x}^T_i\boldsymbol{\beta}  
\]

where \(\mathbf{x}^T_i\) is the design matrix. The posterior
probabilities are estimated by maximizing the log-likelihood function to
find the parameter estimates, \(\hat{\boldsymbol{\beta}}\), to obtain
estimates of the probabilities:

\[
\text{Pr}(Y_i=1 \vert X) = \frac{ \text{exp}(\mathbf{x}^T_1 \hat{\boldsymbol{\beta}}) }{ 1 + \sum^2_{i=1} \text{exp}(\mathbf{x}^T_i \hat{\boldsymbol{\beta}}) }
\]

Logitistic Regression (Logit) is implemented with a logit link function
using the \texttt{"glmnet"} method in the \emph{caret} package. The
parameters settings are \texttt{alpha\ =\ 1} and \texttt{lambda\ =\ 0}
to suppress regularization.

\emph{LDA and QDA:}\\
Discriminant Analysis is a widely used set of classification methods. A
generlization of Fisher's Linear Discriminant
\autocite{fisher_use_1936}, discriminant functions are created through a
combination of the explanatory variables that characterize the classes.

Let \(p(X_i \vert Y_i)\) be the densities of distributions of the
observations for each class where \(Y_i\) are independent Bernoulli
random variables and let \(\pi_{Y_i}\) denote the prior probabilities of
the \(Y^{th}_i\) class; that is, the prior probability that a randomly
sampled observation belongs to the \(Y_i^{th}\) class based on the class
proportions. The posterior probabilities may be written using Bayes
Theorem as:

\[
p(Y_i \vert X_i) = \frac{p(X_i \vert Y_i) ~\pi_{Y_i}}{p(X_i)} \propto p(X_i \vert Y_i) ~\pi_{Y_i}   \tag{1}
\]

Suppose the class distribution for class \(Y_i\) is Multivariate Normal
with mean \(\mu_{Y_i}\) and covariance matrix \(\Sigma_{Y_i}\), so that:

\[
p(X \vert Y_i) = \frac{1}{(2 \pi_{Y_i})^{p/2} \vert\boldsymbol{\Sigma}_{Y_i} \vert ^{1/2}} \text{exp} \left[-\frac{1}{2}(X - \mu_{Y_i})^T \boldsymbol{\Sigma}^{-1}_{Y_i}(X - \mu_{Y_i})  \right]  \tag{2}
\]

In comparing two classes, it is sufficient to look at the log-ratio: \[
\text{log} \frac{\text{Pr}(Y_i=1 \vert X)}{\text{Pr}(Y_i=2 \vert X)} = \text{log}\frac{p(X \vert Y_i=1)}{p(X \vert Y_i=2)} + \text{log}\frac{\pi_1}{\pi_2}   \tag{3}
\]

and using Bayes Discriminant Rule stating that \emph{an observation
should be allocated to the class with the largest posterior
probability}. From Equation (1), the posterior probability may be
written as \[
p(Y_i \vert X) \propto \text{exp} \left( Q_{Y_i} \right)    \tag{4}
\]

where discriminant function is

\[
Q_{Y_i} = (X - \mu_{Y_i}) \Sigma^{-1}_{Y_i} (X - \mu_{Y_i})^T + \text{log} \vert \Sigma_{Y_i} \vert - 2\text{log} ~\pi_{Y_i}   \tag{5}
\]

for class \(Y\). The Bayes Discriminant Rule is then: \emph{allocated
the observation to the class with the largest discriminant function,
\(Q_Y\)}. This method of classification is called \emph{Quadratic
Discriminant Analysis} (QDA) because the decision boundaries between
classes are elliptical and defined by \(Q_Y\), an equation quadratic in
\(X\). If the covariance matrix, \(\Sigma_Y\) is assumed to be equal for
each class then the discriminant function is defined as

\[
L_Y = X \Sigma^{-1} \mu_Y^T -\frac{1}{2}\mu_Y \Sigma^{-1} \mu_Y^T  - \text{log} ~\pi_Y     \tag{6}
\]

This method has linear decision boundaries between classes defined by
\(L_Y\), an equation linear in \(X\), and is known ad \emph{Linear
Discriminant Analysis} (LDA). The Bayes Discriminant Rule is then:
\emph{allocated the observation to the class with the largest
discriminant function, \(L_Y\)}.

There is a bias-variance trade-off; both assume the covariates are
normally distributed, there is no multicollinearity, and the
observations are independent \autocite{cover_geometrical_1965}. LDA
additionally assumes equal class covariances. Discriminant Analysis can
only utilize continuous covariates with no missing observations. The
bias from simple linear or quadratic class boundaries can be acceptable
because it is estimated with less variance. Despite the many assumptions
and limitations, both LDA and QDA are widely used and perform well on on
a diverse set of classification tasks \autocite{hastie_elements_2009},
even when the classes are not normally distributed.

Logitistic Regression (Logit) is implemented using the \texttt{"lda"}
and \texttt{"qda"} methods in the \emph{caret} package.

\emph{K-Nearest Neighbors:}\\
\(K\)-Nearest Neighbors (KNN) is a commonly used non-parametric
classification method. To predict the class of a new observation, a
distance matrix is constructed between all observations and the K
nearest labelled observations to the new observation are considered. The
new observation is then assigned the class label that the majority of
its neighbors share. In case of only two classes, ties in class
assignments are avoided by using odd values of K. In the event of a tie,
a class can be chosen at random. Various distance metrics may be used
but it is common to use Euclidean distance to determine the closest
training points, though it is advisable to scale variables so that one
direction does not dominate the classification
\autocite[pp.456]{hastie_elements_2009}.

KNN is sensitive to the local structure of the data. As \(K\) increases,
the variability of the classification tends to decrease at the expense
of increased bias. KNN is implemented using the \texttt{"kknn"} method
in the \emph{caret} package with a grid search over the number of
observations considered in classifying a new observation,
\(k=3,...,19\). A Gaussian kernel and Euclidean distances are used.

\emph{Random Forests:}\\
Random Forests \autocite{breiman_random_2001} are one of the most
successful general-purpose modern algorithms\autocite{biau_random_2016}.
They are an ensemble learning method that can be applied to a wide range
of tasks, namely classification and regression. A random forest is
created by building multiple decision trees, where randomness is
introduced during the construction of each tree. Predictions are made by
classifying a new observation to the mode of the multiple decisions tree
classifications. Random forests often make accurate and robust
predictions, even for very high-dimensional
problems\autocite{biau_analysis_2012}. See Figure
\ref{fig:alg:random-forests-alg} in Appendix B for an explanation of the
random forests algorithm. Random Forests (RF) are implemented using the
\texttt{"rf"} method in the \emph{caret} package with a grid search over
the number of variables considered at each split, \(mtry=3,...,15\).

\subsection{Accuracy Metrics}\label{accuracy-metrics}

A classification method is typically assesed using a confusion matrix.
Table \ref{tab:confusion-matrix} represents a confusion matrix for a
binary classification.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-1}\label{tab:confusion-matrix} Confusion matrix for two classes.}
\centering
\fontsize{12}{14}\selectfont
\begin{tabular}{lc|cc}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{2}{c}{Observed} \\
\cmidrule(l{3pt}r{3pt}){3-4}
  &   & N & Y\\
\midrule
\rowcolor{gray!6}   & N & a & b\\

\multirow{-2}{*}{\raggedright\arraybackslash Predicted} & Y & c & d\\
\bottomrule
\end{tabular}
\end{table}

Accuracy is the percentage of correctly classifies instances out of all
instances. It is often a poor performance metric to use alone. There are
two significant problems with it. Accuracy applies a naive 0.50
threshold to decide between classes, and this is usually wrong when the
classes are imbalanced. Second, classification accuracy is based on a
simple count of the errors. It does not provide information on which
classes are being improperly classified or where. For the two class
confusion matrix in Table \ref{tab:confusion-matrix} accuracy is defined
as:

\[
\text{accuracy} = \frac{a+d}{a+b+c+d}
\]

For binary classification, sensitivity and specificity provide more
insight into performance of a classifier.

\[
\begin{aligned}
\text{sensitivity} &= \frac{a}{a+c} \\
\text{specificity} &= \frac{d}{b+d}
\end{aligned}
\] Here, sensitivity is a measure of how accurately non-survival is
predicted, specificity is a measure of how accurately survival is
predicted, and accuracy is a measure of how well both survival and
non-survival are predicted. While sensitivity and specificity state the
accuracy each class prediction, accuracy is a poor measure for model
performance in an imbalanced dataset. On the ARDS datasets, for example,
if \texttt{ECMO\_Survival} is predicted to be ``Y'' for all cases, then
the accuracy is 75\% but the prediction is no better than the baseline
likelihood of the class proportions.

\emph{Cohen's Kappa:} Kappa or Cohen's Kappa
\autocite{cohen_coefficient_1960} is a classification performance metric
that is normalized at the baseline of random chance on the dataset. It
is a useful performance measure on problems with imbalanced classes.
Kappa is defined as:

\[
\kappa = \frac{p_o - p_e}{1 - p_e}
\] where \(p_o\) is simply the accuracy, the relative observed agreement
between observed and predicted classes and \(p_e\) is the probability of
chance agreement based on the class probabilities. \[
p_o = \frac{a+d}{a+b+c+d}  ~~~~~\text{and}~~~~~ p_e = p_{o,Y} + p_{o,N} 
\]

where \[
p_{o,Y} = \frac{a+d}{a+b+c+d} ~\cdot~ \frac{a+c}{a+b+c+d}
\]

\[
p_{o,N} = \frac{c+d}{a+b+c+d} ~\cdot~ \frac{b+d}{a+b+c+d}
\]

Kappa is used in this paper to compare the performance of different
classifiers. A classifier with a larger Kappa is considered to predict
better than a classifier with a lower Kappa. If all the observations are
predicted correctly then \(\kappa=1\). If the observations are predicted
no better than expected by the class probabilities, \(p_e\) then
\(\kappa=0\). If all the observations are predicted incorrectly, then
\(\kappa=-1\). A positive \(\kappa\) indicates that the model predicts
better than would be expected by chance whereas a negative \(\kappa\)
indicates that the model predicts worse than would be expected by
chance.

\subsection{Missing Data}\label{missing-data}

Missing data is a common problem that must be dealt with in machine
learning, statistics, and medicine. Understanding the missing mechanism
for the missing observations is important in the analysis.
\autocite{rubin_inference_1976} defined three types of missing data
mechanisms: missing completely at random (MCAR), missing at random
(MAR), and missing not at random (MNAR). The data are said to be missing
completely at random (MCAR) if the probability of being missing is the
same for all cases. This implies the causes of the missing data are
unrelated to the data itself. While MCAR is conveinient because it
allows many complexities that arise because data are missing to be
ignored, it is typically an unrealistic assumption
\autocite{van_buuren_flexible_2012}. The data is said to be MAR if the
probability of being missing is the same only within groups defined by
the observed data. MAR is a more general and more realistic assumption
than MCAR. If neither MCAR nor MAR applies, then the probability of
being missing depends on an unknown mechanism and said to be MNAR. Most
simple approaches to dealing with missing data are only valid under MCAR
assumption. Modern methods to dealing with missing data begin from the
MAR assumption.

\subsection{Imputation Methods}\label{imputation-methods}

\emph{Complete Case Analysis:} Complete case canalysis is a convenient
method for handling missing data and is the default method in many
statistical packages. If there is a missing value in an observation, it
is dropped from the analysis. This is often a poor appraoch as complete
cases analysis assumes MCAR. In sparse datasets a complete case analysis
can cause an analysis to be underpowered and if MCAR does not hold, can
severely bias estimates of means, regression coefficients, and
correlations \autocite{van_buuren_flexible_2012}.

The ARDS dataset considered in this paper has 268 of 450 observations
with missing data.

\emph{Mean Imputation:}\\
Another common method for handling missing data is mean imputation; the
missing value is replaced by the mean of the observed values (the mode
for categorical data). This approach is satisfactory for a moderate
amount of MCAR-generated missing values. However, it distorts the
distribution of the data by reducing the variance of the imputed
variables and the correlations between variables
\autocite{little_bayes_2014}. Van Buuren suggests mean imputation should
only be used only when there are few missing values, and should be
generally avoided \autocite{van_buuren_flexible_2012}. Mean imputation
(SI1) is implemented using the \texttt{"mean"} method in the
\emph{micemd} package with the number of imputations set as \(m=1\).

\emph{Multiple Imputation:}\\
Multiple imputation is a method that accounts for the uncertainty in the
imputed values. The observed dataset is imputed multiple times to create
\(m>1\) complete datasets. The imputed values are drawn from a
distribution specifically modeled for each missing entry. The \(m\)
datasets are analyzed using the same method that would have been used
had the data been complete. The results will differ because of the
variation in the input data caused by the uncertainty in the imputed
values.

Multiple imputation can handle data that is both MAR and MNAR.

There is uncertainty as to the true value of the unseen data, and that
uncertainty should be included in the analysis. Multiple imputation is a
method created by Donald Rubin wherein multiple datasets are imputed,
the analysis is conducted on each dataset, and the results are pooled.
Details of the \emph{MICE} algorithm can be found in Algorithm
\ref{alg:mice-alg}.

\emph{Predictive Mean Matching:}\\
Predictive Mean Matching (PMM) is a semi-parametric imputation approach
to imputing missing values. It fills in a value randomly from among the
a observed donor values from an observation whose regression-predicted
values are closest to the regression-predicted value for the missing
value from the simulated regression model \textbf{(Heitjan and Little
1991; Schenker and Taylor 1996)}. PMM method ensures that imputed values
are plausible; it might be more appropriate than the regression method
(which assumes a joint multivariate normal distribution) if the
normality assumption is violated \textbf{(Horton and Lipsitz 2001,
p.~246)}. PMM is fairly robust to transformations of the target
variables \autocite{van_buuren_flexible_2012}, yielding similar results
for a Yeo-Johnson transformation or no transformation.

\begin{itemize}
\tightlist
\item
  \textbf{Equations for Predictive Mean Matching}
\end{itemize}

Multiple imputation is implemented using the \texttt{"pmm"} method in
the \emph{micemd} package with the number of imputations set as \(m=9\)
and \(m=99\).

\subsection{Ensemble Multiple
Imputation}\label{ensemble-multiple-imputation}

While the topic of multiple imputation has been widely researched, how
to best use multiple imputation in conjunction with cross-validation has
not. Two approaches have been proposed for pooling results from several
SVMs \autocite{belanche_handling_2014} and Cox regression
\textbf{(Zavrakidis 2017)} from multiply imputed datasets. The method is
to concatenate the \(m\) imputed datasets and fit a classifier, and
optimize, to the resulting set; this accounts for the variability of the
parameter estimates as well as the variability of the training
observations in relation to the imputed values
\autocite{belanche_handling_2014}. The second proceedure fits separate
classifiers to each imputed data set and get the pooled (i.e.~avereaged)
performance of the \(m\) classifiers. Results from both studies either
show similar results between appraoches \textbf{(Zavrakidis 2017)} or
slightly better performance with the first approach
\autocite{belanche_handling_2014}. For simplicity and the sake of
computational costs, this paper, only considers the first approach as
outlined in Figure \ref{fig:ensemble-imputation}.

The following steps describe the ensemble approach for multiply imputed
data in k-fold cross-validation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Randomly partition the training data into \(k\) folds while retaining
  class proportions
\item
  Define the \(k^{th}\) as the validation set and the remaining \(k-1\)
  folds as the training set
\item
  Impute the training set \(m\) times, with the response variable
  \texttt{ECMO\_Survival} included, to create \(m\) imputed training
  sets
\item
  Concatenate the \(m\) imputed training sets into one extended training
  set
\item
  A model is fitted to the extended training set
\item
  The validation set is concatenated with the extended training set
\item
  Impute the combined validation and extended training set, with the
  response variable \texttt{ECMO\_Survival} excluded, to create \(m\)
  imputed combined validation and extended training sets
\item
  Extract the \(m\) validation sets
\item
  Make \(m\) predictions on the \(m\) imputed validation sets
\item
  Take the majority vote of the \(m\) predictions as the prediction for
  the fitted model
\item
  Validate the prediction against the validation set by calculating
  Cohen's Kappa (note there are no missing values for the response
  variable in the data)
\item
  Repeat steps 2-11 \(k\) times and validate the fitted model on each
  training set against the test set for each fold
\item
  Average the \(k\) calculated Cohen's Kappas as the estimated in-sample
  performance
\end{enumerate}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/ensemble-imputation} 

}

\caption{\label{fig:ensemble-imputation}Outline of Kth step in the ensemble algorithm used to combine MI in cross-validation.  (a) The Kth fold is taken as the test set (valid) and the remaining K-1 folds are taken as the training set. (b) Valid is separated from the analysis. (c) Train is imputed m times with the response included. (d) The m imputed datasets are 'stacked' to form one training set.  (e) Valid is concatenated with the imputed and 'stacked' training set.  (f) The test set is imputed m times using the imputed training set without the response included.  (g) A model is fitted to the imputed and 'stacked' training set.  (h) The fitted model makes predictions on each of the m valid sets.  (i) The m predictions are pooled by a majority vote.}\label{fig:unnamed-chunk-2}
\end{figure}

``Rubin's Rules'' \autocite{rubin_inference_1976} provide a simple
method for pooling parameters estimates from multiple imputation for
linear and generalized linear models but to the author's knowledge,
there has been insufficient work on estimating the required number of
imputations for estimating posterior probabilities in classification
problems. The classic advice for the choice of \(m\) is between 3 and 5
for moderate amounts of missing information but it is often beneficial
to set \(m\) higher and create between 20-100 imputations
\autocite[pp.112-113]{van_buuren_flexible_2012}.

There has been sufficient exploration into pooling of posterior
probabilities resulting from classification problems
\autocite{kittler_combining_1996}\autocite{james_majority_1998}, but
there has been little research into the pooling of predictions in
classification problems on multiply imputed datasets
\autocite{belanche_handling_2014}. Pooling multiple predictions can be
implemented using a variety of strategies, among which majority vote is
one of the simplest, and has been found to be just as effective as more
complicated schemes \autocite{lam_optimal_1995}. Indeed, others have
pooled predictions from various classification methods by taking the
majority vote
\autocite{james_majority_1998}\autocite{belanche_handling_2014} and
comparing prediction performance.

This study involved four phases: (a) complete case analysis (CC) with
the variable \texttt{PreECMO\_Albumin} dropped from the analysis due to
46.44\% missingness, (b) mean imputation (SI1) on variables with missing
values, (c) imputation via the MICE algorithm implemented with PMM for
\(m=9\) imputed datasets (MI9), and (d) imputation via the MICE
algorithm implemented with PMM for \(m=99\) imputed datasets (MI99). In
each phase the data is first randomly stratified into 75\% training and
25\% test sets, with the test set held-out. Five classification models
are trained (Logit, KNN, LDA, QDA, RF) and tuned in 10-fold
cross-validation using the ensemble imputation described in Figure
\ref{figensemble-imputation}. To determine the expected performance of
future predictions, one iteration of the ensemble imputation is
conducted for the full training set, the held-out test set, and the
tuned classification methods.

\subsection{Feature Selection}\label{feature-selection}

One of the goals of this analysis is to identify the variables most
useful for accurate prediction. There are various methods that can be
used for feature selection: stepwise selection, Recursive Feature
Elimination (RFE), LASSO regularization, and principle Component
Analysis (PCA). However, some of these methods are either highly
criticized, dependent on the classification method considered, or cannot
be integrated into the ensemble cross-validation approach used. Stepwise
selection, while very common, is only applicable to regression models
and it is often criticised \autocite{kemp_applied_2003}; problems
include falsely narrow confidence intervals for effects and predicted
values \autocite{altman_bootstrap_1989} and multiple hypothesis testing
inflating risks of capitalising on chance features of the data
\autocite{altman_practical_1991}, such as noise covariates gaining entry
into the model when the number of candidate variables is large
\autocite{derksen_backward_1992}. RFE is an iterative procedure
analogous of backward feature selection. A new classifier is trained on
a subset of the features and the importance of the feature is a measure
of the change in performance. The training time scales linearly with the
number of classifiers to be trained \autocite{guyon_gene_2002}. Both
logistic regression with LASSO regularization
\autocite{tibshirani_regression_1996} and the analogous Sparse
Discriminant Analysis \autocite{clemmensen_sparse_2011} are embedded
feature selection methods that are dependent on the classification
method.

principle Component Analysis (PCA) \autocite{f.r.s_liii._1901} is a
feature extraction method that is independent of the classification
method. The training set are orthogonally transformed into new
uncorrelated variables called principle components that are linear
combinations of the original variables. Feature extraction is
accomplished by selecting the \(k\) largest principle components that
contain a chosen percent of the variance in the original feature space.

PCA can also be used for feature selection by calculating the
contribution of each variable to the extracted features
\autocite{song_feature_2010}. Let \(C_i\) be the contribution of a given
variable on the principle component, \(\text{V}_i\), and let
\(\lambda_i\) be the eigenvalue of \(\text{V}_i\), where
\(\text{V}_{i} = \lambda_i \text{C}_i\). The eigenvalues measure the
amount of variation retained by each principle component. The total
contribution of a variable, \(\text{C}_j\), on explaining the variations
retained by \(k\) extracted features, \(\text{V}_1, ..., \text{V}_k\),
is

\[
\text{C}_j = \sum^k_{i=1}\lambda_{ij} \text{C}_{ij} = \sum^k_{p=1} \vert \text{V}_{ij} \vert 
\]

The \(\text{C}_j\) are sorted in descending order where \(\text{C}_1\)
contributes the most variation to the extracted principle components
among all the \(\text{C}_j\) for \(j=1,2,...p\), variables. Variables at
the beginning of the sorted list are considered more important for the
analysis than variables at the end. Here, any variable that contributes
more than the expected average contribution, if all variables
contributed equally, is selected as important for the analysis.

\begin{itemize}
\tightlist
\item
  \textbf{How are the top \(C_j's\) chosen?}
\end{itemize}

The number of principle components retained, \(k\), is based on the
proportion of variance retained of the \(p\) principle components, where
the variance threshold is chosen to be 80\%.

\[
0.8 = \frac{\sum^k_{i=1} \lambda_i}{\sum^p_{j=1} \lambda_j}
\] The number of principle components retained is based on the
proportion of variance. If the contribution of the \(p\) variables were
uniform, the expected value would be \(1/\text{p} =\) 0.03. For a given
component, an observation with a contribution larger than this cutoff
could be considered as important in contributing to the component.

Variable selection is implemented using the \texttt{FactoMineR} package

\newpage

\section{Exploratory Data Analysis}\label{exploratory-data-analysis}

To get an idea of the distribution of the data, the following summary
statistics were obtained for the categorical variables in Table
\ref{tab:categorical-summaries} and for the continuous variables in
Figure \ref{fig:violin-standardized}.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-3}\label{tab:categorical-summaries} Summary statistics for categorical variables.}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}{lccc}
\toprule
Variable & Level & n & \%\\
\midrule
 & N & 109 & 24.22\\

\multirow{-2}{*}{\raggedright\arraybackslash ECMO\_Survival} & Y & 341 & 75.78\\
\cmidrule{1-4}
 & m & 305 & 67.78\\

\multirow{-2}{*}{\raggedright\arraybackslash Gender} & w & 145 & 32.22\\
\cmidrule{1-4}
 & 1 & 66 & 14.67\\

 & 2 & 181 & 40.22\\

 & 3 & 31 & 6.89\\

 & 4 & 28 & 6.22\\

 & 5 & 71 & 15.78\\

 & 6 & 12 & 2.67\\

\multirow{-7}{*}{\raggedright\arraybackslash Indication} & 7 & 61 & 13.56\\
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:categorical-summaries} shows that the repsonse variable
\texttt{ECMO\_Survival} is imbalanced; of the 450 individuals, only
75.78\% in the study sample survived ECMO treatment (341 survived vs 109
did not survive). The variable \texttt{Gender} is also imbalanced with
only 67.78\% of the individuals in the study sample are male (305 male
vs 145 female). The distribution disease indication, \texttt{Indication}
shows a majority are of level 2 and levels 3, 4, and 6 relatively rare
occurances in this dataset.

Many of the standardized continuous variables in Figure
\ref{fig:violin-standardized} are highly skewed with a number of
outliers. This can affect the performance of discriminant analysis
classification methods that assume a distributional form for the data
\autocite{hastie_elements_2009}.

The heatmap in Figure \ref{fig:heatmap-standardized} shows only a few
variables with moderate to strong correlation. Only a few variables,
\texttt{PreECMO\_NAdose} and \texttt{PreECMO\_Lactate}, are moderately
correlated with many other variable. Feature selection methods based on
the correlation matrix may not show strong feature importance for a
subset of the variables.

\subsection{Missing Data Exploration}\label{missing-data-exploration}

Before imputation, and indeed multiple imputation, it is important to
inspect the missingness patterns in the data and check assumptions.
Figure \ref{fig:missing-data} shows the missingness patterns in the
dataset, where a black bar represents a missing value. Many missing
values occur in observations with other missing values. The missing
values could be conditionally dependent on other variables, in which
case the data would be MAR. The missing values could also be due to some
unknown mechanism at the time of recording (\emph{i.e.} a failure of the
measurement device) that happens to effect multiple readings (the
biomarkers are measured from blood samples and measurements are likely
done in batches). In this case the data would be MCAR. \textbf{Without
more information, this analysis assumes the data is MCAR.}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/graphics-unnamed-chunk-4-1} 

}

\caption{\label{fig:missing-data}Visual representation of missing observations in the ARDS dataset.}\label{fig:unnamed-chunk-4}
\end{figure}

From Figure \ref{fig:missing-data}, \texttt{PreECMO\_Albumin} is seen to
have 46.44\% missingness. To conserve more observations for the training
set, \texttt{PreECMO\_Albumin} is dropped from the complete case
analysis. Of the remaining variables only half contain missing values
with moderate to low missingness up to 6\%. \textbf{An analysis into how
difficult variables are to impute and how helpful variables are when
imputing is beyond the scope of this paper but is provided in Appendix
C.}

Table \ref{tab:missing-statistics} provides some measures about variable
dependence in the dataset. The first column shows the probability of
observed values for each variable. The following are coefficients that
give insight into how the variables are connected in terms of
missingness. \(\mathbf{Influx}\) is the ratio of the number of variables
pairs \((Y_j, ~Y_k)\) with \(Y_j\) missing and \(Y_k\) observed, divided
by the total number of observed data. For a variable that is entirely
missing, influx is 1, and 0 for if the variable is complete.
\(\mathbf{Outflux}\) is defined in the opposit manner, by dividing the
number of pairs \((Y_j, ~Y_k)\) with \(Y_j\) observed and \(Y_k\)
missing, by the total number of complete cells. For a completely
observed variable, outflux will have a value of 1 and 0 if completely
missing. Outflux gives an indication of how useful the variable will be
for imputing other variables in the dataset, while influx is an
indicator for how easily the variable can be imputed. Table
\ref{tab:missing-statistics} shows that all variables will be useful
during impuation except \texttt{PreECMO\_Albumin}. A high outflux
variable might turn out to be useless for the imputation procedure if it
is unrelated to the incomplete variables, while the usefulness of a
highly predictive variables is severely limited by a low outflux value
\autocite*{van_buuren_flexible_2012}.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-5}\label{tab:missing-statistics} Missing pattern statistics for variables in dataset.}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}{lrrr}
\toprule
  & Proportion & Influx & Outflux\\
\midrule
ECMO\_Survival & 1.00 & 0.00 & 1.00\\
Gender & 1.00 & 0.00 & 1.00\\
Indication & 1.00 & 0.00 & 1.00\\
Age & 1.00 & 0.00 & 1.00\\
PreECMO\_RR & 0.97 & 0.03 & 0.85\\
\addlinespace
PreECMO\_Vt & 0.97 & 0.03 & 0.85\\
PreECMO\_FiO2 & 1.00 & 0.00 & 1.00\\
PreECMO\_Ppeak & 0.97 & 0.03 & 0.85\\
PreECMO\_Pmean & 0.98 & 0.02 & 0.90\\
PreECMO\_PEEP & 0.97 & 0.03 & 0.85\\
\addlinespace
PreECMO\_PF & 1.00 & 0.00 & 1.00\\
PreECMO\_SpO2 & 1.00 & 0.00 & 1.00\\
PreECMO\_PaCO2 & 1.00 & 0.00 & 1.00\\
PreECMO\_pH & 1.00 & 0.00 & 1.00\\
PreECMO\_BE & 1.00 & 0.00 & 1.00\\
\addlinespace
PreECMO\_Lactate & 1.00 & 0.00 & 0.99\\
PreECMO\_NAdose & 1.00 & 0.00 & 1.00\\
PreECMO\_MAP & 0.99 & 0.01 & 0.97\\
PreECMO\_Creatinine & 1.00 & 0.00 & 1.00\\
PreECMO\_Urea & 0.98 & 0.02 & 0.94\\
\addlinespace
PreECMO\_CK & 0.95 & 0.05 & 0.87\\
PreECMO\_Bilirubin & 0.97 & 0.03 & 0.91\\
\textbf{PreECMO\_Albumin} & \textbf{0.54} & \textbf{0.46} & \textbf{0.26}\\
PreECMO\_CRP & 0.94 & 0.05 & 0.88\\
PreECMO\_Fibrinogen & 0.96 & 0.04 & 0.85\\
\addlinespace
PreECMO\_Ddimer & 0.95 & 0.04 & 0.86\\
PreECMO\_ATIII & 0.94 & 0.06 & 0.84\\
PreECMO\_Leukocytes & 1.00 & 0.00 & 0.99\\
PreECMO\_Platelets & 1.00 & 0.00 & 1.00\\
PreECMO\_TNFa & 0.98 & 0.02 & 0.93\\
\addlinespace
PreECMO\_IL6 & 1.00 & 0.00 & 1.00\\
PreECMO\_IL8 & 0.98 & 0.02 & 0.93\\
PreECMO\_siIL2 & 0.95 & 0.05 & 0.87\\
\bottomrule
\end{tabular}
\end{table}

\newpage

\section{Results}\label{results}

\subsection{Prediction Performance}\label{prediction-performance}

This study involved four phases: (a) complete case analysis with the
variable \texttt{PreECMO\_Albumin} dropped from the analysis due to
46.44\% missingness, (b) mean imputation on variables with missing
values, (c) imputation via the MICE algorithm implemented with PMM for
\(m=9\) imputed datasets, and (d) imputation via the MICE algorithm
implemented with PMM for \(m=99\) imputed datasets.

The dataset was split into 75\% training and 25\% test with class
proportions preserved. The five classification models were trained in
10-fold cross-validation using the ensemble imputation approach. Table
\ref{tab:cv-kappa} shows the averaged Kappa from each analysis in
10-fold cross-validation. In complete case analysis and mean imputaion,
LDA is the highest performer. While for predictive mean-matching with
\(m=9\) and \(m=99\) logistic regression has the highest averaged Kappa.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-6}\label{tab:cv-kappa} Averaged Cohen's Kappa for each model fitted in cross-validation.  The tuned parameters for KNN and RF on each imputation emthod are (a) K=5 and mtry=11(b) K=5 and mtry=11(c) K=5 and mtry=13 (d) K=13 and mtry=15, respectively.}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}{lrrrrr}
\toprule
  & Logit & LDA & QDA & KNN & RF\\
\midrule
CC & 0.139 & 0.205 & 0.038 & 0.053 & 0.035\\
SI1 & 0.191 & 0.220 & 0.040 & 0.136 & 0.085\\
MI9 & 0.179 & 0.124 & 0.106 & 0.088 & 0.136\\
MI99 & 0.185 & 0.158 & 0.037 & 0.127 & 0.177\\
\bottomrule
\end{tabular}
\end{table}

\emph{Validation on Test Set:}\\
Using the parameters values learned in cross-validation, models were fit
on the full training set and validated against the test set. In complete
case analysis, KNN with \(K=5\) performed the best with
\(\kappa=0.161\). For the mean-imputed data, RF was the top performer
with \(\kappa=0.197\). For both MI with \(m=9\) (MI9) and \(m=99\)
(MI99), logistic regression outperformed the other classfication methods
with \(\kappa=0.153\) and \(\kappa=0.274\), respectively.

The highest overall accuracy was \(0.777\) using RF on the mean-imputed
dataset. However, the class-specific accuracies were \(0.965\) for
survival and \(0.185\) for non-survival. The best predictor of
non-survival was logistic regression on MI99.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-7}\label{tab:metrics} Pooled performance results of trained models validated on test set.}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}{lccccc}
\toprule
 &  & Sensitivity & Specificity & Accuracy & Kappa\\
\midrule
 & Logit & 0.200 & 0.814 & 0.658 & 0.015\\

 & LDA & 0.200 & 0.847 & 0.684 & 0.054\\

 & QDA & 0.000 & 0.966 & 0.722 & -0.048\\

 & KNN & 0.300 & 0.847 & 0.709 & 0.161\\

\multirow{-5}{*}{\raggedright\arraybackslash CC} & RF & 0.050 & 0.966 & 0.734 & 0.022\\
\cmidrule{1-6}
 & Logit & 0.222 & 0.894 & 0.732 & 0.137\\

 & LDA & 0.148 & 0.894 & 0.714 & 0.051\\

 & QDA & 0.111 & 0.882 & 0.696 & -0.008\\

 & KNN & 0.222 & 0.824 & 0.679 & 0.050\\

\multirow{-5}{*}{\raggedright\arraybackslash SI1} & RF & 0.185 & 0.965 & 0.777 & 0.197\\
\cmidrule{1-6}
 & Logit & 0.222 & 0.906 & 0.741 & 0.153\\

 & LDA & 0.148 & 0.906 & 0.723 & 0.067\\

 & QDA & 0.111 & 0.882 & 0.696 & -0.008\\

 & KNN & 0.222 & 0.847 & 0.696 & 0.077\\

\multirow{-5}{*}{\raggedright\arraybackslash MI9} & RF & 0.148 & 0.941 & 0.750 & 0.116\\
\cmidrule{1-6}
 & Logit & 0.333 & 0.906 & 0.768 & 0.274\\

 & LDA & 0.185 & 0.906 & 0.732 & 0.111\\

 & QDA & 0.111 & 0.894 & 0.705 & 0.006\\

 & KNN & 0.185 & 0.882 & 0.714 & 0.080\\

\multirow{-5}{*}{\raggedright\arraybackslash MI99} & RF & 0.185 & 0.929 & 0.750 & 0.144\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Feature Selection}\label{feature-selection-1}

At least 16 principle components are needed to explain 80\% of the
variance in the imputed training data and at least 15 principle
components for the complete case analysis. The red dashed lines in
Figure \ref{fig:feature-importance-pca} indicate the expected average
contribution of each variable to the selected principle components if
each variable contributed equally to each principle component.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/graphics-unnamed-chunk-8-1} 

}

\caption{\label{fig:feature-importance-pca}Contribution of variables to the principle components whose cumulative sum explains >80\% of the variation in the data.}\label{fig:unnamed-chunk-8}
\end{figure}

\newpage

\section{Discussion}\label{discussion}

Model performance on the imputed datasets were generally better than in
complete case analysis. Non-parametric methods, KN and RF, performed
better on the complete case analysis and single mean imputation while
logistic regression performed better on the multiply imputed datasets.
All the methods in each analysis were able to predict survival with
\(>80\%\) accuracy. The ability to predict non-survival was the limiting
factor in the performance of a method. Non-survival was best predicted
by logistic regression in MI99 with a prediction accuracy of \(0.333\).

Logistic regression performed consistently well in predicting
non-survival and performed well for imputation methods except for
complete case analysis. LDA also performs rather consistently for each
imputed dataset. The consistent performance of LDA and logistic
regression is not surprising given that they are similar methods,
however logistic regression outperforms LDA in each analysis. LDA can
perform better than logistic regression when the covariates are normally
distributed \autocite{efron_efficiency_1975}, but LDA is not robust to
outliers \autocite{hastie_elements_2009} and Figure
\ref{fig:violin-standardized} shows a number of outliers in almost every
variable. Logistic regression is robust to outliers and makes less
assumptions than LDA \autocite{hastie_elements_2009} allowing it to
generalize better.

There were 136 less observations for complete case analysis than for the
other experiments. Performance metrics have moderate variance due to the
non-survival class in the test set only having 27 observations.
Predicting one or two more observations as non-survival can have
moderately large effects on Kappa. The relatively low number of
observations compounded by the imbalance in the response classes make
prediction difficult. Low predictive power of the variables make this
problem eeven more difficult.

A surprising result is that KNN performed the best in the complete case
analysis, \(\kappa=0.161\) and also performed better than the best
performing model on MI9 (Logit with \(\kappa=0.153\)). RF performed
poorly on CC (\(\kappa=0.022\)) but was the best performer on SI1
(\(\kappa=0.197\)) and second best on MI9 and MI99 (\(\kappa=0.116\) and
\(\kappa=0.144\)). The inverse performance of KNN and RF may be
surprising but Tang et al. show \textcite{tang_when_2018} trees datasets
that work well for nearest-neighbor search problems can be bad
candidates for forests without sufficient subsampling, due to a lack of
diversity. On the imputed datasets KNN performed relatively poorly but
similarly to LDA. It should be noted that KNN consistently was able to
predict non-survival better than other methods, btu at the cost of lower
accuracy in predicting survival. QDA consistently performed the worst,
no better than random chance based on the class likelihoods
(\(\kappa \approx 0\)), suggesting that the class distributions do not
support a quadratic decision boundary.

\emph{Feature Importance:} The selected variables via PCA were the same
for SI1, MI9, and MI99 with some variations in the order of importance.
There were four selected variables in CC that were not selected in the
imputed datasets: \texttt{PreECMO\_IL8}, \texttt{PreECMO\_Creatinine},
\texttt{PreECMO\_Platelets}, and \texttt{PreECMO\_SiL2}.

\subsection{Improvements}\label{improvements}

One way to increase predictive performance is to include more
observations in the analysis. Obtaining new data to include in the
analysis could prove expensive or difficult. Instead, some observations
from the test set could be retained for training the model in a nested
cross-validation approach. The analyses done in this paper would
constitute one iteration of the \(K_\text{o}\) outer cross-validation
iterations where a new test set is selected by stratified randomly
sampling, models are trained on the \(K_\text{o}-1\) via an inner
\(K_{i}\)-fold cross-validation. Since the data was originally split
into 25\% test and 75\% train, If \(K_\text{o}\) is chosen to be
\textgreater{}4, more observations can be retained in the training set.
If \(K_\text{o}=10\) were chosen, the prediction models would be trained
on 67 more observations. The outer cross-validtion would then give the
expected test prediction since it averages over different training sets
\autocite{hastie_elements_2009}. The drawback to nested cross-validation
is that the time complexity scales from \(O(K_\text{i})\) to
\(O(K_\text{o}K_\text{i})\). Indeed, the full time complexity for \(m\)
imputations and a grid search over \(p\) parameters would then be
\(O(K_\text{o}K_\text{i}mp)\).

Variable selection via PCA is independent of the classification method
and allows important variables to be identified outside of the
classification analysis. Embedded variable selection may select
variables more pertinent to the classification method. Regularized
logistic regression can select variables through use of the LASSO
\autocite{tibshirani_regression_1996}. LASSO methods have also been
developed for LDA and QDA Sparse Discriminant Analysis
\autocite{clemmensen_sparse_2011} and DALASS
\autocite{trendafilov_dalass:_2007}. Random forests naturally select
important variables by accumulating the improvement in the
split-criterion over all the trees in the forest for each variable
\autocite[pp.~593]{hastie_elements_2009}.

\begin{itemize}
\tightlist
\item
  Predict with selected features
\item
  Pool posterior probabilities of predictions
\end{itemize}

\subsection{Conclusion}\label{conclusion}

\begin{itemize}
\tightlist
\item
  Summary of proceedure
\item
  Summary of results
\end{itemize}

The CC has some distinctly different properties than the imputed
datasets; the best classification method was KNN, which performed
noteably worse in SI1, MI9, and MI99. The variables selected as
important for the predictions also differed in CC compared to the
imputed datasets.

\newpage

\section*{Appendices}\label{appendices}
\addcontentsline{toc}{section}{Appendices}

\subsection*{A. Additional Exploratory Data
Analysis}\label{a.-additional-exploratory-data-analysis}
\addcontentsline{toc}{subsection}{A. Additional Exploratory Data
Analysis}

\appendixA

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/graphics-unnamed-chunk-9-1} 

}

\caption{\label{fig:violin-standardized}Violin plot of standardised continuous variables.}\label{fig:unnamed-chunk-9}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/graphics-unnamed-chunk-10-1} 

}

\caption{\label{fig:violin-transformed}Violin plot of standardised and transformed continuous variables.}\label{fig:unnamed-chunk-10}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/graphics-unnamed-chunk-11-1} 

}

\caption{\label{fig:heatmap-standardized}Heatmap of standardized and transformed variables.}\label{fig:unnamed-chunk-11}
\end{figure}

\subsection*{B. Algorithms}\label{b.-algorithms}
\addcontentsline{toc}{subsection}{B. Algorithms}

\appendixB

\subsubsection{Random Forests Algorithm}\label{random-forests-algorithm}

The random forests algorithm depicted is adapted from
\autocite{hastie_elements_2009}.

\begin{algorithm}[H]
\label{alg:random-forests-alg}
\caption{Random Forest Classifier}
\DontPrintSemicolon
\SetAlgoLined
\BlankLine

\begin{enumerate}
  \item For ($b=1$ to B):
    \begin{enumerate}
      \item Draw a bootstrap sample $\mathbf{Z*}$ of the size $N$ from the training data.
      \item Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repreating the following steps for each terminal node of the tree, until the minimum node size $n_{min}$ is reached.
      \begin{enumerate}
        \item Select $mtry$ variables at random from the $p$ covariates. 
        \item Pick the best covariate/split-point among the $mtry$. 
        \item Split the node into two daughter nodes. 
      \end{enumerate}
    \end{enumerate}
  \item Output the ensemble of trees $\{T_B\}^B_1$
\end{enumerate}
\BlankLine

Let $\hat{Y}_b(x)$ be the class prediction of the $b^{\text{th}}$ random-forest tree.  Then a new observation, $x$, is classified as:

$$\hat{Y}^B_{\text{rf}}(x) = \text{majority vote } \left\{ \hat{Y}_b(x) \right\}^B_1$$

\end{algorithm}

\subsubsection{MICE Algorithm}\label{mice-algorithm}

The MICE algorithm is adapted from \autocite{van_buuren_flexible_2012}.

\begin{algorithm}[H]
\label{alg:mice-alg}
\caption{Multiple Imputation via Chained Equations}
\DontPrintSemicolon
\SetAlgoLined
\BlankLine

\begin{enumerate}
  \item Specify an imputation model $P(Y^{\text{mis}}_j \vert Y^{\text{obs}}_j, Y_{-j}, R)$ for variable $Y_j$ with $j=1,...,p$
  \item For each $j$, fill in starting imputation $Y^0_j$ by random draws from $Y^{\text{obs}}_j$
  \item Repeat for $t=1,...,T:$
  \item Repeat for $j=1,...,p:$
  \item Define $Y^t_{-j} = (Y^t_1,...,T^t_{j-1}, Y^{t-1}_{j+1},..., Y^{t-1}_p)$ as the currently complete data except $Y_j$ 
  \item Draw $\phi^t_j \sim P(\phi^t_j \vert Y^{\text{obs}}_j, Y^t_{-j}, R)$.
  \item Draw imputations from $Y^t_j \sim P(Y^{ \text{mis} }_j \vert Y^{ \text{obs} }_j, Y^t_{-j}, R, \phi^t_j)$.
  \item End repeat $j$.
  \item End repeat $t$.

\end{enumerate}
\BlankLine

\end{algorithm}

\subsection*{C. Additional Missing Data
Diagnostics}\label{c.-additional-missing-data-diagnostics}
\addcontentsline{toc}{subsection}{C. Additional Missing Data
Diagnostics}

\appendixC

\subsubsection{Visual Insepction of
Imputations}\label{visual-insepction-of-imputations}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/graphics-unnamed-chunk-14-1} 

}

\caption{\label{fig:xyplot-mean}Scatterplot of each imputed dataset}\label{fig:unnamed-chunk-14}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/graphics-unnamed-chunk-15-1} 

}

\caption{\label{fig:xyplot-pmm}Scatterplot of each imputed dataset}\label{fig:unnamed-chunk-15}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{density plot of original and imputed data for MEAN imputation}
\item
  \textbf{density plot of original and imputed data for PMM imputation}
\end{itemize}

This plot compares the density of observed data with the ones of imputed
data. We expect them to be similar (though not identical) under MAR
assumption.

\subsubsection{Convergence Monitoring}\label{convergence-monitoring}

\begin{itemize}
\tightlist
\item
  \textbf{Plot of convergence}
\end{itemize}

\subsection*{D. Feature Selection}\label{d.-feature-selection}
\addcontentsline{toc}{subsection}{D. Feature Selection}

\appendixD

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/graphics-unnamed-chunk-16-1} 

}

\caption{\label{fig:rfe-logit}Ordered feature importance from Logit model}\label{fig:unnamed-chunk-16}
\end{figure}

\subsection*{E. Code Structure}\label{e.-code-structure}
\addcontentsline{toc}{subsection}{E. Code Structure}

\appendixE

The code organization is described in Figure \ref{fig:r-code-chart}.
\texttt{libraries.R} contains all the libraries used in the analysis.
\texttt{functions.R} contains functions used in \texttt{training.R} and
\texttt{model-evaluation.R}. The ensemble cross-validation algorithm is
done in the \texttt{crossValidation()} function. The data is initially
cleaned and split into test and training sets in \texttt{preprocess.R}.
The cleaned datasets are saved to \texttt{processed-data.RData} for use
in \texttt{training.R} and in creating tables and figures in the thesis
rmarkdown. The training data is loaded into \texttt{training.R} where
each of the five classification methods are trained via ensemble
cross-validation. This is done for the four imputation methods: complete
case analysis, mean imputation, MICE using PMM for \(m=9\), and MICE
using PMM for \(m=99\) imputed datasets. The trained models for each
imputation method are saved into separate \texttt{trained-models.RData}.
The methods are then then fit to the full training set in
\texttt{model-evaluation.R} using the trained parameters found in
\texttt{training.R}. The final fitted models are evaluated on the test
set and the fitted models and performance metrics are saved to
\texttt{metrics.RData}.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/r-code-chart} 

}

\caption{\label{fig:r-code-chart}Flowchart of code structure.}\label{fig:unnamed-chunk-17}
\end{figure}

\subsection*{F. OLD PLOTS \& FIGURES}\label{f.-old-plots-figures}
\addcontentsline{toc}{subsection}{F. OLD PLOTS \& FIGURES}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-21}\label{tab:cc-metrics} Complete case analysis accuracy metrics.  The tuned hyperparameters for K-Nearest Neighbors and Random Forests are K=5 and mtry=11, respectively.}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}{lrrrr}
\toprule
  & Sensitivity & Specificity & Accuracy & Kappa\\
\midrule
Logit & 0.20 & 0.814 & 0.658 & 0.015\\
LDA & 0.20 & 0.847 & 0.684 & 0.054\\
QDA & 0.00 & 0.966 & 0.722 & -0.048\\
\textbf{KNN} & \textbf{0.30} & \textbf{0.847} & \textbf{0.709} & \textbf{0.161}\\
RF & 0.05 & 0.966 & 0.734 & 0.022\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-22}\label{tab:mean-metrics} Mean imputation accuracy metrics (m=1).  The tuned hyperparameters for K-Nearest Neighbors and Random Forests are K=5 and mtry=11, respectively.}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}{lrrrr}
\toprule
  & Sensitivity & Specificity & Accuracy & Kappa\\
\midrule
Logit & 0.222 & 0.894 & 0.732 & 0.137\\
LDA & 0.148 & 0.894 & 0.714 & 0.051\\
QDA & 0.111 & 0.882 & 0.696 & -0.008\\
KNN & 0.222 & 0.824 & 0.679 & 0.050\\
\textbf{RF} & \textbf{0.185} & \textbf{0.965} & \textbf{0.777} & \textbf{0.197}\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-23}\label{tab:pmm-metrics} MICE via predictive mean matching accuracy metrics (m=9).  The tuned hyperparameters for K-Nearest Neighbors and Random Forests are K=5 and mtry=13, respectively.}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}{lrrrr}
\toprule
  & Sensitivity & Specificity & Accuracy & Kappa\\
\midrule
\textbf{Logit} & \textbf{0.222} & \textbf{0.906} & \textbf{0.741} & \textbf{0.153}\\
LDA & 0.148 & 0.906 & 0.723 & 0.067\\
QDA & 0.111 & 0.882 & 0.696 & -0.008\\
KNN & 0.222 & 0.847 & 0.696 & 0.077\\
RF & 0.148 & 0.941 & 0.750 & 0.116\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-24}\label{tab:pmm99-metrics} MICE via predictive mean matching accuracy metrics (m=99).  The tuned hyperparameters for K-Nearest Neighbors and Random Forests are K=13 and mtry=15, respectively.}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}{lrrrr}
\toprule
  & Sensitivity & Specificity & Accuracy & Kappa\\
\midrule
\textbf{Logit} & \textbf{0.333} & \textbf{0.906} & \textbf{0.768} & \textbf{0.274}\\
LDA & 0.185 & 0.906 & 0.732 & 0.111\\
QDA & 0.111 & 0.894 & 0.705 & 0.006\\
KNN & 0.185 & 0.882 & 0.714 & 0.080\\
RF & 0.185 & 0.929 & 0.750 & 0.144\\
\bottomrule
\end{tabular}
\end{table}

\newpage

\printbibliography


\end{document}
